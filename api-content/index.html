{"posts":[{"title":"计算机网络之七 - OSI 与 TCP/IP 分层模型","content":"本篇是计算机网络系列的第七篇。在前面的文章中，我们已经分别探讨了 IP、DNS、DHCP、TCP、HTTP 等具体的技术点。然而，这些技术并非孤立存在，它们是一个庞大而精密的系统中的有机组成部分。为了理解它们如何协同工作，我们需要一个宏观的框架来审视整个网络世界。这个框架，就是网络分层模型。 终于轮到分层模型了。其实在绝大多数讲计算机网络的文章中都会在一开始就介绍分层模型，但是我每次读到这种文章时，都会被其庞大的架构所阻拦 —— 它太完美，也太完整，我貌似没法一下子完全理解与接受。所以本系列文章并没有选择从分层模型入手，也有这方面的原因。 本文将作为系列的一个阶段性总结，涉及到业界最核心的两个网络模型——OSI 七层模型和 TCP/IP 模型。我们不仅将理解它们是什么，更将探讨它们为何如此设计、关键协议的内部细节、设计哲学的差异，以及它们在现实世界中的局限性，帮助你将先前学到的知识“融会贯通”，构建一个系统性的网络知识体系。 其余几篇的目录： 计算机网络之一 - IP 与端口 计算机网络之二 - URL 与 DNS 计算机网络之三 - DHCP 与内网穿透 计算机网络之四 - 虚拟网卡与 WireGuard 计算机网络之五 - HTTP 与 HTTPS 计算机网络之六 - 可靠的 TCP 与高效的 UDP 为什么需要网络分层 在深入模型细节之前，我们首先要回答一个根本问题：为什么网络通信需要“分层”？ 想象一个极其复杂的跨国寄送包裹的流程。如果我们将整个流程视为一个单一的、巨大的任务，那么任何一个环节的微小变动（例如更换了运送卡车），都可能需要重新设计整个系统，这无疑是低效且脆弱的。 一个更合理的设计是将其模块化，即分层。每一层都只关心自己的任务，并为上一层提供服务，同时使用下一层提供的服务。这种设计带来了几个显而易见的好处： 简化问题：将一个复杂的大问题分解为多个简单的小问题。 标准化：每一层都可以建立统一的标准（如 RFC），只要接口不变，任何一方的内部实现变化都不会影响其他层。 解耦合：不同层的技术可以独立发展和演进。例如，我们可以将物理传输从电缆升级到光纤，而无需改变上层的网页浏览体验。 易于教学和排错：当网络出现问题时，我们可以逐层排查，极大地提高了故障定位的效率。 当然，分层也并非没有代价，例如数据在各层之间传递会带来一定的性能开销。但在绝大多数场景下，其带来的结构性优势远大于性能上的微小损失。 OSI 七层参考模型：理论的丰碑 OSI 模型（Open Systems Interconnection model） 是由国际标准化组织（ISO）提出的一个概念模型，旨在为计算机网络提供一个标准的、通用的体系架构。它被誉为理论上最完整、最严谨的网络模型，虽然在商业上并未取得绝对的成功，但它对于理解网络通信的各个环节具有极高的指导价值。 OSI 模型将网络通信精确地划分为了七个层次。下面我们自顶向下逐层解析其功能与核心协议。 第七层：应用层 (Application Layer) 这是用户最直接接触的一层，负责为应用程序提供网络服务。它定义了应用程序之间如何交换和解释数据。正如我们在 《计算机网络之五 - HTTP 与 HTTPS》 中探讨的 HTTP 协议，就工作在这一层。 第六层：表示层 (Presentation Layer) 表示层主要处理数据的格式化、加密和压缩，确保一个系统的应用层所发送的数据能被另一个系统的应用层正确理解。它如同一个“翻译官”。 第五层：会话层 (Session Layer) 这一层负责建立、管理和终止不同设备间的会话（Session）。 第四层：传输层 (Transport Layer) 传输层为两个主机之间提供端到端（end-to-end）的数据传输服务。其协议数据单元（PDU）称为数据段 (Segment)。 第三层：网络层 (Network Layer) 网络层负责在复杂的网络环境中，为数据包选择最佳的路由路径，实现逻辑地址（IP 地址）的寻址。其 PDU 称为数据包 (Packet)。 第二层：数据链路层 (Data Link Layer) 数据链路层负责在相邻的两个网络节点之间传输数据帧。它处理物理地址（MAC 地址）的寻址。其 PDU 称为数据帧 (Frame)。 第一层：物理层 (Physical Layer) 这是模型的最底层，负责传输原始的二进制比特流（0和1）。其 PDU 称为比特 (Bit)。 TCP/IP 模型：实践的胜利者 与 OSI 的理论性不同，TCP/IP 模型是伴随着互联网的实践发展而来的，是当前事实上的工业标准。在深入其结构之前，我们先回答一个问题：它为什么叫 “TCP/IP” 模型？ 名字的由来 这个模型得名于其体系中的两个最核心、最基础的协议： TCP (传输控制协议)：工作在传输层，提供可靠的、面向连接的数据传输服务。它如同网络通信的“质量总监”。 IP (网际协议)：工作在网络层，负责数据包的寻址和路由。它如同网络通信的“全球邮政系统”。 TCP 和 IP 的重要性如此之高，以至于它们成为了整个协议族（Protocol Suite）的代名词。因此，描述这个协议族架构的模型，也就自然而然地被称为 “TCP/IP 模型”。 分层结构 TCP/IP 模型更加简洁和实用，通常被描述为四层或五层模型。 TCP/IP 四层模型 这是最经典的 TCP/IP 模型划分，它将功能相近的几个层进行了合并，更侧重于描述协议簇的宏观结构： 应用层 (Application Layer)：对应 OSI 的应用层、表示层、会话层。包含了所有高层协议，如 HTTP, FTP, DNS 等。 传输层 (Transport Layer)：对应 OSI 的传输层。负责端到端的通信，核心是 TCP 和 UDP 协议。 网际层 (Internet Layer)：对应 OSI 的网络层。核心是 IP 协议，负责数据包的寻址和路由。 网络接口层 (Network Interface Layer)：对应 OSI 的数据链路层和物理层。负责处理与物理网络媒介（如以太网、Wi-Fi）相关的所有事务。 TCP/IP 五层模型 为了教学和理解上的便利，业界更常使用一个五层模型。它实际上是 OSI 和 TCP/IP 四层模型的一个折中，将四层模型中的“网络接口层”重新拆分为“数据链路层”和“物理层”，从而能更清晰地描述底层的工作原理。这个五层模型也是我们后续文章将主要参照的结构。 应用层 (Application Layer)：同四层模型，对应 OSI 的上三层。 传输层 (Transport Layer)：同四层模型，对应 OSI 的传输层。 网络层 (Network Layer)：同四层模型的网际层，对应 OSI 的网络层。 数据链路层 (Data Link Layer)：对应 OSI 的数据链路层。 物理层 (Physical Layer)：对应 OSI 的物理层。 下面的表格清晰地展示了三者之间的映射关系： OSI 七层参考模型 TCP/IP 五层模型 TCP/IP 四层模型 应用层 应用层 应用层 表示层 会话层 传输层 传输层 传输层 网络层 网络层 网际层 数据链路层 数据链路层 网络接口层 物理层 物理层 核心协议深度剖析 理解了分层，我们还需要深入协议的内部，看看它们是如何通过精巧的设计来完成各自使命的。 网络层核心：IPv4 头部结构 我们在 《计算机网络之一 - IP 与端口》 中已经初步认识了 IP 协议，它是整个网络层的核心。为了更深入地理解数据包是如何被路由的，我们有必要详细剖析其头部结构： 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |Version| IHL |Type of Service| Total Length | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Identification |Flags| Fragment Offset | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Time to Live | Protocol | Header Checksum | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Source Address | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Destination Address | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Options | Padding | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ 版本 (Version)：4位，指明协议版本，对 IPv4 来说就是 4。 首部长度 (IHL)：4位，表示 IP 头部有多少个 32 位字（4字节）。由于该字段最大值为 15，所以 IP 头部最长为 15 * 4 = 60 字节。 服务类型 (Type of Service)：8位，用于指定数据包的服务质量（QoS），例如低延迟、高吞吐量或高可靠性。现在通常被定义为 DSCP（差分服务代码点）。 总长度 (Total Length)：16位，指明整个 IP 数据包（头部+数据）的总长度，单位是字节。最大长度为 65535 字节。 标识 (Identification)、标志 (Flags) 和 片偏移 (Fragment Offset)：这三个字段共同用于 IP 分片。当一个数据包的大小超过了链路的最大传输单元（MTU）时，它必须被分割成多个小的数据包（分片）。 标识：16位，唯一标识一个原始数据包。同一原始数据包的所有分片都具有相同的标识号。 标志：3位。第一位保留；第二位是 DF (Don't Fragment)，如果置 1，表示禁止分片；第三位是 MF (More Fragments)，如果置 1，表示后面还有更多分片，如果是最后一个分片则为 0。 片偏移：13位，指明当前分片的数据部分在原始数据包中的位置。 生存时间 (Time to Live, TTL)：8位，数据包在网络中的“寿命”。每经过一个路由器，TTL 值减 1。当 TTL 变为 0 时，数据包被丢弃。这有效防止了数据包在网络中因路由错误而无限循环。 协议 (Protocol)：8位，指明该数据包承载的上层协议是什么。例如，6 代表 TCP，17 代表 UDP。接收方据此决定将数据交给哪个上层模块处理。 首部校验和 (Header Checksum)：16位，用于检验 IP 头部的完整性，不包含数据部分。由于 TTL 在每一跳都会改变，所以路由器需要重新计算校验和。 源地址/目标地址 (Source/Destination Address)：各 32 位，标识了数据包的发送方和接收方。 选项 (Options) 和 填充 (Padding)：选项是可变长的，用于一些特殊处理，如记录路由。填充用于确保整个 IP 头部是 32 位的整数倍。 传输层核心：TCP 与 UDP 传输层负责端到端的通信，其两大核心协议 TCP 和 UDP，我们已在之前的文章中做了详细的探讨。 TCP 通过序列号、确认号、标志位等头部关键字段，以及三次握手和四次挥手等严谨的连接管理机制，提供了可靠的、面向连接的服务。关于这些内部工作细节，强烈建议您回顾 《计算机网络之六 - 可靠的 TCP 与高效的 UDP》，本文不再赘述。 网络层的路由协议 IP 协议只负责数据包的投递，但如何确定投递路径，则是由路由协议决定的。路由协议在路由器之间运行，共同构建和维护路由表。它们主要分为两大流派： 距离矢量协议 (Distance-Vector)：如 RIP。每个路由器只知道与它直接相邻的邻居，并与它们交换整个路由表。它关心的是“到某个目的地有多远（距离）”和“应该从哪个方向（矢量）走”。这种方式简单，但容易产生路由环路且收敛慢。 链路状态协议 (Link-State)：如 OSPF。每个路由器都拥有整个网络的拓扑图。当网络状态变化时，路由器会向所有其他路由器广播这个变化。它关心的是“整个网络的连接状态”。这种方式更复杂，但收敛快且不易产生环路。 OSI 与 TCP/IP：一场理论与实践的赛跑 一个经典问题是：为什么理论上更完美的 OSI 模型，在实践中却输给了更“粗糙”的 TCP/IP 模型？ 时机决定一切：TCP/IP 诞生于 20 世纪 70 年代，与美国国防部的 ARPANET 项目紧密结合，经过了长期的实践检验。当 OSI 模型在 80 年代后期完成其标准化工作时，TCP/IP 早已在学术界和军事领域广泛部署，形成了强大的事实标准。 哲学差异：OSI 遵循“先制定标准，再进行实现”的学院派哲学，力求大而全，导致其过于复杂。而 TCP/IP 则源于“先有可用实现，再提炼标准”的工程师哲学，更注重解决实际问题，因此更简洁、高效。 实现复杂度：OSI 模型的复杂性（尤其是会话层和表示层）使得其实现难度和运行开销都很大。相比之下，TCP/IP 将这些功能交由应用层自行处理，大大降低了核心网络的复杂度。 最终，这场赛跑以实践派的胜利告终。OSI 虽然输掉了市场，但它清晰的层次划分和严谨的定义，使其成为了网络教育和理论分析不可或缺的“教科书”。 应用模型：从理论到实践 理解了分层模型这一“地图”，我们便拥有了分析和诊断网络问题的框架。接下来，我们将通过两个具体的案例，将这些抽象的层次和协议应用到实际场景中。第一个案例将展示一次标准网络请求的完整流程，第二个案例则将剖析一个常见的开发陷阱，以展示分层模型在排错中的指导价值。 案例一：一次完整的 HTTP 请求 这个案例将追踪一次典型的、访问外部网站的 HTTP 请求，以展示数据包如何穿越互联网。 场景设定： 你的电脑：IP 192.168.1.100 (此地址通常通过 DHCP 服务动态获取，相关细节请回顾 《计算机网络之三 - DHCP 与内网穿透》), MAC AA:AA:AA:AA:AA:AA 家庭路由器：内网 IP 192.168.1.1, MAC BB:BB:BB:BB:BB:BB；公网 IP 123.123.123.123 目标网站服务器：IP 216.58.200.46, MAC CC:CC:CC:CC:CC:CC 旅程开始： DNS 查询：在一切开始之前，浏览器需要知道 example.com 的 IP 地址。它会向 DNS 服务器发起查询，将域名解析为 IP 地址 216.58.200.46。关于 DNS 的详细工作原理，请参阅 《计算机网络之二 - URL 与 DNS》。 应用层：你在浏览器输入 http://example.com。浏览器构建一个 HTTP GET 请求报文。 传输层：操作系统为该请求分配一个临时的源端口（如 54321），目标端口为 HTTP 的标准端口 80。然后，它将 HTTP 报文封装进一个 TCP 数据段，并写入 TCP 头部（包含源/目标端口等信息）。 网络层：操作系统继续封装，添加 IP 头部，形成 IP 数据包。头部信息包括源 IP (192.168.1.100) 和目标 IP (216.58.200.46)。 数据链路层 (第一跳：电脑 -&gt; 路由器)： 操作系统查询路由表，发现目标 IP 216.58.200.46 不在本地网络，需发往默认网关 (192.168.1.1)。 操作系统使用 ARP 协议查询 192.168.1.1 对应的 MAC 地址，得到路由器的 MAC BB:BB:BB:BB:BB:BB。 最终，它构建一个以太网帧，头部包含源 MAC (AA:AA...) 和目标 MAC (BB:BB...)，并将 IP 数据包作为其“货物”。 物理层：电脑网卡将该数据帧转换为电信号，通过网线发送出去。 在路由器上 (关键中转)： 路由器的数据链路层接收到数据帧，检查目标 MAC 是自己，于是“拆开”帧，取出 IP 数据包，并将其向上传递到网络层。 路由器的网络层执行 NAT (网络地址转换)。它将 IP 包的源 IP 从 192.168.1.100 修改为自己的公网 IP 123.123.123.123，并记录下这个映射关系，以便响应回来时能正确转发。 路由器准备将修改后的 IP 包发往互联网。它查询自己的路由表，找到下一跳的地址，并构建一个新的数据链路层帧（此时的目标 MAC 将是下一个路由器的 MAC），然后发送出去。 在互联网中：数据包经过多个路由器的多次转发，每一跳都重复着“解封装-查路由-再封装”的过程，但 IP 头部的源/目标 IP 始终不变（除了 NAT 网关）。 抵达服务器：服务器接收到数据，自底向上进行解封装，最终将 HTTP 请求报文送达 Web 服务器应用。服务器处理请求后，再以同样的方式将响应数据封装并发回。 案例二：从 localhost 到局域网的连接问题 本博客基于 Gridea 实现，该客户端提供了一个便捷的“预览”功能，点击后会在浏览器中打开 http://localhost:4000 这样的地址，用于实时查看文章效果。这个过程在本地电脑上运行良好，但当作者希望使用其他设备（如手机）预览地址文章时就显得力不从心了，因为这个地址在其他设备上是 “打不开” 的。 问题的演进 初次尝试：一个直接的想法是在手机浏览器中输入 http://localhost:4000。这个尝试必然会失败。其原因是 localhost 是一个特殊的环回地址，它指向设备自身。手机访问 localhost，访问的是手机自己，而非提供服务的电脑。 二次尝试：在理解了 localhost 的局限性后，基于咱们已经学习过的计算机网络知识，下一步自然是尝试使用电脑的局域网 IP 地址。在 macOS 上我们可以通过 “设置 -&gt; 网络” 查到电脑的 IP，假设为 192.168.1.100，于是在手机上访问 http://192.168.1.100:4000。然而，这种尝试也会失败，浏览器会显示连接超时或无法访问。 这就引出了问题的核心：在网络层面上，数据包已经能够从手机路由到目标主机，为什么连接依然无法建立？这表明问题很可能出在更高层，即运行在主机上的服务程序本身。 验证网络层 (Layer 3) 及以下是否通畅 在怀疑上层应用问题之前，我们必须百分之百确认基础网络是连通的。ping 命令是验证网络层连通性的最佳工具，它使用 ICMP 协议工作。我们可以在 iPhone 上安装任意一款网络调试工具（比如 iNetTools）然后执行： ping 192.168.1.100 如果 ping 成功收到回复，我们将获得一条至关重要的信息：从手机到电脑的 网络层及以下的物理层、数据链路层是完全通畅的。数据包确实已经成功抵达了目标主机的网络协议栈。这就允许我们排除掉所有底层网络问题（如 Wi-Fi 隔离、IP 冲突、子网错误等），将注意力集中到更高层次。 聚焦传输层 (Layer 4) 的可能性 既然网络路径没有问题，数据包在传输层“消失”的可能性就大大增加。在这一层，主要有两个实体在工作：操作系统内核（防火墙） 和 应用程序本身。 可能性 A：被防火墙拦截。电脑的防火墙可能设定了规则，允许 ICMP 包（ping）通过，但阻止了针对 4000 端口的 TCP 连接请求。这是一个非常合理的怀疑点。 可能性 B：端口上没有服务在监听。应用程序在启动时，需要向操作系统内核申请监听一个或多个具体的“地址-端口”对。如果程序只申请监听 127.0.0.1:4000，那么当一个目标地址是 192.168.1.100:4000 的数据包到达时，内核会发现没有任何程序在等待这个数据包，于是只能将其丢弃。 使用工具做出最终裁定 现在，我们需要一个工具来区分到底是可能性 A 还是 B。lsof 命令的作用正在于此，它能直接查询操作系统内核，告诉我们“哪个进程正在监听哪个地址的哪个端口”，从而绕过所有猜测。 lsof -i :4000 其输出结果： COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME Gridea 13682 rakuyo 50u IPv4 0x7e41bc65e0e51533 0t0 TCP localhost:terabase (LISTEN) 这个结果明确地告诉我们，Gridea 进程监听的地址是 localhost。这直接证实了可能性 B。数据包并非被防火墙拦截，而是在到达内核后，因找不到对应的监听程序而被丢弃。至此，问题根源被精准定位。 解决方案 诊断出根源后，解决方案便是在服务层面进行修正。但是很可惜的是 Gridea 并没有提供修改的选项。否则我们可以将默认的 localhost 或 127.0.0.1 修改为 0.0.0.0，实现监听地址的修改。 0.0.0.0 是一个特殊的地址，它指示操作系统监听本机所有可用的网络接口，包括环回地址和所有局域网地址。 这个排查过程清晰地展示了分层模型在解决实际问题时的指导作用。若不理解网络层的地址和监听机制，开发者可能会陷入反复检查防火墙、网络设置的无效循环中。而通过运用正确的理论知识和诊断工具，则可以快速、精准地定位问题根源。 分层视角下的网络安全 分层模型也为我们理解和应对网络安全问题提供了一个清晰的框架。攻击可能发生在任何一层： 数据链路层安全：攻击者在局域网内，可以通过 ARP 欺骗来冒充网关，窃听或篡改流量；或者通过 MAC 泛洪攻击耗尽交换机的 MAC 地址表，使其变为“集线器”，从而嗅探整个网络的流量。 网络层安全：攻击者可以伪造源 IP 地址（IP 欺骗）来隐藏身份或嫁祸他人。经典的 DDoS 攻击（如 SYN Flood）也利用了 IP 层的机制，发送大量伪造源 IP 的连接请求，耗尽服务器资源。 传输层安全：端口扫描是黑客探测目标主机开放服务的主要手段，它直接作用于传输层的端口。 应用层安全：这是最广为人知的攻击面，包括 SQL 注入、跨站脚本（XSS）、CSRF 等，它们都利用了应用程序自身的逻辑漏洞。 为了应对这些威胁，安全协议也作用于不同层面。例如，IPsec 工作在网络层，提供端到端的加密和认证。而我们熟知的 TLS/SSL 则比较特殊，它逻辑上位于应用层和传输层之间，为上层的应用数据提供加密、完整性保护和身份认证。此外，另一种强大的安全工具是 VPN（虚拟专用网络）。技术如 WireGuard 通过创建一张虚拟网卡，并在网络层对所有数据进行加密封装，形成一个安全的“隧道”。所有进出你设备的流量都通过这个隧道传输，从而有效地保护了通信的私密性和完整性，抵御了中间人攻击。更深入的探讨可以参见 《计算机网络之四 - 虚拟网卡与 WireGuard》。 模型的局限性与争议 分层模型是完美的吗？并非如此。在学术和工程实践中，它们也存在一些争议： 归属争议：有些重要协议难以被完美地归入某一层。例如，ARP 协议通过 IP 地址查询 MAC 地址，它似乎横跨了网络层和数据链路层。ICMP 协议（ping 命令所使用的协议）被 IP 协议承载，但它本身是为网络层提供控制和错误信息的，其位置也存在争议。 效率问题：严格的分层会带来性能开销。在一些高性能计算场景中，为了追求极致的低延迟，可能会出现“跨层”操作，绕过某些协议栈以加速数据处理。 模型与现实的脱节：OSI 模型的会话层和表示层在现实中很少被独立实现，它们的功能往往被应用层自身所包含，这也是 TCP/IP 模型将其合并的原因。 理解这些局限性，能帮助我们更辩证地看待这些模型，认识到它们是用于理解复杂系统的强大工具，而非一成不变的教条。 总结 通过本文的深度剖析，我们不仅了解了 OSI 和 TCP/IP 模型是什么，更探讨了它们的设计哲学、历史演进、协议细节、实践流程乃至理论局限。 网络分层模型，本质上是一种“分而治之”的工程智慧。它将一个不可能完成的复杂任务，拆解为一系列定义清晰、可以管理的子任务。理解这些分层模型，就像是拿到了一张高精度的网络世界地图。当我们再去审视之前文章中讨论的 HTTP、TCP、IP、DHCP 等协议时，便能清晰地知道它们在整个通信流程中所处的位置和扮演的角色。有了这个“世界观”，我们后续探索更复杂的网络技术时，才能做到胸有成竹，游刃有余。 ","link":"https://blog.rakuyoo.top/computer-network-osi-and-tcpip-models/"},{"title":"Vibe Coding 的两面：一周体验的甜蜜与挣扎","content":"Vibe Coding，也就是 “氛围编程” 这个概念已经火了很长一段时间了，可惜主包之前碍于种种原因一直没能找到机会亲自试一下。不过就在上周正好要做一个新的系统，给了我尝试 Vibe Coding 的机会。现在系统已经搭建完毕，本文就用来记录这一周多一点的 “氛围编程” 体验吧。 什么是氛围编程 (vibe coding)？ 引用 Google Cloud 上对 Vibe Coding 的介绍： 氛围编程 (vibe coding) 是一种新兴的软件开发实践，它使用人工智能 (AI) 根据自然语言提示生成功能代码，从而加快开发速度，并让应用构建变得更加容易，对于那些编程经验有限的用户尤其如此。 该术语由 AI 研究人员 Andrej Karpathy 于 2025 年初创造，用于描述一种工作流，其中开发者的主要角色从逐行编写代码转变为通过对话风格更浓的过程指导 AI 助理生成、完善和调试应用。这样，您就可以腾出时间和精力思考大方向或应用的主要目标，而 AI 则负责编写实际代码。 在实践中，氛围编程通常有两种主要的应用方式： “纯”氛围编程：在这种最探索性的形式中，用户可能会完全信任 AI 的输出能够按预期工作。正如 Karpathy 所描述的那样，这就好比是“忘记了代码的存在”，因此它最适合用于快速构思，或者他所说的“周末即兴项目”，在这些场景中，速度是首要目标。 Responsible AI 辅助开发：这是该概念的实际专业应用。在这种模式下，AI 工具充当功能强大的协作者或“编程搭档”。用户会指导 AI 操作，然后审查、测试并理解 AI 生成的代码，因此对最终产品拥有完全的所有权。 在我这一周多的工作时间里，我才用的算是介于这两种模式中间的第三种模式： 向 AI 描述任务，指导 AI 操作，设定架构（哪些代码该放在哪个文件夹下），同时框定限制（比如些许代码格式）。 一些并不全面的审查，然后尽量不去理解 AI 所编写的代码的具体细节。 打个比方就是： 我要求 AI 收拾书架，第一层放编程类书籍，第二层放小说，第三层... 并且按照从高到低排序。 最终 AI 排好了，我看了一眼确实是从高到低排序，也是按照编程类、小说... 的顺序放置。 但是我没有去具体的了解每层都有哪些书。 我做了个什么系统？ 我这个的任务是要开发一个小系统，它的作用是尽可能地将 “GitLab MR Review” 这件事自动化起来，并且接入 AI 与思码逸辅助 Review。这套系统的运行流程基本如下图所示： 任务拆解 从流程图上来看，这个系统需要实现以下功能： 它需要部署到服务器上，这里我选择的是 Docker。 有一系列围绕 GitLab 的功能。 监听 MR 中的一些行为事件。 评论的发布。 标签的修改。 在 MR 中发布评论。 为 MR 评论评论添加 emoji（未在流程图中体现）。 为 MR 设置标签。 通过 api 触发 GitLab-CI 流水线。 封装多个 Review 辅助系统。 AI api。 思码逸 api。 第三方通知系统。 这里我接入的是其他部门提供的企业微信机器人发送消息接口。 技术栈 熟悉我的都知道，我之前有 python + Flask 开发后端系统的经验，所以本次这个小型系统主包也采用了相同的系统架构设计。 其余三方库方面主要用到了以下这些： flask-redis python-gitlab python-dotenv dynaconf dataclasses-json 其中最后两个是 AI 推荐使用的，主包之前并未接触过。 AI 客户端方面我主要的以下这些： 客户端 模型 是否免费 使用占比 Gemini-CLI gemini-2.5-pro ✅ 75% Gemini-CLI gemini-2.5-flash ✅ 10% Qwen-Code CLI Qwen/Qwen3-Coder-480B-A35B-Instruct ✅ 15% 也就是说交给 AI 的工作中，75% 是使用 “免费版的 Gemini-CLI + gemini-2.5-pro 模型” 完成的。 之所以用免费的 AI，主要原因还是穷。非要说得好听一点就是想看看 AI 的下限在哪里，又或者大部分人应该用的都是免费的 AI。 另外需要说明的是，在这次工作我没有使用任何的 mcp，也没有使用任何的 AI 配置文件，比如 GEMINI.md 等。 最终效果怎么样？ 抛去项目初始框架的搭建以及部署，几乎所有业务逻辑我都是提供任务描述之后 让 AI 给我实现的。那么结论也就像标题说的这样，对于我这样一个初次尝试 Vibe Coding 的小白而言，可以说是 “又爱又恨”，感觉也是和互联网上大部分人的感受一样。 爱在哪里？ AI 的强大无需多言，可以说 80% 的任务它都完成的很好： 我没有看过 python-gitlab 的文档，我只需要告诉 AI 要使用这个框架，它几乎就可以使用正确的 API，而就在去年，网页版免费的 ChatGPT 还做不到这一点。 只需要告诉 AI “我希望实现配置热更新，这样在我修改配置时就不需要再重启 Docker 服务了”。它就给我推荐了 dynaconf 这个框架，代替搜索引擎的同时还帮我实现了需求。 这个小系统虽然简单，但是它也有一个需要用到算法的地方：在使用 AI 辅助 Review 时，如果代码 diff 过长会导致超出模型的上下文，此时需要分片多次提问。在这点上 AI 处理的也很好，让我真正感受到了什么是 “面向结果而不面向实现细节”。 恨又在哪里？ 调用 python-gitlab 中的方法时，有过好几次使用了错误的返回值的情况。需要我手动查看源码后再告诉 AI 正确的返回类型是什么。 AI 稳定性可以说是让人捉摸不透：它能很好的推荐你通过 dataclasses-json 解决 “接口参数驼峰式命名” 与 “python 属性下划线式命名” 的矛盾，但是它想不到将 Model 转 JSON 时使用的方法改为 dataclasses-json 专用的方法。 对于 &quot;\\n&quot; 换行符会犯一些难以理解的错误，特别是处理多行字符串时，会直接使用换行来代替字符串的 &quot;\\n&quot;。 ... 如果说上面的还是一些 “小细节”，那么接下来的就是针对 Qwen 的吐槽了： Qwen-Code CLI 虽说是基于开源的 Gemini-CLI 进行开发，但是这两者的差距还是很大。在开发思码逸 API 封装模块时，我直接让 AI 去读完整的 swagger json 文档，这份文档非常大，足足有 3万 8 千行 之多。我一开始让 Qwen-Code CLI 去读取，直接占用了将近 20% 的上下文，而且读取时间非常长（Qwen 的模型是分批多次读取）。后来我试了以下用 Gemini-CLI 去读取，结果发现占用的占下文非常小，而且读取速度非常快，同时也能根据文档准确回答出我的问题，完成后续的任务开发。 并且在我让 Qwen 根据这份 swagger 文档所描述的，思码逸 api 所提供的能力，来回答我能否实现某些需求时，它也给出了错误的答案：实际上是不能满足我的需求的。叠加当时我因为缺少 api-key 无法直接验证 AI 编写的逻辑，最终导致了将近两天的工作完全白做。 Gemini 虽说比 Qwen 要强，但是在 “提问的艺术” 方面也有一些需要注意点的： 你需要尽可能地提及所有细节，就像是上面提到的 dataclasses-json 的例子，你没有在提问时说 “同时修改所有 Model 转 JSON 时使用的方法”，那么 AI 就有可能不会去修改 —— 但是如果你像我一样第一次使用这个库，不知道要去修改这个地方，那就会像我一样遇到这个问题；又或者把它写入到一个通用规则里：每次类似的修改，都要检查调用逻辑是否需要修改。 先列出方案再实际修改，会比一开始就让它修改效果好很多。这一点在过去很多人的文章中都有提到，事实也是如此，这不光能减少代码回滚的次数，也能让你去审查 AI 是否真正的理解了你的需求。 只让 AI 使用中文回答还不够，这不足以让它理解它是在为一个中国人打工。你还需要告诉它 “日志和注释都使用中文”。 意外的，合理使用标点符号，比如句号，可以让 AI 更好的理解你的意思，比如这句话是否结束了？ ... 诸如此类的点还有很多，总的来说就是： 你真的真的真的真的需要向 AI 描述的很细很细很细，有的时候这真的很累很辛苦。 以上种种导致的问题就是： 如果你是一个偶尔丢失一些小细节，或者根本就是一个粗心大意的人，那么 AI 的使用体验将会大打折扣； 光有严谨还不够，你还需要足够的耐心去组织每次的提问内容，还要有一定程度上的语文表达能力、语言组织能力（绝不止大家平时日常的口头交流）。对于那些平时在互联网上习惯使用书面表达方式而非口语化描述的人来说会有一定的优势。相反则会面临不少的问题。 另外，光让 AI 列出计划还不够，它还有一个喜欢 “胡编” 的通病。对此我的解决方案是告诉它 “如果有哪些不确定，或者是推测东西，请使用 # TODO 标记，并在修改完成后向我汇报。”，这样它虽然还会执行一些推测，但是至少我可以更好的分辨哪些是它推测的东西，以便在后续的对话中帮助它解决这些 TODO。 Prompt 在这里贴一下我做这个项目时最常用的提示词： 我每完成一个任务，都会 /clear，然后使用新的提示词开启一个新的会话。 你是一位资深的 python 开发工程师，尤其熟悉 flask 框架。现在我们所处的工作目录正是一个 flask python 后端项目。 &lt;!-- 描述具体的需求或者任务 --&gt; 一些规范要求： 1. 始终用中文回复我，并且日志、代码注释都使用中文。 2. 所以日志都应该使用 `logging` 框架实现，比如 `logging.info(f&quot;...&quot;)`。 3. python 文件的开头不需要包含 python 版本注释的内容。 4. 私有方法放在文件的最后面，越是工具性质的方法越靠后。 5. 调用方法时候，如果参数多于两个，则必须显式指定参数；如果只有一个，那么不要添加参数名称。 6. 一个方法的参数的数量最多为 5 个（包括 `self`），如果超过，则应该以**类型**的方式去定义，并放在对应模块的 `/models/param` 文件夹中，并且每个属性都应该有注释。 7. 如果有哪些不确定，或者是推测东西，请使用 `# TODO` 标记，并在修改完成后向我汇报。 8. 请你做任何修改前，先向我确认你的计划，当我认可你的计划后，再实施你的修改。 9. 不要创建 `Optional[List[...]]` 可选值数组，使用空数组 `[]` 代替。 10. 文件的末尾要有一个单独的空行。 11. 注释使用 Google 风格。 聊一些 “理想主义” 说我上班不为了钱是假的，但是我也是真的喜欢敲代码，喜欢通过代码去做产品。 最早的时候让我回答为什么选择做程序员而不是产品经理，我的回答是 “相比较产品，程序员可以通过自己双手亲自实现自己的想法，这更酷”。但是做了这么多年程序员，写了这么多年代码之后，我已经不知道我是更喜欢敲代码还是更喜欢做产品了。 Vibe Coding 对于我而言就像是 “产品经理让 AI 帮我实现了我的想法”，我觉得这也能算是 “亲手”，但是就感觉... 有点奇怪。在过去我很喜欢回过头去对着自己写的框架或组件 “犯花痴”，感慨我真厉害，能写出这么优雅的代码，同时欣赏整个产出。我觉得这不算自大，只是成就感与满足感的一种体现，对外我还是会虚心接受任何的批评的。 但是对于 AI 产出的代码，我觉得它很难让我产生成就感与满足感，目前还没有发现原因。或许等到哪天我能用 AI 开发软件并且挣到钱的时候，我的想法才能有所改观？感觉我现在还处于 “写代码 = 玩玩具”，或者是 “做玩具” 的阶段，至于在这个阶段里加上 AI 后会变成什么，我暂时还没办法很好的去表达描述它。 总结 AI 编程 100% 可以在实际工作中使用，而 Vibe Coding 在某种程度上也能很好的完成工作。 “熟悉编程的人” 与 “会编程的人” 在使用 AI 进行开发时最终应该都能完成工作，但是效率绝对是会有很大很大的差别的，至于 “不会编程的人” 能不能利用 AI 来完成一个项目，就不得而知了（笑）。至少，在当下这个时间节点，我是这么认为的。 ","link":"https://blog.rakuyoo.top/vibe-coding-sweet-struggle/"},{"title":"计算机网络之六 - 可靠的 TCP 与高效的 UDP","content":"TCP 与 UDP 是网络传输层的两大核心协议，它们以截然不同的方式定义了数据在应用程序间的传输。TCP 如同一次通话，追求可靠与完整；UDP 则像一张明信片，主张高效与迅捷。正是这两种设计哲学的差异，决定了它们在网页浏览、文件传输、视频会议、在线游戏等不同场景下的应用。本文将深入解析二者的核心机制与关键差异。 本系列其余几篇的目录： 计算机网络之一 - IP 与端口 计算机网络之二 - URL 与 DNS 计算机网络之三 - DHCP 与内网穿透 计算机网络之四 - 虚拟网卡与 WireGuard 计算机网络之五 - HTTP 与 HTTPS 计算机网络之七 - OSI 与 TCP/IP 分层模型 TCP：可靠的“电话通话” 如果说 UDP 像一张随手寄出的明信片，那么 TCP (Transmission Control Protocol, 传输控制协议) 就是一通严谨的国际长途电话。在通话开始前，你必须先拨号、等待对方接听、双方确认身份并都说“喂，听得到吗？”之后，才会开始真正的交谈。通话结束后，还要礼貌地道别，确保双方都知晓通话结束。 这个过程虽然繁琐，但它确保了整个对话的完整性和有序性，这正是 TCP 的核心设计哲学。 核心特性：面向连接与可靠 面向连接 (Connection-Oriented)：在发送任何应用数据之前，通信双方（客户端和服务器）必须先通过一个标准化的过程建立一个虚拟的连接。所有后续的数据交换都在这个已建立的连接上进行。 可靠传输 (Reliable)：TCP 提供了一系列复杂的机制来保证数据能够准确、有序地从发送方传输到接收方。它承诺“不丢包、不失序、无差错、无重复”。 这一切的可靠性，都始于那个著名的“三次握手”过程。但在深入流程之前，我们必须先了解构成这次“握手”的几个关键“零件”。 TCP 报文：一次通话的“信封” TCP 通信的数据单元被称为报文段 (Segment)。你可以把它想象成一个高度结构化的信封，其“信封皮”，也就是 TCP 头部 (Header)，包含了所有用于控制通信的元信息。下表是其主要字段的概览： 字段 英文名称 简称 大小 描述 源端口 Source Port sport 16 bits 标识发送方应用程序的端口号 目标端口 Destination Port dport 16 bits 标识接收方应用程序的端口号 序列号 Sequence Number seq 32 bits 标记本报文段数据第一个字节在数据流中的位置 确认号 Acknowledgment Number ack 32 bits 期望收到的对方下一个报文段的序列号 数据偏移 Data Offset - 4 bits TCP 头部自身的长度，单位为4字节（32位） 保留 Reserved - 6 bits 未使用的保留位，必须为 0 标志位 Flags - 6 bits 用于控制连接状态，如 SYN, ACK, FIN 窗口大小 Window Size win 16 bits 用于流量控制，表示接收方还能接收多少数据 校验和 Checksum csum 16 bits 用于检查头部和数据的传输错误 紧急指针 Urgent Pointer - 16 bits 当URG标志位为1时有效 选项 Options - 可变 用于携带额外的控制信息 在理解后续的握手流程时，我们无需关注所有细节，只需将注意力集中在最重要的四个“角色”上：SYN 和 ACK 这两个标志位，以及 seq 和 ack 这两个核心编号。 SYN 和 ACK 是 TCP 报文头中的两个非常重要的标志位 (Flags)，它们就像是通信双方用来表达意图的“信号旗”。 SYN (Synchronize Sequence Numbers - 同步序列号) 含义：这个标志位用于发起和建立连接。当一方想要与另一方建立连接时，它会发送一个 SYN 标志位置为 1 的报文。这可以理解为在说：“你好，我想和你建立通信，我们来同步一下初始的序列号吧！” ACK (Acknowledgment - 确认) 含义：这个标志位用于确认收到数据。当 ACK 标志位置为 1 时，意味着报文中的“确认号”字段有效。它告诉对方：“你之前发送的数据我已经收到了。” 在连接建立之后，几乎所有的 TCP 报文都会将 ACK 位置为 1。 而 seq 和 ack 是 TCP 实现可靠传输的基石，它们共同解决了一个核心问题：在不可靠的网络上，如何保证数据不丢、不乱、不重。 seq (Sequence Number - 序列号) 作用：它的核心作用是给数据包进行编号。TCP 把要传输的数据看作一个连续的字节流，seq 就是这个流中每一个数据包里第一个字节的编号。 设计原因： 保证顺序：网络传输中，数据包可能会因为路由不同而失序到达。接收方可以根据 seq 号对数据包进行重新排序，从而恢复出原始的、有序的数据。 丢包检测：接收方如果发现收到的 seq 号不连续（比如收到了 100 和 300，但没收到 200），就知道中间有数据包丢失了，可以请求发送方重传。 ack (Acknowledgment Number - 确认号) 作用：它的作用是告诉发送方我期望接收的下一个字节的序列号是多少。这个设计非常巧妙，因为它隐含地确认了在这个编号之前的所有数据都已成功收到。 设计原因： 高效确认：如果发送方发送了 100、200、300 三个包，接收方只需回复一个 ack=400，就代表“100、200、300 我都收到了，请从 400 开始发”。这比为每个包都单独回复一次确认要高效得多。 建立可靠连接：它是发送方判断对方是否成功收到数据的唯一依据。 三次握手：同步序列号与交换能力 理解了上述几个核心“词汇”后，我们再来审视三次握手的过程，它的每一步都变得有据可循。其本质，是通过三次通信，完成两个核心任务： 交换并确认双方的初始序列号 (ISN)，为后续数据的有序传输打下基础。 确认双方都具备可靠的发送和接收能力。 第一次握手 (Client -&gt; Server): 内容: 客户端发送一个 TCP 报文，其中 SYN 标志位置为 1，并选择一个随机的初始序列号 seq=x。 目的: 客户端向服务器表明“我想要建立连接”，并告知自己的起始序列号。 状态: 客户端进入 SYN_SENT 状态。 第二次握手 (Server -&gt; Client): 内容: 服务器收到客户端的 SYN 包后，回复一个报文。该报文中 SYN 和 ACK 标志位都置为 1。服务器也选择一个自己的随机初始序列号 seq=y，同时将确认号 ack 设置为 x+1。 目的: 服务器通过 ACK=1 和 ack=x+1 告诉客户端：“你的请求我收到了”。通过 SYN=1 和 seq=y 表明：“我也同意建立连接，这是我的起始序列号”。 状态: 服务器进入 SYN_RCVD 状态。此时，服务器已确认客户端的发送能力正常。 第三次握手 (Client -&gt; Server): 内容: 客户端收到服务器的 SYN-ACK 包后，发送最后一个确认报文。该报文 ACK 标志位置为 1，seq 设置为 x+1，并将确认号 ack 设置为 y+1。 目的: 客户端通过 ACK=1 和 ack=y+1 告诉服务器：“你的回应我已收到，现在我们可以开始通信了”。 状态: 此报文发送后，客户端进入 ESTABLISHED 状态。服务器收到后，也进入 ESTABLISHED 状态。连接正式建立。此时，双方都确认了对方的收发能力正常。 说到这里，我有一个问题：“为什么要交换 seq 和 ack 呢？” 本质是双向确认：TCP 是一个全双工的协议，意味着通信双方都可以同时发送和接收数据。因此，每一方都必须有自己的 seq 号来标记自己发送的数据，也必须有自己的 ack 号来确认收到的对方数据。 三次握手就是交换和确认彼此的初始序列号（ISN）的过程： 第一次握手：客户端发送 SYN 和自己的 seq=x。它在说：“我的初始序列号是 x，你收到了吗？” 第二次握手：服务器回复 SYN、自己的 seq=y 和 ack=x+1。它在说：“我收到了你的 x，所以我确认你的下一个应该是 x+1。同时，我的初始序列号是 y，你收到了吗？” 第三次握手：客户端回复 ACK 和 ack=y+1。它在说：“我收到了你的 y，所以我确认你的下一个应该是 y+1。” 为什么必须是三次握手，而不是两次？ 最核心的原因，是为了防止早已失效的、旧的连接请求突然又送达服务器，从而引发错误。 想象一个网络有些延迟的场景： 客户端发送了第一个连接请求 SYN（我们称之为 请求A），但它在网络中被卡住了，迟迟没有到达服务器。 客户端等了一会儿没收到回应，以为丢包了，于是又发送了一个新的连接请求 SYN（请求B）。 请求B 顺利到达，服务器正常回应，双方通过三次握手建立了连接，传输数据，然后正常关闭了连接。 就在这时，那个被卡了很久的 请求A 终于抵达了服务器。 如果只有两次握手，服务器收到 请求A 后，会误以为是客户端又发起了一个新的连接请求。它会立即分配资源，建立连接，然后傻傻地等待客户端发来数据。但此时的客户端对此一无所知，它根本不会理会服务器的确认，更不会发送任何数据。 结果就是，服务器单方面开启了一个“空连接”，白白浪费了系统资源，直到超时后才关闭。而三次握手，通过增加第三次客户端的最终确认，完美地解决了这个问题。服务器只有在收到客户端对自己的 SYN 的最终 ACK 之后，才会确信这是一个有效的、全新的连接请求。 这个过程可以用下面的时序图来表示： TCP 可靠性的基石 TCP 的可靠性并非单一功能，而是一个由多种机制协同工作的复杂系统。这些机制相互配合，共同确保了数据传输的完整性、有序性、无差错和高效性。 序列号 (Sequence Numbers) 与确认应答 (Acknowledgements, ACK)：TCP 将发送的数据分割成一个个小的数据段（Segment），并为每个字节都分配一个唯一的序列号。接收方收到数据后，会发送一个 ACK 报文作为回应，其中包含一个确认号，告诉发送方“我已经收到了你到哪个序列号为止的所有数据，请从下一个序列号开始发”。这种“有问有答”的机制是保证数据不丢失的基础。 超时重传 (Timeout Retransmission)：如果在发送数据后的一段时间内（这个时间是动态计算的）没有收到对方的 ACK，发送方就会认为数据包可能在路上丢失了，于是会重新发送这个数据包。 流量控制 (Flow Control)：接收方会通过 TCP 头部中的“窗口大小 (Window Size)”字段，告诉发送方自己当前还能接收多少数据。发送方则根据这个窗口大小来调整自己的发送速率，确保不会因为发送过快而导致接收方处理不过来，造成数据溢出。 拥塞控制 (Congestion Control)：流量控制关心的是“点对点”的速率匹配，而拥塞控制则着眼于整个网络的健康状况。TCP 通过一系列算法（如慢启动、拥塞避免等）来探测网络的拥堵程度，并主动调整发送速率，避免因自身流量过大而加剧网络拥堵，最终导致大规模丢包。 四次挥手：礼貌地“挂断电话” 与建立连接同样重要的是，如何安全、完整地断开连接。这个过程被称为“四次挥手”，因为它需要四次信息交换来确保双方的数据都已传输完毕。 第一次挥手 (FIN)：客户端决定关闭连接，向服务器发送一个 FIN 报文，表示“我的数据已经全部发送完毕了”。此时客户端进入 FIN_WAIT_1 状态。 第二次挥手 (ACK)：服务器收到 FIN 报文后，回复一个 ACK 报文，表示“收到了你的关闭请求”。但此时服务器可能还有未发送完的数据，所以它还不能立即关闭连接。此时，服务器进入 CLOSE_WAIT 状态，客户端收到 ACK 后进入 FIN_WAIT_2 状态。 第三次挥手 (FIN)：服务器将所有剩余数据发送完毕后，会向客户端发送一个 FIN 报文，表示“我这边的数据也发完了，可以关闭了”。服务器随之进入 LAST_ACK 状态。 第四次挥手 (ACK)：客户端收到服务器的 FIN 报文后，回复最后一个 ACK 报文进行确认。发送完毕后，客户端会进入 TIME_WAIT 状态，等待一段时间（通常是 2MSL，两倍的最大报文段生存时间）以确保服务器收到了这个 ACK，防止网络中可能存在的延迟报文造成问题。服务器收到 ACK 后则直接进入 CLOSED 状态。至此，连接被完全断开。 一定是挥四次手吗 👋？ 我们可以分别从逻辑上以及行为上来看待“四次挥手”这件事： 从逻辑上看：TCP 协议一定会按照逻辑进行完整的四个过程，所以从这个角度上来看，一定是“四次挥手”。四个过程分别是： 客户端关闭发送通道。 服务器确认客户端关闭发送通道。 服务器关闭发送通道。 客户端确认服务器关闭发送通道。 从行为上看：但是如果抓包的话，你可能会发现只有三个报文段的情况，并且这种情况还不少见。这是因为当第二次挥手时，如果服务器没有剩余要发送给客户端的数据，那么 TCP 就会将第二、三次挥手进行合并，所以最终只有三个报文段。相关逻辑如下图所示： TCP 状态机：连接的生命周期 三次握手和四次挥手描述了 TCP 连接建立和断开的关键时刻。但一个完整的 TCP 连接生命周期，远不止这几个瞬间。它由一系列精确定义的状态组成，这些状态之间的转换共同构成了一个“状态机 (State Machine)”。这个模型清晰地展示了从连接的萌芽到最终消亡的全过程。 下面的流程图描绘了 TCP 中所有状态以及它们之间可能的转换： 这张图看起来复杂，但它其实是将我们之前讨论的握手和挥手过程，以及一些中间状态，串联成了一幅完整的地图。我们可以将这些状态归为几类来理解： 连接建立: LISTEN: 仅存在于服务端。当服务器应用程序调用 listen() 函数后，进入此状态，表示已准备好接收来自客户端的连接请求。一旦收到客户端的 SYN 报文，将发送 SYN+ACK 并进入 SYN_RCVD 状态。 SYN_SENT: 客户端在调用 connect() 函数后，发送 SYN 报文请求建立连接，随即进入此状态。在此状态下，客户端等待接收服务器的 SYN+ACK 报文。如果收到，则发送 ACK 并进入 ESTABLISHED 状态；如果超时未收到，则会重传 SYN 报文。 SYN_RCVD: 服务端在 LISTEN 状态下收到客户端的 SYN 报文后，会发送 SYN+ACK 报文并进入此状态。在此状态下，服务端等待接收客户端的最终 ACK 报文。一旦收到 ACK，连接即建立，进入 ESTABLISHED 状态。 数据传输: ESTABLISHED: 连接已成功建立，双方可以自由地进行双向数据传输。这是 TCP 连接最主要、最活跃的状态，在三次握手完成后进入此状态。 连接断开: FIN_WAIT_1: 主动关闭方（即发起关闭连接的一方，可能是客户端也可能是服务端）发送 FIN 报文后进入此状态。在此状态下，主动关闭方等待接收对方对 FIN 报文的 ACK。一旦收到 ACK，则进入 FIN_WAIT_2 状态。 CLOSE_WAIT: 被动关闭方（即收到对方 FIN 报文的一方）在收到 FIN 报文后进入此状态。此时，TCP 层已经接收到对方关闭发送通道的请求，并向应用层报告连接已中断。在此状态下，被动关闭方会等待本地应用层处理完所有剩余数据并调用 close() 函数，然后发送自己的 FIN 报文，进入 LAST_ACK 状态。 FIN_WAIT_2: 主动关闭方在收到对方对其 FIN 报文的 ACK 后进入此状态。此时，主动关闭方已经完成了数据发送，并且也收到了对方对其关闭请求的确认。在此状态下，它将等待接收被动关闭方发送的 FIN 报文。一旦收到对方的 FIN 报文，主动关闭方将发送最终的 ACK 并进入 TIME_WAIT 状态。 LAST_ACK: 被动关闭方在发送完所有剩余数据并发送自己的 FIN 报文后进入此状态。在此状态下，被动关闭方等待接收主动关闭方对其 FIN 报文的最终 ACK。一旦收到此 ACK，连接即完全关闭，进入 CLOSED 状态。 TIME_WAIT: 主动关闭方在收到对方的 FIN 并发送了最后一个 ACK 后进入此状态。这是状态机中一个至关重要的状态。 TIME_WAIT 状态的深意 TIME_WAIT 状态，也常被称为 2MSL 等待状态，是 TCP 可靠性的最后一道屏障。主动关闭连接的一方，在发送最后一个 ACK 后，必须在这个状态停留两倍的 MSL (Maximum Segment Lifetime, 最大报文段生存时间)。MSL 是网络中任何 IP 数据包能够存活的最长时间。 这个等待机制有两个核心目的： 确保最后一个 ACK 报文能够到达对方：如果这个 ACK 在网络中丢失了，对方（处于 LAST_ACK 状态）会因为收不到确认而超时重传 FIN 报文。如果主动关闭方此时已经彻底关闭（进入 CLOSED），它将无法响应这个重传的 FIN，导致对方无法正常关闭。TIME_WAIT 状态的存在，确保了它有足够的时间来处理这种情况，重新发送 ACK，帮助对方顺利关闭。 防止“旧连接”的延迟报文干扰新连接：假设没有 TIME_WAIT，一个连接（例如，源端口 10000 -&gt; 目标端口 80）刚关闭，马上又用完全相同的四元组（源IP、源端口、目标IP、目标端口）建立了一个新连接。此时，如果前一个连接中迷路的、延迟的数据包突然到达，它可能会被新连接错误地接收，造成数据混乱。等待 2MSL 的时间，足以让本次连接中所有在网络中“游荡”的报文段都自行消亡，从而保证新连接的环境是“干净”的。 UDP：轻快的“明信片” 与 TCP 严谨的通话模式截然相反，UDP (User Datagram Protocol, 用户数据报协议) 奉行的是极简主义。你可以把它想象成一张明信片：写好地址、贴上邮票，然后直接投进邮筒。你不会先打电话确认收件人是否在家，也不会收到对方的回信确认。 这种“发完即走”的模式，正是 UDP 的核心。 核心特性：无连接与尽力而为 无连接 (Connectionless)：UDP 在发送数据之前，不需要进行三次握手来建立连接。它直接将数据打包成“数据报 (Datagram)”就发送出去。 尽力而为 (Best-Effort)：UDP 不提供任何可靠性保证。它不保证数据包一定能到达目的地，不保证数据包的顺序，也不会进行流量控制或拥塞控制。如果网络拥堵导致丢包，UDP 不会进行重传。这种看似“不负责任”的特性，正是其速度和效率的来源。 UDP 报文结构 UDP 的极简主义也体现在其报文头部上。它的头部固定只有 8 个字节，开销极小，所有字段一目了然： 字段 英文名称 简称 大小 描述 源端口 Source Port sport 16 bits 标识发送方应用程序的端口号（此字段可选） 目标端口 Destination Port dport 16 bits 标识接收方应用程序的端口号 长度 Length len 16 bits UDP 头部和数据的总长度（以字节为单位） 校验和 Checksum csum 16 bits 用于简单的错误检测（此字段可选） 适用场景 UDP 的高效和低延迟特性，使其在以下场景中备受青睐： 实时通信：在线游戏、视频会议、语音通话（VoIP）、直播等。在这些应用中，最新的数据远比旧数据重要。我们更能容忍画面偶尔的花屏（丢包），也无法接受为了等一个丢失的数据包而导致整个画面卡住。 查询类协议：如 DNS（域名系统）查询。客户端向服务器发送一个简短的查询请求，服务器返回一个简短的响应。这种“一问一答”的模式使用 UDP 效率极高。 广播与多播：当需要向网络中的多个节点发送相同的信息时，UDP 的无连接特性使其非常适合用于广播和多播。 对比与选择：TCP vs. UDP 现在，我们可以通过一个清晰的表格来总结 TCP 和 UDP 的核心差异，这将帮助我们理解在不同场景下该如何做出选择。 特性 TCP (传输控制协议) UDP (用户数据报协议) 连接性 面向连接 无连接 可靠性 可靠 不可靠（尽力而为） 传输效率 慢，开销大 快，开销小 头部大小 至少 20 字节 固定 8 字节 控制机制 流量控制、拥塞控制 无 应用场景 网页(HTTP/S)、文件传输(FTP)、邮件(SMTP) 视频会议、在线游戏、DNS、直播 选择的艺术：从上表可以看出，TCP 和 UDP 之间没有绝对的优劣之分。它们是为解决不同问题而设计的两种工具。选择哪种协议，完全取决于应用场景对可靠性和实时性的权衡。如果你的应用（如银行转账）绝不能容忍任何数据差错，那么 TCP 是不二之选；如果你的应用（如在线游戏）更看重实时反馈，可以容忍偶尔的数据丢失，那么 UDP 将是更明智的选择。 新的挑战与未来：QUIC 既然 TCP 如此可靠，为什么像 HTTP/3 这样的现代协议反而开始转向基于 UDP 构建？这引出了 TCP 一个长期存在的痛点。 在系列第五篇《计算机网络之五 - HTTP 与 HTTPS》中我们提到，HTTP/2 虽然通过多路复用技术，解决了应用层的队头阻塞，但它无法解决其底层 TCP 协议自身的队头阻塞 (Head-of-Line Blocking) 问题。在一条 TCP 连接中，如果一个数据包丢失了，那么后续所有的数据包（即使已经到达）都必须排队等待，直到那个丢失的包被成功重传。对于高并发的现代 Web 应用来说，这是一个巨大的性能瓶颈。 为了从根本上解决这个问题，QUIC (Quick UDP Internet Connections) 协议应运而生。它是一个构建在 UDP 之上的、全新的传输层协议。 QUIC 的设计非常巧妙，它相当于在 UDP 的“快车道”上，重新实现了一套现代化的可靠传输机制： 内置多路复用：QUIC 的流是独立的，一个流的丢包完全不会影响其他流的传输，从根本上解决了队头阻塞。 更快的连接建立：它将 TCP 的三次握手和 TLS 的加密握手过程合并，大大减少了建立安全连接所需的往返时间。 更好的拥塞控制：拥有比传统 TCP 更先进的拥塞控制算法。 QUIC 代表了传输层协议的未来演进方向，它试图将 TCP 的可靠性与 UDP 的低延迟优势集于一身，为下一代互联网应用提供更坚实的基础。 总结 TCP 和 UDP 是互联网传输层最核心的两个协议，它们各自代表了一种截然不同的设计哲学。 TCP 如同一位严谨的工程师，通过三次握手、序列号、确认应答、超时重传等一系列复杂机制，构建了一个几乎万无一失的可靠数据通道。它的座右铭是：“宁可慢，不出错”。 UDP 则像一位追求极致速度的信使，它卸下了所有保证可靠性的“包袱”，以最轻量、最直接的方式投递数据。它的信条是：“天下武功，唯快不破”。 正是这两种协议的差异化设计与共存，才共同支撑起了我们今天这个既需要高度可靠（如在线交易）又需要极致实时（如视频直播）的、丰富多彩的互联网世界。 ","link":"https://blog.rakuyoo.top/computer-network-tcp-and-udp/"},{"title":"计算机网络之五 - HTTP 与 HTTPS","content":"本系列的前几篇文章中，我们已经探讨了设备如何通过 IP 地址获得身份、如何通过 DNS 将域名解析为地址，以及如何通过 DHCP、NAT 和 VPN 等技术接入并穿梭于复杂的网络环境。至此，我们的设备已经具备了“定位”并“连接”到目标服务器的能力。 本文将深入探讨这个连接建立之后，应用程序之间进行通信所使用的“语言”——HTTP 协议，以及其安全版本 HTTPS。这两种协议构成了现代万维网的基石。 本系列其余几篇的目录： 计算机网络之一 - IP 与端口 计算机网络之二 - URL 与 DNS 计算机网络之三 - DHCP 与内网穿透 计算机网络之四 - 虚拟网卡与 WireGuard 计算机网络之六 - 可靠的 TCP 与高效的 UDP 计算机网络之七 - OSI 与 TCP/IP 分层模型 前置知识说明：在阅读本文时，您会遇到 TCP 和 UDP 这两个概念。它们是网络通信的两种基础“运输方式”，我们将在本系列的下一篇文章中深入探讨。为方便您理解本文，您可以暂时将它们想象为： TCP：一种可靠的“电话通话”。在通话前，双方必须先建立连接，确保线路通畅，通话过程中能保证数据按顺序、不丢不漏地送达。 UDP：一种简单的“明信片”。它直接将数据包发出，不保证一定或按序到达，但胜在快捷高效。 本文中提到的 HTTP/1.1 和 HTTP/2 都构建在可靠的 TCP 之上，而最新的 HTTP/3 则创造性地选择了 UDP 作为基础。带着这个初步印象，您就可以顺利地理解下文内容了。 HTTP：构建 Web 的无状态协议 HTTP (HyperText Transfer Protocol，超文本传输协议) 是一种用于分布式、协作式和超媒体信息系统的应用层协议。它是万维网数据通信的基础。 核心特性 基于“请求-响应”模式：通信由客户端发起。客户端向服务器发送一个 HTTP 请求，服务器在处理后返回一个 HTTP 响应。 无状态 (Stateless)：协议本身不保存任何关于过去请求的信息。服务器处理每个请求时，都认为它是一个全新的、独立的事务，不依赖于之前的请求。您可以将它想象成一个记忆力很短的接待员，处理完你的当前请求后，立刻就会忘记你是谁。为了让它记住你，就需要 Cookies 这样的“身份牌”来帮忙。 灵活可扩展：通过请求头和响应头，HTTP 允许传输任意类型的数据，不仅仅是 HTML，还包括图片、视频、JSON 等。其基础交互模型如下所示： HTTP 报文结构 HTTP 通信由纯文本的报文（Message）组成，分为请求报文和响应报文。 不知道你有没有想过为什么“message”被翻译成了“报文”？其实是因为前辈们沿用了“电报”时代的相关概念，那时“报文”一词就已经蕴含了“通过电子信号传递的、有特定格式的文书”的意义。另外，“报”字蕴含了遵循协议的规范性，而“文”字则强调了其内容与格式的结构化，如同一份严谨的文书。所以最终这个概念的中文被译为了“报文”。 至于为何英语沿用通用的“message”而非更专业的词，是因为“message”本就是指代通信内容的基础词，而“telegraph”（电报）则指通信系统本身，而非内容单元。英语习惯用限定词（如 HTTP message）来明确上下文，无需替换基础词。这是两种语言在技术术语演变上的路径差异。 请求报文 (Request Message) 一个典型的 HTTP GET 请求报文结构如下： GET /path/to/resource/index.html HTTP/1.1 Host: www.example.com User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) ... Accept: text/html,*/* Accept-Language: en-US,en;q=0.5 它由三部分构成： 请求行 (Request Line)：包含请求方法（GET）、请求资源的路径（/path/...）和 HTTP 协议版本（HTTP/1.1）。 请求头 (Headers)：以键值对形式提供关于请求的元数据，或客户端自身的信息。例如 Host 指定了目标服务器的域名，User-Agent 描述了客户端的类型。 请求体 (Body)：对于 GET 请求，请求体为空。对于 POST 或 PUT 请求，这里会包含需要提交给服务器的数据，例如表单信息或 JSON 数据。 常见的请求方法（Method）有： 方法 描述 GET 请求获取指定资源。 POST 向指定资源提交数据，请求服务器进行处理（例如提交表单或上传文件）。 PUT 用请求中的数据体替换目标资源的全部当前内容。 DELETE 请求服务器删除指定的资源。 HEAD 与 GET 类似，但服务器在响应中只返回头部，不返回实体主体内容。 响应报文 (Response Message) 服务器在收到并处理请求后，会返回一个响应报文： HTTP/1.1 200 OK Date: Mon, 18 Aug 2025 10:00:00 GMT Content-Type: text/html; charset=UTF-8 Content-Length: 1270 &lt;!DOCTYPE html&gt; &lt;html&gt; ... &lt;/html&gt; 它同样由三部分构成： 状态行 (Status Line)：包含 HTTP 协议版本、状态码（200）和状态描述（OK）。 响应头 (Headers)：提供关于响应的元数据，如 Content-Type 指明了响应体的媒体类型。 响应体 (Body)：实际返回的资源内容，例如 HTML 文档。 状态码是服务器对请求处理结果的标准化表示，常见的有： 有一个很好玩的小网站分享给大家：https://http.cat/ ，用可爱的猫猫表示 HTTP 状态码。 状态码 名称 类别 描述 200 OK 成功 请求已成功。 201 Created 成功 请求成功，并因此创建了一个新的资源。 204 No Content 成功 服务器成功处理了请求，但没有返回任何内容。 301 Moved Permanently 重定向 请求的资源已被永久移动到新位置。 302 Found 重定向 请求的资源被临时重定向到另一个 URI。 304 Not Modified 重定向 用于缓存。表示资源未被修改，客户端可以继续使用已缓存的版本。 400 Bad Request 客户端错误 由于语法无效，服务器无法理解该请求。 401 Unauthorized 客户端错误 请求要求身份验证。客户端需要提供凭据。 403 Forbidden 客户端错误 服务器理解请求，但拒绝执行。与 401 不同，身份验证无法改变结果。 404 Not Found 客户端错误 服务器找不到请求的资源。 405 Method Not Allowed 客户端错误 服务器知道请求的方法，但目标资源不支持该方法。 429 Too Many Requests 客户端错误 用户在给定的时间内发送了太多的请求（速率限制）。 500 Internal Server Error 服务器错误 服务器遇到了一个未曾预料的状况，导致其无法完成对请求的处理。 502 Bad Gateway 服务器错误 作为网关或代理的服务器，从上游服务器收到了无效的响应。 503 Service Unavailable 服务器错误 服务器当前无法处理请求（由于超载或停机维护）。 为了更直观地对比，下表总结了请求报文与响应报文的核心区别： 构成部分 请求报文 (Request) 响应报文 (Response) 起始（行） 格式：方法 URI HTTP版本示例：GET /index.html HTTP/1.1 格式：HTTP版本 状态码 描述示例：HTTP/1.1 200 OK 首部（头） 描述客户端信息、请求的资源等。示例：Host, User-Agent 描述服务器信息、返回内容的类型等。示例：Content-Type, Server 正文（体） 通常用于 POST 等方法，携带提交给服务器的数据。GET 请求通常为空。 包含返回给客户端的实际资源内容，如 HTML 文档、JSON 数据等。 HTTP 的版本演进与协商机制 HTTP 协议并非一成不变，它在过去几十年中经历了数次重要迭代，以适应日益复杂的 Web 应用和用户对性能的更高要求。 HTTP/1.0 与 HTTP/1.1 HTTP/1.0 (1996)：作为早期版本，其主要特点是短连接。每次请求都需要建立一个新的 TCP 连接，请求完成后立即断开，效率低下。 HTTP/1.1 (1999)：这是统治了互联网近 20 年的经典版本。它最重要的改进是引入了持久连接（Keep-Alive），允许在同一个 TCP 连接上发送多个请求，极大地减少了连接建立的开销。但它也存在一个著名的问题——队头阻塞（Head-of-Line Blocking），即在一个连接上，前一个请求的响应必须完全接收后，后续的请求才能被处理，导致效率瓶颈。 HTTP/2 (2015) HTTP/2 的目标是彻底解决 HTTP/1.1 的性能问题，其核心改进包括： 二进制分帧：将所有传输的信息分割为更小的消息和帧，并对它们采用二进制格式编码，解析效率更高。 多路复用 (Multiplexing)：这是 HTTP/2 最具革命性的特性。它允许在单个 TCP 连接上同时、并行地发送和接收多个请求和响应，彻底解决了队头阻塞问题。 头部压缩 (HPACK)：使用特定算法压缩冗余的请求头和响应头，减少了传输的数据量。 HTTP/3 (2022) HTTP/3 的演进更进一步，它将底层的传输协议从 TCP 换成了 QUIC。 基于 QUIC：QUIC 是一个基于 UDP 的新传输协议，它整合了 TCP 的可靠性、TLS 的安全性以及更多新特性。 解决 TCP 队头阻塞：HTTP/2 虽然解决了应用层的队头阻塞，但无法解决 TCP 协议本身的队头阻塞（一个数据包丢失，整个连接都要等待重传）。QUIC 基于 UDP，一个流的丢包不会影响其他流，从根本上解决了这个问题。 更快的连接建立：QUIC 将 TCP 和 TLS 的握手过程合并，减少了建立连接所需的往返时间（RTT），让连接更快。 版本协商：客户端与服务器如何“对暗号”？ 既然有这么多版本，客户端和服务器是如何确定使用哪个版本的呢？这个过程被称为协议协商。 HTTP/2 的协商：这是一个有趣的地方。虽然 HTTP/2 的协议规范本身并不强制加密（存在明文的 h2c 模式），但所有主流浏览器都达成了一个共识：只支持通过 HTTPS 运行的 HTTP/2。这既是为了推动全网加密，也是为了绕过可能不支持新协议的旧网络设备。因此，在浏览器实践中，HTTP/2 的协商几乎总是通过 TLS 的一个扩展 ALPN (Application-Layer Protocol Negotiation) 来实现。在 TLS 握手时，客户端会告诉服务器它支持的协议列表（如 [&quot;h2&quot;, &quot;http/1.1&quot;]），服务器则从中选择一个它也支持的最高版本，并在 TLS 握手完成时通知客户端。 HTTP/3 的协商：由于 HTTP/3 运行在不同的协议（UDP）上，它的发现机制有所不同。通常，服务器会通过 HTTP/1.1 或 HTTP/2 的响应头 Alt-Svc (Alternative Service) 来“广播”自己的 HTTP/3 服务地址。例如，Alt-Svc: h3=&quot;:443&quot;。浏览器收到后，就会尝试在后台建立一个到该地址的 QUIC 连接，并在后续请求中升级到 HTTP/3。 HTTPS：为对话加上安全封印 HTTP 协议本身是明文传输的，这意味着任何在传输路径上的中间节点（如路由器、ISP）都能轻易地读取、甚至篡改通信内容。这带来了三大安全风险： 窃听风险 (Eavesdropping)：通信内容可能被第三方窃听。 篡改风险 (Tampering)：通信内容在传输过程中可能被修改。 冒充风险 (Impersonation)：无法验证通信对方的真实身份，可能连接到伪造的服务器。 为了解决这些问题，HTTPS (HTTP Secure) 应运而生。 HTTPS 的核心 HTTPS 并非一个全新的协议，它的结构是 HTTPS = HTTP + TLS/SSL。它在标准的 HTTP 应用层和 TCP 传输层之间，增加了一个加密、认证和完整性保护的 TLS/SSL 安全层。 SSL (Secure Sockets Layer) 是 TLS (Transport Layer Security) 的前身，由于存在安全漏洞现已弃用。 我们目前实际使用的都是 TLS 协议，但出于历史习惯，人们常将二者并称为 TLS/SSL。 这个安全层主要提供三大核心安全保障： 加密 (Encryption)：确保通信内容无法被窃听。有趣的是，这里使用的是对称加密。因为非对称加密虽然更灵活，但计算开销巨大，速度过慢，不适合加密海量的应用数据。而对称加密速度极快，正适合此场景。 身份验证 (Authentication)：通过数字证书，客户端可以验证服务器的身份，确保连接到的是预期的、真实的服务器，防止冒充。 数据完整性 (Integrity)：通过消息认证码（MAC），确保数据在传输过程中没有被篡改。 TLS 握手机制 当浏览器访问一个 HTTPS 网站时，在真正发送 HTTP 请求之前，它必须先与服务器完成一次 TLS 握手，以建立一个安全的信道。这个过程在概念上可以分为几个步骤，其交互流程可通过以下序列图来描述： 客户端问候 (Client Hello)：客户端向服务器发起连接，并发送它支持的加密算法列表、TLS 版本等信息。 服务器响应与身份证明 (Server Hello &amp; Certificate)：服务器从中选择一套双方都支持的加密算法，然后将其数字证书（包含了服务器的公钥和由权威证书颁发机构 CA 的签名）发送给客户端。 客户端验证与密钥交换（非对称加密的应用）： 客户端验证服务器证书的有效性（是否由受信任的 CA 签发、是否过期、域名是否匹配等）。 验证通过后，客户端生成一个用于本次会话的“暗号”（pre-master secret），然后使用从证书中获取的服务器公钥对其进行加密（非对称加密），并发送给服务器。 生成会话密钥与建立安全信道：服务器使用自己的私钥解密，是全网唯一能获取该“暗号”的实体。随后，客户端和服务器使用这个共享的“暗号”，通过相同的算法，各自生成出完全一样的对称会话密钥。握手完成。 安全通信：后续所有的 HTTP 数据都将使用这个对称密钥进行加密和解密，实现安全的通信。 在这个流程中，非对称加密（公钥/私钥）被巧妙地用于安全地协商和交换对称加密所使用的密钥，兼顾了安全性和性能。这就像用一把非常复杂、开锁很慢但无法被复制的钥匙（非对称加密），去打开一个保险箱，从里面取出一把开门又快又方便的普通钥匙（对称加密），然后锁上保险箱，以后进出都用这把普通钥匙。 TLS 握手与“三次握手”的区别 需要明确的是，TLS 握手不是我们常说的 TCP“三次握手”。这是一个非常常见但重要的混淆点，它们是两个发生在不同阶段、目的也完全不同的过程： 三次握手 (TCP Handshake)：发生在传输层，目的是建立一个可靠的通信连接，确保双方都准备好了收发数据。它不关心内容是否加密。 TLS 握手 (TLS Handshake)：发生在 TCP 连接建立之后，目的是在已有的可靠连接上，再建立一个安全的加密信道，协商后续通信要使用的加密密钥。 简单来说，先有 TCP 三次握手建立起“物理通话线路”，再有 TLS 握手在这条线路上约定“加密暗号”。 融会贯通：连接系列知识 HTTP/HTTPS 作为应用层协议，其运行离不开底层网络设施的支持。 协议、端口与 DNS 的协同工作：在第二篇文章中我们了解到，URL https://www.example.com 中的 https 部分，正是告诉浏览器需要通过 443 端口（回顾第一篇）连接到由 DNS 解析出的 IP 地址，并启动上文所述的 TLS 握手流程。 应用层加密与网络层加密：HTTPS vs. WireGuard：在第四篇文章中，我们深入了 WireGuard。现在可以清晰地对比二者的区别： HTTPS 在应用层工作，它保护的是单个应用程序（如浏览器）与单个目标服务器之间的通信内容。它不关心数据包如何路由，只关心应用数据的安全。 WireGuard 在网络层工作，它保护的是设备与VPN 服务器之间的所有 IP 流量。一旦连接，无论是 HTTP 请求、DNS 查询还是其他任何网络活动，都会被封装在加密隧道中。它提供的是全面的网络链路级安全。 总结 HTTP 定义了 Web 内容交换的基本规则，而 HTTPS 则为其增加了至关重要的安全保障。理解这两种协议的原理、结构以及它们在整个网络协议栈中的位置，是理解现代互联网运作方式的关键。它们与底层的 IP、DNS、TCP/UDP 等协议协同工作，共同构成了我们每天都在使用的、复杂而又高效的全球信息网络。 ","link":"https://blog.rakuyoo.top/computer-network-http-and-https/"},{"title":"将 ARouter 的思想带入 Swift：用声明式语法构建类型安全的 API 请求","content":"最近读到喵神的《编译器，靠你了！使用类型改善状态设计》，深受启发。这篇文章让我意识到，我在设计 RaAPIWrapper 时，为了解决一些工程问题而采用的方案，在不经意间与“类型即状态”的设计思想不谋而合。 我之前还没有正式介绍过这个库，所以借此机会，正式与大家分享 RaAPIWrapper 的设计，特别是它为了解决特定问题而采用的架构，以及这些决策在事后看来，是如何“碰巧”实现了更先进的设计模式。 RaAPIWrapper 的诞生 RaAPIWrapper 的设计初衷，源于对现有网络库 Moya 的反思，以及对 Android 路由框架 ARouter 设计模式的借鉴。 为何不直接用 Moya？ 在 Swift 社区，Moya 是一个非常优秀的类型安全网络库，它通过 enum 和 protocol 的组合，解决了大量 API 管理中的问题。然而，在深度使用后，我们发现 Moya 的设计在某些方面存在优化空间。 Moya 的核心是 TargetType 协议，一个 API 的完整定义需要你在 enum 的 switch 中，为 path, method, task 等多个属性分别提供实现。这导致了一个问题： 一个 API 的核心信息（如请求方法、路径、参数）被分散在了多个代码块中。 当项目 API 数量增多时，开发者需要在一个庞大的 switch 语句中不断上下文切换和上下跳转，才能拼凑出一个 API 的完整样貌。这降低了代码的可读性和维护性，也是 RaAPIWrapper 希望解决的核心痛点。 不过需要强调的是，RaAPIWrapper 的目的并非全盘否定 Moya，当前也不具备替代 Moya 的能力。Moya 依然是一个功能强大的网络执行层，尤其在插件化、请求执行等领域。RaAPIWrapper 的设计初衷是专注于解决 API 定义的内聚性问题，它被设计为一个上层的“API 定义层”，可以与像 Moya 这样的成熟网络库协同工作，形成优势互补。 ARouter 是什么？ ARouter 是 Android 生态中一个著名的路由框架，由阿里巴巴开发并开源。该框架通过注解 (@Route) 的方式，让开发者能够以一种极其声明式和解耦的方式定义页面路径。在 README 中有如下的示例： @Route(path = &quot;/test/activity&quot;) public class Test1Activity extends Activity { ... } 我非常欣赏这种将“路由”从具体的实现中抽离出来，变成一个集中、静态、易于管理的设计。 于是，一个想法在我脑中萌生：我能否将这种优雅的、基于注解的声明式语法，从“页面路由”领域迁移到“API 定义”领域？ 这个想法，成为了 RaAPIWrapper 的起点。我的目标是创建一个网络层，它应该具备以下特质： API 声明化：API 的定义应该像 ARouter 的路由一样，通过类似注解的语法附加到一个静态属性上，而不是散落在各个业务代码中去手动拼接 URL。 集中化管理：所有单个 API 定义所需要的内容都应该被清晰地组织在一起，不会散落在多个地方，查找起来要非常方便。 定义与实现解耦：API 的“定义”应该与其“如何被请求”完全分离。定义部分只关心 API 的元数据（路径、方法、参数结构），而底层的网络请求库（是 Alamofire 还是 URLSession）可以随时被替换，而不影响定义。 像定义路由一样定义 API 为了实现上述目标，我将 RaAPIWrapper 的核心架构设计为一个声明式的 API “地址簿”： 项目带有一个用于演示的 Playground，您可以通过该项目进一步了解本框架。 @GET(&quot;/api/v1/no_param&quot;) static var noParamAPI: APIParameterBuilder&lt;()&gt;? = nil @POST(&quot;/api/v1/tuple_param&quot;) static var tupleParamAPI: APIParameterBuilder&lt;(id: Int, name: String?)&gt;? = .init { // `Dictionary` and `Array` can be used directly as parameters. [&quot;id&quot;: $0.id, &quot;name&quot;: $0.name] } @POST(&quot;/post&quot;) static var postWithModel: APIParameterBuilder&lt;Arg&gt;? = .init { // When the parameter `Arg` complies with the `APIParameter` (`Encodable &amp; Hashable`) protocol, // it can be used directly as a parameter. $0 } 就像 ARouter 的 @Route(&quot;/path/to/page&quot;) 一样，最核心的信息应该在声明的那一刻就一目了然。这种方式的优势是显而易见的： 高度可读：你可以像 Moya 一样将多个接口定义在一个文件中，任何人打开这个文件，就能立刻了解模块中有哪些可用的 API。 高内聚性：您不用再通过多次跳转，才能获取到一个接口的全部信息，所有信息都定义在一起。 易于维护：修改、添加或废弃一个 API，都只需要在这个“地址簿”中进行，而不需要深入到业务逻辑中去寻找那些零散的 URL 字符串。 天然解耦：@GET 和 @POST 的定义，只包含了 API 的元信息。它本身并不关心网络请求是如何发出的。底层的实现被 RaAPIWrapper 的执行层所封装，这为后续替换网络库或增加统一的请求处理逻辑（如加密、加签）提供了极大的便利。 核心设计：一个声明式的 API 定义 在确立了“声明式、高内聚低耦合”这些核心设计理念之后，我需要寻找一些足够强大的技术手段来支撑它。@propertyWrapper 属性包装器与 Swift 的泛型系统正是实现这些理念的最佳方案。 最初我使用泛型，只是想找到一种方法来避免 switch 语句和增强代码的扩展性，但最终的成果却意外地与“类型即状态”思想相契合。这对我来说，也是一个在实践中学习和发现的过程。 @propertyWrapper 属性包装器 @GET、@POST 是基于一个名为 API 的通用属性包装器（@propertyWrapper）构建的。 我们以 @POST 为例，它的定义非常简单，只是一个类型别名（typealias）： // file: Sources/Core/Wrapper/HTTPMethod/POST.swift public enum PostHTTPMethod: APIHTTPMethodWrapper { public static var httpMethod: APIHTTPMethod { &quot;POST&quot; } } /// Encapsulates the data needed to request the `POST` api. public typealias POST&lt;Parameter&gt; = API&lt;Parameter, PostHTTPMethod&gt; 真正的核心在于 API 这个类。它是一个泛型属性包装器，接受两个类型参数：Parameter 代表 API 的参数类型，HTTPMethod 则是一个遵循 APIHTTPMethodWrapper 协议的类型，用于提供具体的 HTTP 请求方法（如 &quot;POST&quot;）。 // file: Sources/Core/Wrapper/API.swift @propertyWrapper public class API&lt;Parameter, HTTPMethod: APIHTTPMethodWrapper&gt; { public var wrappedValue: APIParameterBuilder&lt;Parameter&gt;? public let path: String // ... other properties public init( wrappedValue: APIParameterBuilder&lt;Parameter&gt;?, _ path: String, // ... other parameters ) { self.wrappedValue = wrappedValue self.path = path // ... } public var projectedValue: API&lt;Parameter, HTTPMethod&gt; { self } } @propertyWrapper 的设计要点在于： 初始化时捕获信息：当我们写下 @POST(&quot;/post&quot;) 时，Swift 编译器会调用 API 的 init(wrappedValue:_:) 方法。路径 &quot;/post&quot; 作为参数被传入，并保存在 path 属性中。wrappedValue 则是我们赋给静态变量的 APIParameterBuilder 实例，它包含了参数的构建逻辑。 分离“定义”与“执行”：API 包装器通过 projectedValue 暴露了自身。这意味着，当我们使用 $ 符号（如 BasicAPI.$postWithModel）时，我们获取到的是 API 这个包装器对象本身。这个对象已经包含了构建一个完整请求所需的所有信息：路径、HTTP 方法、以及参数构建器。这就实现了 API “定义”与“执行”的分离——定义时只关心元数据，执行时通过 $ 获取到一个可执行的请求对象。 通过这种方式，@propertyWrapper 不仅提供了声明式的语法，更在底层完成了信息的聚合与封装，是实现 API 高内聚设计的关键。 “类型驱动”思想 如果说 @propertyWrapper 提供了骨架，那么泛型系统则为这个骨架提供了核心的类型安全保障。如今回顾，我才意识到这个为解决扩展性问题而选择的方案，其本质恰好就是“类型即状态”思想的体现，并且这一思想同时运用在了 API 的两个泛型参数 HTTPMethod 和 Parameter 之上。 我们再次审视 API 的定义：API&lt;Parameter, HTTPMethod: APIHTTPMethodWrapper&gt;。 首先，HTTPMethod 参数本身并不是一个字符串值（如 &quot;POST&quot;），而是一个类型。例如 PostHTTPMethod 这个类型，它的唯一作用就是通过遵循 APIHTTPMethodWrapper 协议来携带 &quot;POST&quot; 这个字符串。通过 typealias POST&lt;P&gt; = API&lt;P, PostHTTPMethod&gt;，我们将 POST 这个别名与 PostHTTPMethod 这个“状态类型”永久绑定。这样，请求方法就在类型层面被固定下来，杜绝了传入错误字符串的可能性。此外这种写法还有很多好处： 利于扩展，可以很简单的定义多个 HTTP Method。 通过 extension + where PostHTTPMethod = ...，可以针对不同的 HTTP Method 定义不同的请求方法。比如业务层面规定 GET 请求不允许携带参数，那么就可以定义不带参数的 request() 方法。 在业务层需要获取到 HTTP Method 时，可以避免 switch case。 其次，Parameter 参数以一种更动态的方式应用了同样的思想。观察 API 的定义： // API with no parameters @GET(&quot;/api/v1/no_param&quot;) static var noParamAPI: APIParameterBuilder&lt;()&gt;? = nil // API with parameters @POST(&quot;/api/v1/tuple_param&quot;) static var tupleParamAPI: APIParameterBuilder&lt;(id: Int, name: String?)&gt;? = .init { ... } 我们并没有在 @GET 或 @POST 中显式指定 Parameter 的类型。这里的机制是 Swift 的类型推断。 当我们声明 static var noParamAPI: APIParameterBuilder&lt;()&gt;? 时，Swift 编译器会分析变量的类型。它看到 APIParameterBuilder&lt;()&gt;，便会推断出 API 包装器的 Parameter 泛型参数就是 ()（即 Void）。 同理，当声明 tupleParamAPI 的类型为 APIParameterBuilder&lt;(id: Int, name: String?)&gt;? 时，编译器就确切地知道，这个 API 的参数类型是一个元组 (id: Int, name: String?)。 这种设计带来了显著的优势： API 合约化：API 的参数类型（它的“状态”）被固化在了定义中，成为一个不可改变的“合约”。 编译期安全：当你调用 API 时，编译器会强制执行这份合约。 对于 $noParamAPI，调用 request() 方法时如果试图传入参数，编译器会直接报错。 对于 $tupleParamAPI，调用 request(with:) 方法时，你必须传入一个 (id: Int, name: String?) 类型的元组，参数名、类型、数量、可选性稍有差池，都无法通过编译。 这与传统 Moya 中使用 [String: Any] 作为参数的方式形成了鲜明对比。字典是类型不安全的，开发者很容易在运行时因为拼错 key、传错 value 类型而导致请求失败。而 RaAPIWrapper 将这种潜在的运行时错误提前到了编译期，这正是该设计的关键优势之一。 最后，APIParameterBuilder&lt;Input&gt; 扮演了类型转换的角色。它接收强类型的 Input（即 Parameter），并通过一个闭包，将其转换为网络层所需的、经过类型擦除的 any APIParameter 格式。这使得 API 的定义可以保持类型纯净，而将参数构建的细节封装在了构建器内部。 回顾与展望 设计的得与失 通过组合运用 @propertyWrapper 的声明式语法和以泛型为核心的类型系统，RaAPIWrapper 成功地将 API 定义从分散、易错的字符串和字典，转变成了集中的、类型安全的 API “地址簿”。这个以提升代码内聚性和可维护性为目标的尝试，最终收获了编译期安全这一关键优势。 不过，正如一些批判性声音指出的，任何一个设计良好的 Swift 强类型 API，都在某种程度上利用了“类型即状态”。RaAPIWrapper 的创新更多在于其语法和模式，而非思想本身。并且这种“事后诸葛亮”的回顾也让我意识到在技术方面我还差的很远，还有很长的路要走。 清晰的定位：API 定义层 更重要的是，必须明确 RaAPIWrapper 的定位是一个 API 定义的包装层（Wrapper），而非一个大而全的网络请求框架。 这一定位意味着它刻意地保持了职责的单一。它只做一件事：以类型安全和高内聚的方式，构建出一个完整的请求元数据。至于这个元数据如何被执行、如何处理回调、如何实现插件、如何管理通用请求头等问题，则完全交由其背后的执行层来处理。 这种关注点分离的设计，恰好回答了“为何不直接用 Moya”的问题： 您可以继续使用 Moya：Moya 就是一个非常出色的执行层。开发者完全可以将 RaAPIWrapper 作为 API “合约”层，将生成的请求数据传递给 Moya Provider，继续享受 Moya 强大的插件系统和请求管理能力。 拥抱变化：如果未来团队希望从 Moya 迁移到 URLSession，或其它新的网络库，您只需要更换执行层的实现，而所有用 RaAPIWrapper 定义的 API 代码都无需改动。 未来规划 目前，RaAPIWrapper 尚不自带执行层，也未集成插件系统。未来的规划是提供一个基于 Alamofire 的官方默认执行层，并逐步完善其功能，使其成为一个更加开箱即用的网络解决方案。 ","link":"https://blog.rakuyoo.top/RaAPIWrapper/"},{"title":"计算机网络之四 - 虚拟网卡与 WireGuard","content":"在上一篇文章的结尾，我们探讨了多种内网穿透技术，并初步介绍了 WireGuard 作为一种现代化 VPN 解决方案的优势。其实在更早之前，我们就亲手编译过 WireGuard 在 iOS 端的 SDK，也使用 WireGuard 实现过群晖 NAS 的公网访问。 本文将以 macOS 环境为基础，深入 WireGuard 内部，从内核交互的底层细节出发，解构虚拟网卡的实现机制，并详细剖析其密码学和路由设计。 本系列其余几篇的目录： 计算机网络之一 - IP 与端口 计算机网络之二 - URL 与 DNS 计算机网络之三 - DHCP 与内网穿透 计算机网络之五 - HTTP 与 HTTPS 计算机网络之六 - 可靠的 TCP 与高效的 UDP 计算机网络之七 - OSI 与 TCP/IP 分层模型 虚拟网卡：用户空间与内核的契约 要理解 WireGuard 的工作原理，首先需要解决一个根本性问题：像 WireGuard 这样的普通应用程序，是运行在操作系统的“用户空间”的，而网络数据包的收发、路由选择等核心功能，都由操作系统的“内核空间”牢牢掌控。那么，一个用户空间的程序，是如何能“拦截”并“处理”本应由内核直接发送到物理硬件的IP数据包呢？ 答案就是虚拟网卡 (Virtual Network Interface)，它是用户空间与内核空间在网络层面达成的一种“契约”。这个核心思想在不同操作系统上是共通的，但具体实现有所差异。 内核的“委托”：utun 接口 在 macOS 上，契约的实现者是 utun 虚拟网络接口。与 Linux 下有明确设备文件路径的 tun 设备不同，utun 接口是由程序在运行时通过向内核发出特定请求来动态创建的。WireGuard 这样的应用会通过系统调用，请求内核创建一个 utun 类型的网络接口，并获得一个用于与其通信的文件描述符（File Descriptor）。 这个文件描述符，就是这份“契约”的实体。它对于应用程序来说，就像一个普通的文件句柄，可以对其进行 read() 和 write() 操作。但它的另一端，连接的却是内核网络协议栈中一个新创建的、功能完整的虚拟网络接口（例如 utun0 或 wg0）。 当这个接口被 ifconfig 命令激活并分配了IP地址（如 10.8.0.1）后，它就会出现在 ifconfig 的输出中，对于内核的路由、防火墙等子系统来说，它与一张真实的物理网卡并无二致。 下面是这个过程的伪代码表示，它被 WireGuard 客户端在后台自动处理了： // 1. 向内核请求创建虚拟接口，获取文件描述符 int vpn_fd = request_virtual_interface(&quot;wg0&quot;); // 2. 使用命令行工具为接口配置IP地址并激活 system(&quot;ifconfig wg0 inet 10.8.0.1/24 up&quot;); // 3. 进入主循环，像读写文件一样处理网络包 while(1) { // 从内核的 wg0 接口读取一个IP包 int nread = read(vpn_fd, packet_buffer, sizeof(packet_buffer)); // ... 对 packet_buffer 进行加密处理 ... // 通过物理网卡的UDP socket发送出去 sendto(udp_socket, encrypted_packet, ...); } 数据包的“契约”履行过程 这份契约的履行过程，即数据包的流转，是理解一切的关键： 下行（发送数据）： 一个上层应用（如浏览器）试图访问 10.8.0.2。 内核根据其主路由表（可通过 netstat -nr 查看），发现目标地址 10.8.0.2 匹配 10.8.0.0/24 dev utun0 这条规则（这条规则通常由 wg-quick 脚本自动添加）。 内核将这个原始的、未经任何修改的IP数据包，作为一个字节流，“写入” 到与 utun0 关联的那个文件描述符中。 WireGuard 进程一直在用户空间通过 read() 系统调用阻塞式地读取这个文件描述符。一旦内核写入数据，WireGuard 进程就会被唤醒，并读出完整的IP数据包。 上行（接收数据）： WireGuard 进程通过物理网卡收到了一个来自对端的、加密的UDP包。 在用户空间完成解密，还原出原始的IP数据包（例如，一个从 10.8.0.2 发往 10.8.0.1 的ICMP响应包）。 WireGuard 进程将这个原始IP数据包的字节流，通过 write() 系统调用 “写入” 到它持有的文件描述符中。 数据被写入后，会立刻出现在内核空间的 utun0 接口上，仿佛它刚从外部网络到达。随后，内核的网络协议栈会接管它，进行后续的路由、分发给上层应用等操作。 为了更直观地理解数据包在用户空间与内核空间之间的流转过程，可以参考下面的序列图： WireGuard 的设计哲学与核心机制 理解了虚拟网卡如何作为用户空间与内核的桥梁后，我们再来深入探讨 WireGuard 本身的设计。其设计哲学体现在其极其简明的配置文件中，通过解读这份文件，我们就能掌握其核心机制。 WireGuard 本质上是无状态和无连接的。它不像 TCP 或 OpenVPN 那样需要维持一个长期的、有明确 “连接/断开” 状态的会话。通信双方仅通过基于 Noise 协议框架的握手来交换最新的会话密钥。如果一段时间没有流量，双方不会有任何通信。一旦有数据需要发送，它会尝试使用现有的会话密钥，如果密钥已过期，则会静默地发起一次新的握手。这种设计对移动设备和不稳定的网络环境极为友好。 一个典型的 WireGuard 配置文件（通常是 wg0.conf）由若干个“块（section）”组成，最核心的就是 [Interface] 块和 [Peer] 块。 [Interface] 块：定义隧道端点 [Interface] 块用于配置隧道的“本地”这一端，也就是你正在配置的这台机器上的 wg0 虚拟网卡。 [Interface] # 本机的私钥 PrivateKey = [私钥内容] # 本机在 VPN 网络中的内网 IP 地址 Address = 10.8.0.1/24 # 监听的 UDP 端口 ListenPort = 51820 PrivateKey：一段 Base64 编码的字符串，是该接口的私钥。这是接口的唯一身份凭证，绝不能泄露。与之配对的公钥（由私钥生成）将分发给其他对端（Peer）。 Address：分配给该接口的虚拟 IP 地址。请注意，这是在 WireGuard 构建的虚拟网络内部的地址，而非机器的物理 IP。我们在本系列的第一篇文章《计算机网络之一 - IP 与端口》中已经详细讨论过 IP 地址的概念。 ListenPort：指定 WireGuard 在哪个物理网络端口上监听来自其他对端的入站连接。这是一个 UDP 端口，因为 WireGuard 完全基于 UDP 协议。同样，关于端口的概念，您也可以回顾第一篇文章。 [Peer] 块：定义通信对端 [Peer] 块定义了该接口希望连接的 “对端” 或 “伙伴”。你可以有多个 [Peer] 块，每一个块代表一个你希望与之通信的节点（例如，一个中心服务器或另一个客户端）。 [Peer] # 对端的公钥 PublicKey = [对端公钥内容] # 允许通过该对端路由的IP地址范围 AllowedIPs = 10.8.0.2/32 # 对端的公网地址和端口（仅客户端需要） Endpoint = server.public.ip.address:51820 # NAT 穿透心跳包（仅客户端需要） PersistentKeepalive = 25 PublicKey：对端的公钥，与对端 [Interface] 中 PrivateKey 相对应。WireGuard 依靠这对密钥来验证对方的身份并加密数据。 Endpoint：对端的公网 IP:端口。这是你的接口将数据包发往的实际网络地址。通常，只有需要主动发起连接的一方（如客户端）才需要配置此项。服务器端因为是被动监听，所以不需要为每个 Peer 指定 Endpoint。 PersistentKeepalive：一个可选的便携功能，用于维持 NAT 映射。它会每隔指定秒数（例如25秒）向对端发送一个“心跳包”，这对于处于 NAT 设备（如家用路由器）后面的客户端非常重要，可以防止连接中断。我们在系列第三篇文章《计算机网络之三 - DHCP 与内网穿透》中探讨过 NAT 的工作原理。 AllowedIPs：这是 WireGuard 最具革命性的设计，也是其 CryptoKey 路由机制的核心。这个参数同时承担了路由和安全两项关键职责： 路由功能（出站）：它告诉本地的 wg0 接口：“当有一个IP包需要发送，如果它的目标IP地址属于 AllowedIPs 的范围，那么就应该通过隧道，加密后发送给这个 [Peer]”。例如，客户端配置 AllowedIPs = 0.0.0.0/0 意味着将所有出站流量都路由到这个 Peer（即中心服务器）。 安全功能（入站）：它定义了一个“白名单”。当从这个 Peer 收到一个解密后的数据包时，WireGuard 会检查其源IP地址。只有当源IP地址属于 AllowedIPs 范围时，这个包才会被接受，否则将被静默丢弃。 这种将密码学身份（公钥）与路由策略（AllowedIPs）紧密绑定的设计，就是 CryptoKey 路由。它极大地简化了传统 VPN 复杂的路由表和防火墙策略，使得网络拓扑完全由密钥和 AllowedIPs 静态定义，清晰且安全。并且如果收到一个无法解密，或者解密后源地址不匹配任何 Peer 的 AllowedIPs 的数据包，WireGuard 会静默丢弃它。这使得 WireGuard 对网络扫描工具来说是“隐形”的。 让我们对 “静默丢弃” 和 “拒绝” 做一下技术层面的比较： 拒绝：当数据包发往一个关闭的端口时，操作系统通常会回复一条 ICMP Port Unreachable 消息。这个响应明确地告知了发送方：端口存在，但是关闭的。 静默丢弃：WireGuard 的行为则不同。它接收所有发到监听端口的数据包，但只处理能够被正确解密和验证的。对于所有无效数据包，它在内部直接丢弃，不产生任何对外响应。对于网络扫描工具而言，这种“无响应”的状态与数据包在传输途中被防火墙拦截或网络波动导致丢失是无法区分的。因此，扫描工具无法确认端口的真实状态，从而大大提升了 WireGuard 的隐蔽性。 密码学基石：固定的现代加密套件 WireGuard 在安全上的一个核心设计是 “固执己见”（Opinionated），这正是其卓越之处。它不像 IPsec 或者 OpenVPN 那样提供一个庞大的、可协商的加密算法列表，而是坚定地选择了唯一一套固定的、最先进的密码学原语。这种“固执”并非限制，而是深思熟虑后的安全与性能保障： “密码学原语” 指的是构建更复杂密码系统时所使用的、标准化的基础算法模块。它们是功能单一的最小单元，每个原语用于解决一个特定的密码学问题，例如“密钥交换”、“对称加密”或“消息认证”。 极致的简洁与可审计性：WireGuard 的核心代码库仅有约 4000 行，而 OpenVPN 和 IPsec 则高达数十万行。这种巨大的差异，很大程度上得益于其固定且精简的密码学套件。代码量越小，就越容易进行安全审计，发现并修复潜在的漏洞，从而大大降低了攻击面。 杜绝配置错误与降级攻击：在传统的 VPN 协议中，用户或管理员需要从一大堆加密算法中进行选择，这极易因配置不当而引入安全漏洞，甚至遭受“降级攻击”（攻击者强制连接使用较弱的加密算法）。WireGuard 则完全规避了这个问题，它只提供一套经过严格审查的、现代且安全的算法组合，从根本上消除了这类风险。 卓越的性能表现：WireGuard 所选用的密码学原语（如 ChaCha20-Poly1305 和 Curve25519）都是为现代 CPU 优化设计的，它们能够以极高的效率完成加密和解密操作，并且天然支持“恒定时间（Constant-Time）”代码，有效抵御旁路攻击。这使得 WireGuard 在吞吐量和延迟方面通常优于 OpenVPN 和 IPsec。 面向未来的安全性：WireGuard 从一开始就拥抱了最新的密码学研究成果，摒弃了那些可能存在历史遗留问题或性能瓶颈的旧有算法。这确保了它在当前乃至可预见的未来都具备强大的安全性。 正是这种对“少即是多”的深刻理解和对现代密码学的坚定选择，让 WireGuard 成为了一个更安全、更快速、更易于部署和维护的 VPN 解决方案。它不仅仅是一个工具，更是一种理念的胜利。 以下是 WireGuard 所采用的固定密码学原语： 作用 原语名称 说明 密钥交换 Curve25519 (ECDH) 基于椭圆曲线迪菲-赫尔曼协议，让通信双方能在一个不安全的网络上，安全、高效地计算出一个用于加密的共享密钥。 对称加密 ChaCha20 一种流式加密算法，用于高速地加密和解密流经隧道的数据。它以其在通用CPU上的卓越性能和高安全性而闻名。 消息认证 Poly1305 这是一个消息认证码（MAC）算法。它为每个数据包生成一个简短的“标签”，用于验证数据的完整性和真实性，确保数据在传输过程中没有被篡改。 哈希 BLAKE2s 一个速度极快且高度安全的哈希函数，在协议的多种场景中用于生成数据的“指纹”，例如用于密钥派生和公钥哈希。 密钥派生 HKDF 基于哈希的密钥派生函数。它能从一个初始的密钥材料（如密钥交换的结果）中，安全地派生出多个用于不同目的的、独立的加密密钥。 这套组合不仅性能卓越，且在设计上就非常适合编写成**“恒定时间（Constant-Time）”**代码（即无论处理什么数据，其运算时间都保持一致），能有效抵抗旁路攻击。更重要的是，它消除了因配置错误或降级攻击导致的安全风险，大大提升了协议的健壮性。 **旁路攻击（Side-Channel Attack）**是一种不直接攻击加密算法数学逻辑的攻击方式，它转而通过分析加密设备在运算时产生的物理“副产品”来窃取信息。 **时序攻击（Timing Attack）**是旁路攻击中最著名的一种。它的核心思想是：精确测量加密操作所花费的时间。如果代码实现不当，处理不同数据时运算时间出现微小差异，攻击者就可能通过分析这些差异，逐步反推出密钥等敏感信息。 当 WireGuard 加入后：重塑数据之旅 在上一篇文章的结尾，我们描绘了一台设备从开机到访问 www.google.com 的标准流程。现在，让我们将 WireGuard（配置为全局隧道模式，即 AllowedIPs = 0.0.0.0/0）加入这个场景，分析启用 WireGuard 后，数据包的流向会发生何种变化。 基础连接（不变）： 路由器就位：路由器通过 DHCP/PPPoE 从 ISP 获取公网 IP。 设备入网：你的电脑通过 DHCP 从路由器获取内网 IP 192.168.1.100、子网掩码和默认网关 192.168.1.1。到此为止，一切都和原来一样。这是建立隧道的基础。 隧道建立（新步骤）： 你在电脑上启动 WireGuard 客户端。 客户端根据配置，通过物理网络（Wi-Fi/以太网）向远端 WireGuard 服务器（拥有公网 IP）发起握手，建立起一条加密隧道。 启动后，wg-quick 脚本会自动修改你电脑的主路由表，添加一条优先级极高的规则，内容是：“所有目标地址为 0.0.0.0/0（即任何地址）的流量，都必须经由 wg0 这个虚拟网卡发送。”这是实现流量转发的核心步骤。 域名解析（路径改变）： 你在浏览器输入 www.google.com。操作系统需要解析域名，于是创建一个 DNS 查询请求。 操作系统查询路由表，发现这个 DNS 请求（无论发往哪个 DNS 服务器）也匹配 0.0.0.0/0 规则。因此，DNS 请求包被直接交给 wg0 虚拟网卡，而不是发往物理网络的默认网关 192.168.1.1。 WireGuard 进程从 wg0 接口读到这个 DNS 包，将其加密，套上一个 UDP 包的外壳，然后通过物理网卡发往 WireGuard 服务器。 WireGuard 服务器收到后解密，代你向公共 DNS（如 8.8.8.8）查询，并将收到的结果加密后，沿隧道发回给你的电脑。 访问公网（二次封装）： 浏览器拿到了 Google 的 IP 地址 142.250.199.68，于是构建一个目标为此 IP 的 HTTP 请求包。 路由决策再次生效：操作系统查询路由表，142.250.199.68 依然匹配 0.0.0.0/0 规则，于是这个 HTTP 包也被直接交给了 wg0 虚拟网卡。 第一次封装（WireGuard）：WireGuard 进程读到这个 HTTP 包，用 ChaCha20 对其加密，并封装成一个发往 WireGuard 服务器公网地址的 UDP 包。这个 UDP 包的源 IP 是你的内网 IP 192.168.1.100。 第二次封装（路由器 NAT）：这个 UDP 包被交给物理网卡，发往局域网的默认网关 192.168.1.1。路由器收到这个 UDP 包后，并不知道里面是什么，它只做一件事：执行 NAT，将 UDP 包的源 IP 从 192.168.1.100 替换为你的家庭公网 IP，然后将其发送到 WireGuard 服务器。 出口：最终，你的请求从 WireGuard 服务器那里进入公共互联网，访问 Google。Google 服务器看到的访问来源是你的 WireGuard 服务器的 IP，而不是你的家庭公网 IP。 通过这个流程，WireGuard 借助虚拟网卡和路由表，在操作系统层面 “劫持” 了所有对外流量，将你的设备完全置于一个安全的、加密的虚拟网络中，彻底改变了数据的流向和你在互联网上的“身份”。下面的序列图详细描绘了这一经过重塑的数据之旅： 总结 WireGuard 以其独特的设计，将复杂的 VPN 技术简化为易于理解和审计的配置。它不仅仅是一个工具，更是学习现代网络原理、密码学应用和内核交互的绝佳范例。 通过虚拟网卡这一巧妙的“契约”，它在用户空间实现了对内核网络流的完全掌控。而其基于公钥的 CryptoKey 路由机制，则为我们提供了一种前所未有的、兼具安全与简洁的网络构建方式。掌握了它，你就拥有了在复杂的公共互联网之上，灵活地构建属于自己的、安全高速的私人网络的能力。 ","link":"https://blog.rakuyoo.top/computer-network-virtual-nic-and-wireguard/"},{"title":"计算机网络之三 - DHCP 与内网穿透","content":"本篇是计算机网络系列的第三篇，主要讲 DHCP 和内网穿透。在探索广阔的外网之前，先把内网相关的内容做个收尾。 其余几篇的目录： 计算机网络之一 - IP 与端口 计算机网络之二 - URL 与 DNS 计算机网络之四 - 虚拟网卡与 WireGuard 计算机网络之五 - HTTP 与 HTTPS 计算机网络之六 - 可靠的 TCP 与高效的 UDP 计算机网络之七 - OSI 与 TCP/IP 分层模型 局域网的自动配置：DHCP 的世界 任何设备要接入网络，首要任务是获取一个有效的网络配置，包括 IP 地址、子网掩码等信息。手动为每台设备配置这些信息不仅效率低下，还极易出错。为此，DHCP（动态主机配置协议）应运而生，它实现了网络参数的自动化分配。 DHCP 的工作流程 (DORA) DHCP (Dynamic Host Configuration Protocol) 是一种基于 UDP 的网络管理协议，其核心工作流程包含四个步骤，通常简称为 DORA 过程： Discover (发现)：客户端设备启动并接入网络后，以广播形式发送一个 DHCP Discover 报文，在物理网段内寻找可用的 DHCP 服务器。 Offer (提供)：所有收到 Discover 报文的 DHCP 服务器，会从其地址池中选择一个可用的 IP 地址，并连同其他网络配置一起，构建一个 DHCP Offer 报文，发送给客户端。 Request (请求)：客户端选择一个 Offer（通常是第一个收到的），然后向网络广播一个 DHCP Request 报文，正式请求使用该地址，并告知所有服务器它的选择。 Acknowledge (确认)：被选择的 DHCP 服务器收到 Request 报文后，执行最终的绑定操作，并发送 DHCP Acknowledge 报文给客户端，正式确认配置生效。租约开始计时。 整个过程可以用下面的序列图来概括： DHCP 在实践中的角色 理论上，DHCP 服务可以由网络中的任何服务器提供。在绝大多数家庭和小型网络中，DHCP 服务器功能都集成在路由器内。这台我们熟悉的设备，其实在扮演着两个截然不同的角色： 对内（LAN）是 DHCP 服务器：它管理着一个私网地址池（如 192.168.1.0/24），为所有接入的电脑、手机等设备分配内网 IP 地址和相关配置。 对外（WAN）是 DHCP 客户端：那么，路由器自身的公网 IP 又是从何而来的呢？很多情况下，它同样是通过 DHCP 获取的。此时，路由器扮演一个客户端，向您的互联网服务提供商（ISP）的 DHCP 服务器发起请求。ISP 的服务器从其管理的公网 IP 地址池中，取出一个地址“租”给您的路由器。 因此，DHCP 是一个通用的地址分配协议，既被 ISP 用来分配公网 IP，也被路由器用来分配私网 IP。当然，除 DHCP 外，ISP 也常用 PPPoE（需要账户密码拨号）等方式来分配公网 IP。 DHCP 的核心：参数详解与实践 DHCP 在 Acknowledge 阶段提供的“大礼包”中，包含了以下至关重要的网络参数。理解这些参数的理论和实践，是掌握网络配置的关键。 IP 地址 设备在网络中的唯一数字标识，用于寻址和数据包路由。DHCP 分配的通常是私有 IP 地址，仅在局域网（LAN）内有效。 子网掩码 (Subnet Mask) 子网掩码是一个 32 位数值，其核心功能是划分 IP 地址的网络部分和主机部分，是设备进行路由决策的基础。 历史背景：从有类地址到子网划分 在早期，IP 地址根据其第一个八位字节被划分为 A、B、C 等有类（Classful）地址，每个类别有固定的网络位和主机位。这种方式缺乏灵活性，常常导致大量 IP 地址被浪费。 例如，早期的有类网络划分如下： A 类地址 (首位为0，范围 1-126): 默认子网掩码 255.0.0.0。一个 A 类网络可容纳超过 1600 万台主机，若分配给仅需数百地址的组织，是巨大的浪费。 B 类地址 (首位为10，范围 128-191): 默认子网掩码 255.255.0.0。一个 B 类网络可容纳 65,534 台主机，对很多组织来说依然过大。 C 类地址 (首位为110，范围 192-223): 默认子网掩码 255.255.255.0。一个 C 类网络仅能容纳 254 台主机，对于需要稍多设备（如 300 台）的组织来说又太小。 正是为了解决这种“要么太大，要么太小”的僵化问题，子网掩码应运而生，它允许网络管理员根据实际需求，灵活地将大网络划分为多个小子网 (Subnet)，极大地提高了 IP 地址的利用率。 核心原理：网络地址与主机地址 子网掩码通过按位与 (bitwise AND) 运算来提取网络地址。这是设备进行路由决策的根本依据。 计算网络地址： 假设 IP 地址为 192.168.1.100，子网掩码为 255.255.255.0 (/24)。 11000000.10101000.00000001.01100100 (IP: 192.168.1.100) &amp; 11111111.11111111.11111111.00000000 (Mask: 255.255.255.0) ------------------------------------ 11000000.10101000.00000001.00000000 (Network ID: 192.168.1.0) 运算的核心在于，子网掩码中为 1 的部分会保留 IP 地址的位，为 0 的部分会将 IP 地址的位清零，从而得到纯粹的网络地址。 那么如何计算主机地址呢？与计算网络地址相对应，我们也可以计算出地址中的主机部分。这需要对子网掩码进行“按位非”（NOT）运算（即 1 变 0，0 变 1），得到反掩码，然后再与原 IP 地址进行“按位与”。 反掩码: NOT 255.255.255.0 -&gt; 0.0.0.255 运算: 192.168.1.100 AND 0.0.0.255 -&gt; 0.0.0.100 这个 100 就是这台设备在该子网中的主机标识。完整的 IP 地址可以看作是网络地址 (192.168.1.0) + 主机地址 (0.0.0.100) 的结果。 CIDR 表示法 在现代网络中，使用无类别域间路由 (CIDR) 表示法来指定子网掩码更为常见。它在 IP 地址后附加一个斜杠和数字（如 /24），表示子网掩码中前 24 位为 1。 网络容量与扩容问题 子网掩码的设定直接决定了网络的容量。例如，/24 (255.255.255.0) 的网络，主机位有 8 位，最多支持 2^8 - 2 = 254 台设备。 当地址池被用尽后，新设备将无法获取 IP 地址。其 DHCP Discover 请求会因服务器无地址可供 Offer 而石沉大海。最终，新设备可能会获取一个 169.254.x.x 的自动专用地址（无法访问互联网），或直接提示 IP 配置失败。 网络设计有一个核心原则：稳定性与确定性。子网掩码作为网络的基础架构定义，其变更属于架构级调整，必须由管理员进行统一规划和手动实施。自动扩容会因无法通知到所有老设备、可能与现有网络规划冲突等原因，造成网络混乱和安全问题，因此在设计上是不被允许的。 默认网关 (Default Gateway) 默认网关是局域网连接到其他网络的“出口”，物理上通常是路由器的内网接口 IP。其核心作用是路由转发。 数据包的旅程：一个完整的故事 为了理解默认网关的真正作用，我们来完整地追踪一个数据包从内网主机 A 到外网服务器 B 的旅程。 主角: 主机 A: 你的电脑，IP 地址 192.168.1.100。 默认网关: 你的路由器，IP 地址 192.168.1.1。 服务器 B: 远端网站服务器，IP 地址 8.8.8.8。 第一步：主机 A 的“思考”—— 我该把信交给谁？ 主机 A 想要发送数据给 8.8.8.8。它首先要解决一个问题：“这个目标是在我的局域网内，还是在互联网上？” 它通过自己的子网掩码 (255.255.255.0) 进行计算，发现 8.8.8.8 并不和自己在同一个网络（192.168.1.0）内。于是，主机 A 得出结论：不能直接把数据发送给目标，必须先把数据交给默认网关，让它负责转发。 第二步：主机 A 的“打包”—— 准备好信封和快递盒 现在主机 A 知道，逻辑上要把数据发给网关 192.168.1.1。但在局域网这个物理世界里，数据传输依靠的是硬件的物理地址（MAC 地址）。 寻找物理地址 (ARP): 如果主机 A 不知道网关的 MAC 地址，它会先通过 ARP (地址解析协议) 在局域网里广播：“请问谁的 IP 是 192.168.1.1？请把你的 MAC 地址告诉我。” 路由器会回应：“是我的，我的 MAC 地址是 AA:BB:CC:DD:EE:FF。” 进行双层封装: 主机 A 现在开始打包数据。这是一个精妙的双层结构： 内层信封 (IP 数据包): 这里写的是最终目标。 收件人 IP: 8.8.8.8 发件人 IP: 192.168.1.100 外层快递盒 (以太网帧): 这里写的是下一跳目标。 收件人 MAC: AA:BB:CC:DD:EE:FF (网关的 MAC 地址) 发件人 MAC: 主机 A 自己的 MAC 地址 第三步：网关的“转发”—— 拆开快递盒，送出信封 主机 A 将这个“快递盒”发送到局域网上。网络中的所有设备都会看到它，但只有 MAC 地址为 AA:BB:CC:DD:EE:FF 的路由器会接收并拆开它。 路由器拆开“外层快递盒”（以太网帧）。 它看到里面的“内层信封”（IP 数据包），发现收件人是 8.8.8.8，不是自己。 于是，路由器执行它的核心功能：路由。它会查询自己的路由表，找到通往 8.8.8.8 的最佳路径（比如指向 ISP 的下一个路由器）。 最后，它将这个原始的 IP 数据包（里面的 IP 地址不变）拿出来，套上一个新的快递盒（新的以太网帧，目标 MAC 地址是下一个路由器的 MAC 地址），然后发往互联网。 这个“拆包-查路-重新打包”的过程会在每一跳的路由器上重复，直到数据包最终抵达服务器 B。下面的序列图清晰地展示了这一旅程： DNS 服务器地址 DHCP 同样会分配 DNS 服务器的地址，使设备能将域名解析为 IP 地址。 DNS 的分配模式：直接下发与代理转发 在实践中，路由器上的 DHCP 服务分配 DNS 时，通常有两种模式： 直接下发 (Passthrough)：路由器将从 ISP 获取到的上游 DNS 地址（如 8.8.8.8）直接分配给内网设备。 代理转发 (Proxy/Forwarder)：路由器将自身 IP (192.168.1.1) 作为 DNS 服务器分配给内网设备。设备向路由器查询，路由器再向上游 DNS 查询，并可缓存结果以加速后续访问。 跨越网关：内网穿透技术 现在，我们的设备已通过 DHCP 在局域网中完美运行。但如果我们想从公司、酒店或任何外部网络，访问这台内网中的设备（如 NAS、Web 服务器），就会遇到 NAT 的阻碍。内网穿透就是解决这个问题的技术集合。 端口映射 (Port Forwarding) 这是最基础和常用的穿透方式。它在路由器上创建一条静态转发规则，将 路由器公网IP:外部端口 收到的访问请求，转发到 内网指定设备IP:内部端口。 与 DHCP 的关键结合点：静态 IP 分配 端口映射的规则是写死的，它需要内网设备的 IP 地址永远不变。但 DHCP 默认分配的 IP 地址是有租期的，可能会变化。 对于大多数家庭用户来说，公网 IP 本身也是动态变化的，这为端口映射带来了另一个挑战，也是后续 VPN 和第三方服务方案的优势之一 解决方案就是使用路由器的 DHCP 静态地址分配 (Static DHCP Lease) 功能，通过 MAC 地址绑定，确保特定设备永远获得同一个 IP 地址。这是实现可靠内网穿透的重要前提。 UPnP (通用即插即用) UPnP (Universal Plug and Play) 是一种更为便捷的穿透技术，它的目标是实现“零配置”的网络连接。 UPnP 是什么？ 它是一系列协议的组合，允许局域网中的设备（如电脑、游戏机、打印机、摄像头）彼此自动发现，并动态地建立网络服务。在内网穿透的场景下，我们主要利用的是它的 IGD (Internet Gateway Device) 协议。 它是如何工作的？ UPnP IGD 可以看作是自动化的端口映射。其工作流程如下： 内网中一个支持 UPnP 的应用程序（如游戏、下载软件）启动后，会先在局域网中寻找支持 UPnP IGD 的网关设备（即你的路由器）。 找到路由器后，应用程序会向路由器发送一个请求，例如：“你好，我是 192.168.1.101，请帮我把你的公网 XXXX 端口，映射到我自己的 YYYY 端口上。” 路由器接收到这个请求后，如果开启了 UPnP 功能，就会自动在自己的 NAT 表中添加这条端口映射规则，无需任何人工干预。 应用程序还可以通过 UPnP 查询路由器当前的公网 IP，以及检查映射是否成功。 便利性与风险 优点：极其方便，用户完全无感，应用程序可以自行完成所有穿透配置，大大提升了联机游戏、P2P 下载等应用的体验。 缺点：存在安全风险。由于该过程是自动的，局域网中的任何程序（包括恶意软件）都有可能利用 UPnP 在你不知情的情况下打开路由器的端口，将内部服务暴露于公网，从而带来安全隐患。因此，很多对安全性要求较高的用户会选择手动关闭路由器的 UPnP 功能。 基于隧道的解决方案：代理与 VPN 当手动配置端口映射不便或不可行时（例如，你没有路由器的管理权限，或者你的 ISP 不提供公网 IP），基于隧道技术的穿透方案便成为了主流选择。它们的本质都是在内网设备与一台公网服务器之间，建立一条稳定、加密的“隧道”，所有流量都通过这个隧道进行转发。 反向代理隧道 (frp, Ngrok) 这类工具的核心思想是暴露单个服务。 原理：你在内网的机器上运行一个客户端，主动连接到一台你部署在公网的 frp 服务端或 Ngrok 的公共服务器。连接成功后，服务端会提供一个公网地址（域名或 IP）和端口，并将所有访问这个公网地址的请求，通过已经建立好的隧道，原封不动地转发给内网的客户端，客户端再将请求发给本地的 Web 服务或 SSH 服务。 适用场景：非常适合临时或长期地将某个特定的 Web 服务、API 或远程桌面分享出去。其工作原理如下图所示： VPN 与 WireGuard VPN 的目标则更为宏大，它致力于构建一个安全的虚拟局域网，将你的所有设备都“拉”进这个虚拟网络中，让它们如同真的在同一个房间里一样可以互相通信。WireGuard 是实现这一目标的现代化、高性能的 VPN 协议。 未来我们将更加详细地介绍 WireGuard，此处先简单介绍一下。 原理：与 frp 类似，你也需要一台拥有公网 IP 的服务器作为所有设备的“汇集点”。所有设备（家里的 NAS、公司的电脑、你的手机）都作为客户端，通过加密隧道连接到这台服务器。成功后，每个设备都会被分配一个全新的、虚拟的 IP 地址（例如 10.0.0.x）。 区别与优势： 全网络访问：一旦连上 WireGuard VPN，你访问的就不再是单个服务，而是整台设备。你可以通过 10.0.0.x 这个虚拟 IP，访问到设备上的任何端口和服务（Web、SSH、文件共享等），就像在本地访问一样。 极高的安全性：你只需要在公网服务器上开放一个 UDP 端口给 WireGuard 使用，就能访问内网设备的所有服务，极大地减小了公网暴露面。 性能卓越与配置简单：WireGuard 以其高性能、低延迟和简洁的配置著称，非常适合个人和团队自建使用。 总的来说，如果你只想快速分享一个应用，frp/Ngrok 这类反向代理隧道更轻便；如果你希望安全、全面地远程管理和访问你的整个内网环境，构建一个 WireGuard VPN 是目前最优的实践之一。下图描绘了这种虚拟网络的拓扑结构： 总结 至此，我们完成了计算机网络系列中关于内网部分的探索。现在，让我们将三篇文章的知识点串联起来，回顾一台设备从开机到成功访问 www.google.com 的完整流程： 路由器就位：路由器开机后，首先作为客户端，通过 DHCP 或 PPPoE 从互联网服务提供商（ISP）处获取到公网 IP 和上游 DNS 服务器地址。 设备入网：你的电脑开机并连接 Wi-Fi，它会立即发送 DHCP Discover 广播。路由器作为 DHCP 服务器响应，为你分配一个内网 IP（如 192.168.1.100）、子网掩码和默认网关（通常是路由器自身地址 192.168.1.1）。 域名解析：你在浏览器输入 www.google.com。操作系统会向路由器（或上游 DNS）发起 DNS 查询，通过层层解析，最终将域名翻译成服务器的公网 IP 地址，例如 142.250.199.68。 访问公网： 电脑通过子网掩码计算，发现目标 IP 142.250.199.68 并不在本地局域网。 它将数据包的目标 IP 设为 142.250.199.68，但数据链路层的目标 MAC 地址则设为默认网关（路由器）的 MAC 地址。 数据包被发往路由器，路由器执行 NAT，将数据包的源 IP 从你的内网 IP 替换为家庭的公网 IP，然后发往互联网。 通过这一系列的流程，你的请求才得以顺利抵达远方的服务器。 ","link":"https://blog.rakuyoo.top/computer-network-dhcp-and-intranet-penetration/"},{"title":"mise: 统一且高效的开发环境管理工具","content":"mise 是一款高性能的开发工具管理器，旨在通过单一工具和单一配置文件，实现对所有开发环境的统一管理。它能有效解决多项目开发中环境配置的复杂性，提升开发效率。 现代开发环境的挑战 在软件开发中，我们频繁地在不同项目间切换，每个项目都可能依赖特定的技术栈与工具链。这导致项目根目录下散落着各种版本配置文件：.nvmrc、.node-version、.python-version、Gemfile、go.mod 等等。同时，还需要 .env 文件来管理环境变量。 每次进入一个新项目，开发者都面临一系列重复性操作： 当前的 Node.js 版本是否符合项目要求？需要手动执行 nvm use 吗？ Python 的虚拟环境激活了吗？ 如何保证团队成员使用完全一致的工具版本，以避免“在我机器上没问题”的窘境？ 这些琐碎的环境管理任务分散了我们的注意力，降低了开发效率。为了应对这一挑战，mise 应运而生。它是一个基于 Rust 构建的高性能开发工具管理器，致力于通过单一工具和单一配置文件，实现对所有开发环境的统一、自动管理。 mise 的核心优势: 单一配置文件: 仅需一个 .mise.toml 文件，即可声明项目所需的所有工具（Node, Python, Go...）及其版本、环境变量和项目脚本。 高性能: 基于 Rust 构建，mise 的运行速度极快，对 Shell 启动和目录切换（cd）的性能影响微乎其微。 功能全面: mise 不仅是版本管理器，还集成了环境变量管理和任务运行器，一个工具覆盖开发中的多个核心环节。 无缝兼容: mise 能自动识别并使用项目中已有的 .nvmrc, .python-version 等传统配置文件，允许团队平滑过渡。 安装与激活 体验 mise 的强大功能，只需简单的两步。 安装 mise 在终端中执行以下命令完成安装： curl https://mise.run | sh 该命令会将 mise 安装到 ~/.local/bin/mise。 激活 mise (关键步骤) 为了让 mise 能够自动管理你的环境，需要将其挂载（Hook）到你的 Shell 中。此步骤至关重要，否则 mise 将无法在目录切换时自动切换环境。 根据你使用的 Shell，选择对应的命令执行，它会将激活脚本添加到你的 Shell 配置文件中： Bash: echo 'eval &quot;$(~/.local/bin/mise activate bash)&quot;' &gt;&gt; ~/.bashrc Zsh: (macOS 默认) echo 'eval &quot;$(~/.local/bin/mise activate zsh)&quot;' &gt;&gt; ~/.zshrc Fish: ~/.local/bin/mise activate fish | source 完成后，必须重启终端以使配置生效。你可以运行 mise doctor 命令来验证 mise 是否已正确安装和激活。 核心配置：.mise.toml 文件详解 mise 的设计围绕“一次配置，自动运行”的理念，其核心就是 .mise.toml 文件。让我们通过一个全栈 Web 应用的场景，来了解其构成和工作方式。 假设项目技术栈包含： 前端: Node.js 和 pnpm 包管理器。 后端: Python 和 FastAPI 框架。 环境变量: 数据库连接信息。 项目脚本: 用于代码检查和测试。 在项目根目录创建 .mise.toml 文件，将所有配置统一管理： # .mise.toml - 项目环境的唯一声明 # [settings] - 配置 mise 的行为 [settings] # 确保为 Python 自动创建和使用虚拟环境 python_venv_auto_create = true # [env] - 管理环境变量，替代 .env 文件 # 进入目录后，这里的变量会自动加载，离开目录则会自动卸载。 [env] DATABASE_URL = &quot;postgres://user:password@localhost:5432/mydb&quot; NODE_ENV = &quot;development&quot; # [tools] - 声明项目所需的全部工具和版本 # mise 负责管理这些“工具”本身的版本 [tools] # 可以指定精确版本，或使用 &quot;lts&quot;, &quot;latest&quot;, &quot;~20&quot; 等模糊版本 node = &quot;20.11.0&quot; pnpm = &quot;8.15.0&quot; python = &quot;3.11&quot; # 将 ruff 和 pytest 作为开发工具依赖项 # mise 会使用 pip 在项目虚拟环境中安装它们 python-ruff = &quot;*&quot; # 代码格式化与检查工具 python-pytest = &quot;*&quot; # 测试框架 # [tasks] - 定义项目脚本，替代 package.json scripts 或 Makefile [tasks] # 定义 lint 任务, 它依赖 ruff lint = { cmd = &quot;ruff check .&quot;, description = &quot;用 Ruff 检查代码风格&quot;, depends = [&quot;python-ruff&quot;] } # 定义 test 任务，并声明它依赖于 pytest test = { cmd = &quot;pytest&quot;, description = &quot;运行测试&quot;, depends = [&quot;python-pytest&quot;] } # 定义复合任务 check = { depends = [&quot;lint&quot;, &quot;test&quot;], description = &quot;运行所有检查&quot; } 日常工作流 当团队新成员克隆此项目后，只需在项目根目录运行一条命令： mise install mise 会自动读取 .mise.toml 并完成以下工作： 下载并安装 node@20.11.0, pnpm@8.15.0 和 python@3.11。 根据 [settings] 的配置，为 Python 创建一个项目独有的虚拟环境。 在虚拟环境中安装 ruff 和 pytest。 安装完成后，每当 cd 进入该目录，mise 会立即在后台： 将 node, python, pnpm 等命令的路径指向 .mise.toml 中指定的版本。 加载 [env] 中定义的所有环境变量。 自动激活 Python 的虚拟环境。 当你 cd 离开该目录时，所有上述环境配置都会被自动卸载，恢复到之前的状态。 运行在 [tasks] 中定义的脚本也同样简单： mise run lint mise run test mise run check # 该命令会依次执行 lint 和 test # 也可以使用简写: mise x check mise 会确保在执行脚本前，其 depends 中声明的工具都已安装并处于激活状态，从根本上解决了“脚本依赖的工具未安装或版本不正确”的问题。 核心功能详解 掌握了基本用法后，让我们深入探索 mise 最核心的功能，它们是构建统一开发环境的基石。 全能的版本管理 mise 能够管理几乎所有开发工具，其秘诀在于它完全兼容 asdf 的插件生态系统，拥有一个包含数百个插件的官方仓库。 智能插件安装：当你在 .mise.toml 中首次声明一个新工具（例如 go = &quot;1.21&quot;）时，mise 会自动提示是否安装对应的 go 插件。 手动管理插件：你也可以手动搜索和安装插件：# 搜索所有可用的 terraform 插件 mise plugins ls-remote terraform # 安装指定的插件 mise plugins install terraform 插件的工作机制：中心库与自定义插件 你可能会好奇，为什么在 .mise.toml 中只需简单地写入 go = &quot;1.22&quot;，mise 就知道如何去安装 Go？这背后是 mise 强大的插件机制在起作用。 实际上，go、python、terraform 这些看似内置的工具名称，都是插件的简称 (Shorthand)。mise 默认依赖一个庞大的插件中心库（即 asdf 的官方插件库）。当你使用这些简称时，mise 会自动在中心库中查找同名插件并使用它来安装和管理工具。这就是为什么大多数常见工具都能开箱即用的原因。 然而，mise 的能力远不止于此。当中心库无法满足你的需求时（例如，你需要使用一个未被收录的、公司内部的或第三方的工具），你可以通过直接提供 Git URL 的方式来使用任何自定义插件。 这极大地扩展了 mise 的边界，使其能够管理任何可通过 Git 仓库获取的工具。你只需在 .mise.toml 中按以下格式声明即可： # .mise.toml [tools] # ... 其他工具 # 通过 URL 使用一个自定义插件 # mise 会克隆这个 Git 仓库，并将其作为一个插件来使用 my-internal-cli = { git = &quot;https://github.com/my-company/mise-internal-cli.git&quot;, version = &quot;1.2.3&quot; } 或者，你也可以在命令行中一次性使用： mise use my-internal-cli@https://github.com/my-company/mise-internal-cli.git 通过这种机制，mise 确保了开发者既能方便地使用社区维护的主流工具，也拥有了接入任何自定义工具的强大扩展能力。如果你有更进一步的定制化需求，甚至可以遵循 asdf 的插件规范，自行开发插件并托管在自己的 Git 仓库中，从而将任何工具无缝整合到 mise 的管理体系下。 需要明确的是 mise 与 pnpm/pip 的职责划分： mise 管理“工具”：它确保开发环境中有正确版本的 node, pnpm, python 等可执行程序。 pnpm/pip 管理“库”：它们依据 package.json 或 requirements.txt 文件，管理项目代码所依赖的第三方库（如 react, fastapi）。 简而言之，mise 负责解决“我们用哪个版本的 pnpm”，而 pnpm 负责解决“我们用哪个版本的 react”。两者相辅相成。 灵活的版本控制 (mise use) mise 提供了对项目级、全局和临时（Shell 级）工具版本的多层次管理能力，这是它相比 Homebrew 等系统级包管理器的显著优势。Homebrew 通常在系统中全局安装唯一版本的工具，而 mise 可以轻松应对多版本并存的场景。 mise use 是一个用于在不同层级设置工具版本的灵活命令： mise use 有一个更简单的别名：mise x。下文中将优先使用 mise use。 设置全局默认版本： 该命令会修改全局配置文件 ~/.config/mise/config.toml。 # 此后，在任何没有项目级配置的目录中，node 版本都将是 20.x 的最新版 mise use --global node@20 在项目中设置本地版本： 在项目目录下运行此命令，mise 会自动更新当前目录的 .mise.toml 文件。 # 该命令会自动在 ./.mise.toml 的 [tools] 中添加或修改 go = &quot;1.22&quot; mise use go@1.22 在当前 Shell 中临时使用特定版本： 此功能非常适合快速测试，它不会修改任何配置文件。 # 仅在当前终端会话中，将 node 版本切换到 18.x # 新打开的终端将不受影响 mise use node@18 使用特定版本在 Shell 中执行命令： 此功能适用于在工作空间打开一些工具项目，比如 Gemini 或者 QWen 等 AI 工作。它也不会修改任何配置文件。 # 仅当前命令生效，下一条命令仍然使用配置文件中的版本 mise exec node@20 -- qwen 直接从包管理器安装工具： mise 还能直接从 npm, pipx, cargo 等包管理器安装并管理全局可执行文件，极大地简化了 CLI 工具的管理。 # 使用 npm 安装 Gemini CLI 并使其全局可用 mise use --global npm:@google/gemini-cli@latest mise 会处理好 PATH，让你可以在任何地方直接调用 gemini 命令。 平滑迁移策略 mise 在设计上考虑了与现有生态的兼容性，它会自动读取项目中的 .python-version, .nvmrc, .ruby-version 等文件。这意味着你可以立即在现有项目中使用 mise，而无需进行任何破坏性改动。 推荐的迁移路径如下： 安装并激活 mise。 验证：进入现有项目目录，执行 node -v 或 python -V，确认 mise 已正确读取旧配置文件并切换到相应版本。 统一配置：在确认 mise 工作正常后，将 .nvmrc, .python-version 等文件的内容迁移到统一的 .mise.toml 文件中，然后删除旧的配置文件。 清理旧工具：当所有项目都迁移完毕后，可以安全地从 .zshrc 或 .bashrc 中移除 pyenv init, nvm.sh 等旧工具的加载命令，并最终卸载它们。 实用技巧与问题排查 Shell Hook 未配置：最常见的问题。如果 cd 进入项目后工具版本没有自动切换，99% 的可能是 Shell 激活步骤未正确完成。请运行 mise doctor 进行诊断。 与 asdf 的命令差异：mise 的命令与 asdf 不完全相同。例如，设置全局版本是 mise use --global node@20，而非 asdf global nodejs 20.0.0。 曾用名 rtx：mise 的前身是 rtx。如果搜索问题时找不到相关信息，可以尝试使用 rtx 作为关键字。 使用 mise which：当你需要确认当前命令的来源时，mise which node 可以清晰地显示 node 命令的实际路径以及它是由哪个配置文件所设定的。 使用 mise exec：如果你想在特定项目的环境中执行单个命令，而无需 cd 到该目录，mise exec 非常有用。# 假设 ./my-project/ 中配置使用 node@18 # 你可以在任何位置运行此命令，它会使用 my-project 的环境来执行 mise exec --cwd ./my-project -- node --version # 输出: v18.x.x 总结 mise 提供了一个统一、高效、高性能的开发环境解决方案。它通过单一配置文件，优雅地解决了在现代软件开发中普遍存在的版本管理、环境配置和任务执行等一系列挑战。如果你希望从繁杂的配置工作中解放出来，专注于核心的开发任务，mise 是一个值得投入时间尝试的工具。 写在最后 明眼人或许一眼就能看出，这篇文章是由 AI 撰写，其风格与我以往的文章有所不同。 起初，当我动笔写下引言时，一个疑问忽然浮现在脑海：“在 AI 时代，mise 这类工具是否还有其存在的意义？” 这不禁让我想起，有人认为在 AI 时代，跨平台移动开发框架已无必要（暂且不提他们对跨平台框架的固有抵触），因为 AI 能迅速生成两端原生代码，使得跨平台框架所节省的时间成本变得微乎其微，甚至可能因兼容性问题带来更高的调试成本。同理，mise 也可以被抽象地理解为一种“跨平台框架”。 于是，我决定让 AI 来完成这篇文章，并由它来回答这个核心问题。AI 给出了这样的回应： 我认为，即使在 AI 时代，mise 这样的工具依然有其不可替代的意义，但这意义的侧重点会发生变化： 从“人工处理的便利性”转向“系统级的可维护性与一致性”： AI 的局限性： 尽管 AI 能够快速生成和处理文件，但它本质上是一个“工具”。它生成的内容最终还是要被执行、被维护、被理解。当环境配置分散在多个文件中时，即使是 AI，也需要花费额外的“计算资源”去解析、协调和验证这些文件之间的关系和潜在冲突。 统一配置的优势： mise 提供了一个声明式、单一的真相来源 (Single Source of Truth)。这意味着： 减少错误面： 多个配置文件意味着更多的潜在冲突点、版本不一致的风险。即使 AI 生成，也可能因为逻辑复杂性或历史遗留问题导致生成不完全一致的配置。一个统一的 .mise.toml 从根本上减少了这种错误的可能性。 简化调试： 当环境出现问题时，无论是人类开发者还是 AI 辅助的诊断系统，都只需要检查一个文件，而不是在多个文件中来回跳跃，这极大地简化了问题排查的流程。 提升可审计性： 所有的环境定义都在一个地方，这使得环境的变更、历史版本追溯变得异常清晰。这对于团队协作、CI/CD 流程以及安全审计都至关重要。 AI 自身的效率： 即使是 AI，处理一个结构清晰、统一的配置文件，也比处理多个分散、格式可能不一的文件更高效、更不容易出错。这就像给 AI 提供一份清晰的蓝图，而不是一堆散落的零件图。 mise 不仅仅是配置定义，更是“环境执行器”和“标准化强制者”： mise 的价值不仅在于它能替代 .nvmrc 或 Gemfile，更在于它能自动执行这些配置，确保环境的自动切换和隔离。AI 可以生成配置，但它无法直接在你的 Shell 中自动切换 Node.js 版本，也无法自动激活 Python 虚拟环境。这是 mise 作为底层工具链管理器的核心功能。 它强制了团队内部的环境标准化。AI 可以生成符合标准的配置，但 mise 确保了这些标准在每个开发者的机器上都被严格执行。 人类仍然是最终的决策者和监督者： 即使在 AI 辅助的开发流程中，人类开发者仍然需要理解项目、审查 AI 生成的代码和配置、并对最终的系统负责。一个统一、简洁的环境配置，降低了人类理解和介入的门槛，使得人与 AI 的协作更加顺畅。 结论： 在 AI 时代，mise 这种工具的意义从“减轻人工操作的繁琐”转向了“提供一个更健壮、更一致、更易于系统级管理和调试的开发环境基础设施”。它为 AI 生成的代码提供了一个稳定、可预测的运行基础，并简化了整个开发生命周期中的环境管理复杂性。AI 可能会改变我们与这些工具交互的方式，但不会消除它们所解决的根本问题。相反，一个高度自动化的 AI 开发流程，反而会更依赖于底层环境的标准化和自动化，而这正是 mise 的强项。 尽管 AI 的回答并非全然无懈可击（例如，Cursor 已能自动执行 Shell 命令以切换至合适的环境），但其论证依然说服了我，促使我最终发布了这篇文章。 此外，AI 的回答也让我深切体会到，在学习过程中“有事没事多问问 AI”确实是一件充满“幸福感”的事情。这让我想起十年前我独自摸索 iOS 开发的艰难岁月，而如今，一位随叫随到的 AI 助手能够完美扮演亦师亦友的角色，随时解答我的疑问，帮助我拓宽思路。 现在对于学习新知识而言，可真是一个好的时代。 ","link":"https://blog.rakuyoo.top/mise/"},{"title":"计算机网络之二 - URL 与 DNS","content":"本篇是计算机网络系列的第二篇，主要讲 URL 和 DNS。 其余几篇的目录： 计算机网络之一 - IP 与端口 计算机网络之三 - DHCP 与内网穿透 计算机网络之四 - 虚拟网卡与 WireGuard 计算机网络之五 - HTTP 与 HTTPS 计算机网络之六 - 可靠的 TCP 与高效的 UDP 计算机网络之七 - OSI 与 TCP/IP 分层模型 URL URL，全称是 统一资源定位符 (Uniform Resource Locator)，是我们访问互联网上任何资源的“地址”。它由几个部分组成，共同指明了“去哪里”以及“如何去”。 在接下来的内容中，我们将逐一解析构成 URL 的几个核心元素。 域名 上一篇我们讲到了 IP 和端口，然而绝大多数的时候我们在浏览器地址栏中输入的都不是这些数字，而是由英文字母和部分字符组成的地址，也就是 域名。 对于一般人而言，一串 IP 地址很难记忆，人们更擅长以字词为单位，赋予地址含义再加以记忆。于是乎人们发明了 “域名” 这个概念，用来代替 IP 地址去记忆。 域名的结构 一个域名通常由几个部分组成，用点（.）分隔，从右到左看，层级越来越具体。以 www.google.com 为例，让我们来剖析一下域名的格式和结构： .com：这是顶级域名 (Top-Level Domain, TLD)。它表示域名的类别。 google：这是二级域名 (Second-Level Domain, SLD)。这是域名的核心部分，通常是公司、品牌或个人的名称。这是用户注册和拥有的部分。 www：这是子域名标签 (Subdomain Label)。它是一个可选的、用于进一步细分主域名的“标签”或“前缀”。 它允许你在同一个主域名下创建不同的分区或服务。 www 是最常见的子域名，通常指向网站的主页。 其他子域名也很常见，比如 mail.google.com 指向邮件服务，drive.google.com 指向云盘服务。 还可以从整体的角度看一下： google.com：这被称为主域名（Main Domain），准确来说是 “注册域”（Registered Domain）。 当我们谈论 “域名” 时，通常指的就是这一部分。 通常也是我们在域名注册商那边购买的那部分。 www.google.com：被称为完全限定域名 (Fully Qualified Domain Name, FQDN) 或 主机名 (Hostname) 也可以被称为子域名 (Subdomain) 最后以本站为例说明一下域名的各个部分： blog 是子域名标签 rakuyoo 是二级域名 .top 是顶级域名 rakuyoo.top 是主域名 blog.rakuyoo.top 是一个完全限定域名（或完整的子域名） 协议 除了域名之外，URL 还有一个重要的组成部分：协议（Protocol）。它是一套规则、标准或约定，规定了计算机之间如何进行通信和数据交换。协议定义了数据传输的格式、时序、错误处理等，它将告诉客户端和服务器“如何”进行通信。 常见的协议比如 http、https 以及 ftp 等，因为都非常常见了，所以在本文这一小节就先不展开介绍了。未来涉及到 TCP 数据传输时可能还会再次见面。 协议对于 URL 而言是非常重要的，当我们访问 www.google.com 时，实际上我们是 “用 HTTPS 协议连接到 www.google.com”，而不能只说 “连接到 www.google.com”。 别忘了 “端口” 在没有域名时，我们访问一个服务是通过 “IP 地址 + 端口” 来实现的。那么有了域名之后呢？平时我们在浏览器中输入地址时也没有用到端口呀。 其实不是没有用到，而是浏览器自动帮我们处理了。比如 https 协议的默认端口是 443，所以当我们在浏览器中输入 https://www.google.com 时，其实我们访问的是 https://www.google.com:443。如果我们需要访问某个特殊的端口，那么也需要像上面说的那样显式指定端口号。 DNS：一本厚字典 域名帮助我们省去了记忆复杂 IP 的麻烦，那么，域名是如何找到其对应的 IP 地址的呢？答案就是通过 DNS（Domain Name System）。嗯... 从名字上来看它就很 “域名”。 DNS 的工作原理（简化版） 当我们访问一个域名时，DNS 会通过一系列查询操作，找到这个域名所对应的 IP 地址： 在浏览器中输入域名： 在浏览器中输入 www.google.com 并按下回车。 浏览器查询本地 DNS 缓存： 浏览器会首先检查自己有没有缓存过 www.google.com 对应的 IP 地址。如果有，就直接使用，这样速度最快。 操作系统查询本地 DNS 缓存和 hosts 文件： 如果浏览器没有，它会把请求交给操作系统。操作系统也会检查自己的 DNS 缓存，以及一个叫做 hosts 的本地文件。 请求发送给 DNS 解析器 (DNS Resolver)： 如果本地都没有，操作系统会将请求发送给你的网络配置中指定的 DNS 解析器（通常是 ISP 提供的 DNS 服务器，或者手动配置的公共 DNS，如 Google 的 8.8.8.8）。 DNS 解析器开始递归查询： 查询根域名服务器 (Root Name Servers)： 解析器会首先问全球的 13 组根域名服务器：“www.google.com 的 IP 地址是什么？” 根服务器不会直接告诉你 IP，它会告诉你：“我不知道，但你可以去问负责 .com 域名的服务器。” 查询顶级域名服务器 (TLD Name Servers)： 解析器接着会去问负责 .com 域名的 顶级域名服务器：“www.google.com 的 IP 地址是什么？” .com 服务器会告诉你：“我不知道 www.google.com 的具体 IP，但你可以去问负责 google.com 的权威域名服务器。” 查询权威域名服务器 (Authoritative Name Servers)： 解析器最后会去问 google.com 的权威域名服务器（这是由 Google 自己或其域名注册商维护的服务器）：“www.google.com 的 IP 地址是什么？” 这台服务器知道 www.google.com 对应的确切 IP 地址，并会把这个 IP 地址告诉 DNS 解析器。 IP 地址返回给浏览器： DNS 解析器收到 IP 地址后，会将其返回给你的操作系统，再由操作系统返回给浏览器。同时，这个 IP 地址会被缓存起来，以便下次更快地访问。 浏览器连接服务器： 浏览器拿到 IP 地址后，就可以直接通过这个 IP 地址连接到 Google 的服务器，并请求网页内容。这个完整、层层递进的查询过程，可以通过下面的序列图清晰地展现出来： iOS 系统的 DNS 解析和 PC 上的 DNS 解析还有一点点区别，那就是 iOS 不存在 “浏览器 DNS 缓存” 这一步，所有的 DNS 缓存都在 iOS 系统内部进行统一的管理。各个 App 如果不做特殊的实现，也不存在 “App 级别的 DNS 缓存”。 可能这个流程还是有一些抽象。我们还可以从 “绑定 IP 和域名” 的流程中来解释 DNS 的解析过程： 获取 IP：首先我们从云服务器商处购买了一台云服务器，这个时候我们会得到这台服务器的 IP 地址。 购买域名：之后我们从域名商那里挑选了一个域名。国内的几大云服务器厂商基本都提供域名购买服务。 配置 DNS 解析：有很多地方可以让我们把 IP 和域名绑定在一起，比如有的域名注册商就提供功能，云服务商也提供，甚至还有专门的 DNS 服务商。在这些平台上，我们需要将域名和 IP 绑定到一起。通常是设置一条记录，key 和 value 填写你的域名和 IP。 上传 DNS 记录：当我们配置好了 DNS 后，平台就会将我们的配置上传到真正的 DNS 服务器上，此时域名与 IP 的绑定关系才算建立完成，域名和 IP 被真正的绑定在一起。 后续访问：现在当我们再访问域名时，你可以理解为是一个巨大的查表过程，通过域名就可以查询到对应的 IP 了。 DNS 缓存 在上一节中，我们看到了一个域名需要经过一连串的查询才能最终找到它的 IP 地址。如果每一次访问网站都需要重复这个完整的过程，那效率未免也太低了。 为了解决这个问题，DNS 系统设计了一套至关重要的机制——缓存。简单来说，当一个 DNS 查询的最终结果被找到后，查询路径上的各个参与者（比如你的电脑、DNS 解析器）都会把这个 “域名-IP” 的对应关系暂存一段时间。如果在缓存有效期内再次请求同一个域名，就可以直接返回暂存的结果，从而跳过复杂的递归查询过程，极大地提高了响应速度。 我们在上一节中提到的 “浏览器缓存” 和 “操作系统缓存” 就是这套机制的体现。而控制这一切的核心，是一个叫做 TTL（Time-To-Live，生存时间）的值。 TTL (Time-To-Live) TTL 是由域名管理员在配置 DNS 记录时设置的一个数值，单位是秒。它像一个“保质期”，告诉各级 DNS 服务器和解析器，这个查询结果可以被缓存多久。 例如，本站的 blog.rakuyoo.top 的 TTL 被设置为 600 秒（10 分钟），那么当 DNS 解析器第一次获取到它的 IP 地址后，就会将这个结果缓存 10 分钟。在这 10 分钟内，任何通过这个解析器查询 blog.rakuyoo.top 的请求，都会直接得到缓存中的 IP 地址，而无需再去麻烦权威域名服务器。 DNS 传播 TTL 机制在带来高效的同时，也引入了一个重要的现象：DNS 传播（DNS Propagation）。 当你修改了一条 DNS 记录（比如将网站服务器迁移到了一个新的 IP 地址），这个变更并不会立即在全球范围内生效。因为各地的 DNS 解析器仍然缓存着旧的记录，它们需要等到各自缓存中的 TTL 过期后，才会重新向权威服务器发起查询以获取最新的记录。 这个新旧记录交替、变更信息逐渐扩散到全球的过程，就叫做 DNS 传播。传播所需的时间，就取决于你之前设置的 TTL 值。如果 TTL 设置为 1 小时，那么最坏情况下，一些用户可能需要等待 1 小时才能访问到你的新服务器。 因此，在计划进行服务器迁移等重大变更前，有经验的管理员通常会提前几天将相关域名的 TTL 值修改为一个非常小的值（比如 60 秒），以确保变更发生时，全球的 DNS 缓存能被快速刷新，从而实现平滑过渡。 灵活的绑定关系 一个主机名其实可以解析到多个 IP，这有利于实现负载均衡。只需要在配置 DNS 解析时，为同一个主机名添加多个 IP 地址，即可将一个主机名解析到多个 IP 上。 当用户访问这个主机名时，DNS 服务器会同时返回所有的 IP 地址，客户端（比如浏览器）会尝试连接其中一个，可能是随机选择某一个，也有可能是按顺序尝试。如果连接失败，则会尝试列表中的下一个 IP。 另外我们还可以以子域名标签为单位，或者准确的说，以 “完全限定域名” 为单位设置 DNS 解析。比如 mail.google.com 和 drive.google.com 虽然都在 google.com 这个域名下，但是他们可以部署在不同的服务器上，进而拥有不同的 IP 地址。 资源记录 在你添加 DNS 解析时，会接触到 “资源记录”（Resource Record）这个概念，在腾讯云服务器上它又叫做 “记录类型”。 DNS 十分强大，强大到它其实可以解析很多东西。所以系统设计了一个 “资源记录” 的概念，用来区分 DNS 所解析的内容的类型。常见的资源记录有以下几种： A 记录 (Address Record) 将一个域名或子域名指向一个 IPv4 地址。它是最基本的记录类型。 AAAA 记录 (Quad-A Record) 与 A 记录类似，但它将一个域名或子域名指向一个 IPv6 地址。 CNAME 记录 (Canonical Name Record) 将一个域名或子域名指向另一个域名（而不是直接指向 IP 地址），相当于创建了一个别名。它有以下几种使用场景： 当你的服务提供商（如 CDN、博客平台）给你一个域名（而不是 IP 地址）让你指向时。 当你希望多个子域名都指向同一个主域名，而主域名的 IP 地址可能会变动时。这样你只需要更新主域名的 A 记录，所有 CNAME 记录都会自动生效。 另外需要注意： CNAME 记录不能指向 IP 地址，只能指向另一个域名。而且，通常情况下，根域名（如 yourdomain.com）不能设置为 CNAME 记录，因为它会与其他记录（如 MX 记录）冲突。 根据 DNS 规范（RFC 1034），如果一个域名存在 CNAME 记录，那么它就不能再有任何其他类型的记录（除了用于 DNSSEC 的 RRSIG 和 NSEC 记录）。而一个域的顶点（即 yourdomain.com 本身）必须有 SOA (Start of Authority) 和 NS (Name Server) 记录来标识这个域的权威信息。这两者是冲突的，所以根域名不能设置 CNAME。 本文并未涉及到 SOA 记录和 NS 记录等，此处仅作为拓展阅读。 Swift URL 相信阅读到此文章的大多数都是 iOSer，那么我们可以再从 Swift URL 的角度来看一下域名的各个组成部分，直接上代码： let urlString = &quot;https://www.google.com:443&quot; // 1. 创建 URL 对象 guard let url = URL(string: urlString) else { print(&quot;Invalid URL string&quot;) exit(1) } // 2. 访问 URL 的各个属性 print(&quot;Scheme (协议): \\(url.scheme ?? &quot;N/A&quot;)&quot;) print(&quot;Host (主机名/域名): \\(url.host ?? &quot;N/A&quot;)&quot;) print(&quot;Port (端口): \\(url.port?.description ?? &quot;N/A&quot;)&quot;) // 3. 进一步拆解 host // Swift 的 URL 类型没有直接提供“二级域名”或“顶级域名”的属性 // 你需要自己编写逻辑来从 host 中提取这些信息 let host = url.host ?? &quot;&quot; let components = host.split(separator: &quot;.&quot;).map(String.init) if components.count &gt;= 2 { let tld = components.last ?? &quot;N/A&quot; // .com let sld = components[components.count - 2] // google let mainDomain = &quot;\\(sld).\\(tld)&quot; // google.com print(&quot;推断的主域名 (Main Domain): \\(mainDomain)&quot;) if components.count &gt; 2 { let subdomainLabel = components.first ?? &quot;N/A&quot; // www print(&quot;推断的子域名标签 (Subdomain Label): \\(subdomainLabel)&quot;) } } 输出结果如下： Scheme (协议): https Host (主机名): www.google.com Port (端口): 443 推断的主域名 (Main Domain): google.com 推断的子域名标签 (Subdomain Label): www 说一个小细节：还记得上文中我说过 “协议对于 URL 来说很重要” 吗？如果将示例中 urlString 的 https 部分去掉，改为 www.google.com:443，那么 url.scheme 的结果将会是 www.google.com，url.host 会是 nil。 这是因为 Swift 的 URL 在解析 URL 字符串时，会严格按照 URL 的标准格式来识别各个组成部分，而一个标准的 URL 必须以协议开头，后面跟着 ://。 对于 www.google.com:443 而言，URL 初始化器会将 : 前的内容都识别为协议。假如换成 www.google.com，即字符串中没有 :，那么 url.scheme 和 url.host 都将是 nil。 ","link":"https://blog.rakuyoo.top/computer-network-url-and-dns/"},{"title":"解密 Swift：为何 reduce 偏爱 + 而非 && ？","content":"作为 Swift 开发者，我们都热爱这门语言提供的优雅和简洁性，尤其是它的函数式编程能力。 Swift 原生提供了一些高阶函数，常见的比如 map、filter、reduce 等。这些优雅的高阶函数能够极大的简化我们的代码，但是一个不小心也会报一个让你困扰一整天的错误。 今天，我们就来看看其中一种情况。 思考一下这个场景：对一个整数数组求和。使用 reduce 函数可以写得非常简洁： let numbers = [1, 2, 3, 4, 5] // 冗长但清晰的写法 let sum1 = numbers.reduce(0) { (accumulator, nextElement) in accumulator + nextElement } // 简单写法 let sum1 = numbers.reduce(0) { $0 + $1 } // 极简写法 let sum2 = numbers.reduce(0, +) + 号可以直接作为 reduce 方法的第二个参数，从而进一步简化代码。这很好，非常简洁。 ok，现在让我们尝试用同样的逻辑来处理一个布尔类型的数组，检查是否所有值都为 true： let conditions = [true, true, false] let allTrue1 = conditions.reduce(true) { $0 &amp;&amp; $1 } // ✅ 工作正常 let allTrue2 = conditions.reduce(true, &amp;&amp;) // ❌ 编译错误！ 哦噢，编译错误了。那么为什么 + 可以直接作为参数传递，&amp;&amp; 却不行？其实这背后隐藏着一个关于 Swift 语言设计的深刻且重要的区别。 + 运算符：“一等公民” Swift 标准库中包含一个名叫 AdditiveArithmetic 的协议，这个协议中定义了包括 + 在内的多个函数。我们可以从 Github 中查看到它的源码，官方文档在这里。 所以 + 可以作为参数传递也就不奇怪了 —— 它本身就是 Swift 中的 “一等公民”：函数，因此可以被直接传入任何接受对应函数类型的参数中。 &amp;&amp; 运算符：重要的“短路”特性 相比较 + 而言，&amp;&amp; 和 || 这类逻辑运算符有一个核心特性，即 短路求值 (Short-circuit Evaluation)： 对于 a &amp;&amp; b，如果 a 的结果是 false，那么整个表达式的结果就已经确定是 false 了。因此，b 将永远不会被求值或执行。 对于 a || b，如果 a 的结果是 true，整个表达式就已经确定是 true，b 也不会被执行。 这个特性至关重要，它不仅能提升性能，还能避免潜在的运行时错误，比如我们经常写的代码： if user != nil &amp;&amp; user!.hasPermission { // 如果 user 是 nil，第二个条件根本不会被检查，从而避免了强制解包导致的崩溃。 } 而一个普通的 Swift 函数，在它被调用之前，它的所有参数都必须被完整地求值。可以通过一个简单的实验来证明这一点： func getFalse() -&gt; Bool { print(&quot;getFalse() 被调用了&quot;) return false } func getTrue() -&gt; Bool { print(&quot;getTrue() 被调用了&quot;) return true } // 如果 &amp;&amp; 是一个普通函数，它会像这样工作： func logicalAnd(_ a: Bool, _ b: Bool) -&gt; Bool { return a &amp;&amp; b } print(&quot;\\n--- 测试普通函数 ---&quot;) _ = logicalAnd(getFalse(), getTrue()) // 输出: // --- 测试普通函数 --- // getFalse() 被调用了 // getTrue() 被调用了 &lt;-- 两个参数对应的函数都被执行了 从输出结果我们可以得到结论：“短路” 这个行为无法被普通函数模拟 ... 别忘了，我们还可以看它的源码。 实际上在 Swift 标准库中，&amp;&amp; 和 || 同样是两个函数，他们的定义分别如下所示： extension Bool { /// Performs a logical AND operation on two Boolean values. @_transparent @inline(__always) public static func &amp;&amp; (lhs: Bool, rhs: @autoclosure () throws -&gt; Bool) rethrows -&gt; Bool { return lhs ? try rhs() : false } /// Performs a logical OR operation on two Boolean values. @_transparent @inline(__always) public static func || (lhs: Bool, rhs: @autoclosure () throws -&gt; Bool) rethrows -&gt; Bool { return lhs ? true : try rhs() } } 标准库通过三目运算符以及 @autoclosure，巧妙地实现了短路求值的功能。 但是绕了一圈，既然 &amp;&amp; 也是函数，为什么不能像 + 一样作为参数传入 reduce 函数呢？其实我偷偷把报错信息藏起来了。让我把它放出来，再看一下上面例子中的错误： [true, true, false].reduce(true, &amp;&amp;) // ❌ Cannot convert value of type '(Bool, @autoclosure () throws -&gt; Bool) throws -&gt; Bool' to expected argument type '(Bool, Bool) throws -&gt; Bool' 报错说的很直接：无法将 (Bool, @autoclosure () throws -&gt; Bool) throws -&gt; Bool 类型转换为 (Bool, Bool) throws -&gt; Bool，也就是 @autoclosure () -&gt; Bool 无法被直接当做 Bool 来传递。所以直接原因还是类型转换的问题。 用一段代码可以更好地说明这个问题： func foo(_ action: (Bool, @autoclosure () /*throws*/ -&gt; Bool) throws -&gt; Bool) { /* ... */ } foo(&amp;&amp;) // `&amp;&amp;` 可以作为参数正常使用了 @autoclosure：巧妙的语法糖 @autoclosure 是一个纯粹的语法糖，它告诉编译器：把调用时写在这里的表达式自动包成一个闭包，而不是立即求值。 所以 @autoclosure () -&gt; Bool 本质上还是一个闭包类型 () -&gt; Bool，它不是也没有办法转换成一个 Bool 类型。至少在 Swift 6 时代还没有这种特性。 如果没有 @autoclosure，当我们想通过闭包来实现 “延迟调用” 时，可能就要想一想 “用闭包再额外包一层” 这种不优雅的实现，会不会让自己的早餐吐出来： // `assert` 断言通过闭包实现了判断条件的延迟调用，进而在 RELEASE 模式下获得更好的性能。 // 假设下面是 assert 函数的简化定义 func assert(_ condition: /* @autoclosure */ () -&gt; Bool, ...) { ... } // 如果没有 @autoclosure，调用时就必须手动包裹一层闭包 assert({ a &gt; b }) 总结 总结一下，Swift 从 1.0 开始就开始使用 @autoclosure 来优雅地实现逻辑运算符的短路求值，或者其他延迟求值的相关逻辑。这也导致了 &amp;&amp; 和 || 这两个逻辑运算符不能像普通的运算符一样那么地 “自由”。 如果有需要，开发者可以自行封装相关函数，从而为逻辑运算符插上自由地翅膀。 ","link":"https://blog.rakuyoo.top/why-swift-reduce-prefers-plus-over-and/"},{"title":"计算机网络之一 - IP 与端口","content":"最近在研究 Surge，众所周知主包这方面知识贼差，几乎可以算是从零开始学起，所以也想借这个机会好好补一下相关知识。 预计会写一系列文章，从计算机网络的基础概念开始，后续会包含常见的接口请求流程以及 WireGuard 等相关内容。 本篇就是系列的第一篇。 其余几篇的目录： 计算机网络之二 - URL 与 DNS 计算机网络之三 - DHCP 与内网穿透 计算机网络之四 - 虚拟网卡与 WireGuard 计算机网络之五 - HTTP 与 HTTPS 计算机网络之六 - 可靠的 TCP 与高效的 UDP 计算机网络之七 - OSI 与 TCP/IP 分层模型 IP IP 地址 是计算机网络通信的基石，网络中每一台设备都有唯一的一个 IP 地址。你可以把它想象成设备的身份证号，每个人的身份证号都是唯一的。 常见的 IP 地址比如 8.8.8.8，这是 Google DNS 服务器的 IP 地址。再比如 192.168.1.1，家中路由器的管理页面经常是这个地址。 IPv4 和 IPv6 IP 地址有两个版本：IPv4 和 IPv6。 可能你会好奇有没有 IPv3、IPv5 等等，其实是有的。 所谓的 IPv1, v2, v3，可以被看作是 IPv4 诞生之前的、存在于 TCP 协议内部的“史前草稿”或“实验版本”，它们从未作为独立的 IP 协议存在过。 而 IPv5，更常见的名字可能是 ST2，它是 IP 协议的旁支，从未被打算作为通用的下一代互联网协议，也从未被广泛部署。 本文更偏向于实用性而非历史小课堂，所以暂不会涉及到相关内容。 IPv4 IPv4 是我们目前最常接触到的版本，它使用 32 位 (bit) 来定义一个地址。 理论总数： 2³² = 4,294,967,296 (约 43 亿个) 书写格式： 通常写成“点分十进制”的形式，由 4 个 0 到 255 之间的数字组成，用点号隔开。例如 192.168.1.1。 理论范围： 从 0.0.0.0 到 255.255.255.255。 为什么 IP 段的最大值是 255 而不是 999？相信精通计算机原理的你一定明白： IPv4 一共 4 段，总长 32 位，所以每一段就是 8 位，即一字节。一个字节最大值即 11111111。转换为十进制即 255。而 999 使用二进制表示则是 1111100111，长度为 10 位，超出了最大 8 位的限制。 然而，并不是所有这些地址都可以分配给互联网上的设备使用。其中有大量地址被保留用于特殊目的，比如说下表： 地址范围 名称 用途说明 10.0.0.0 - 10.255.255.255 私有网络 (A 类) 用于大型企业的内部网络 (局域网)。 172.16.0.0 - 172.31.255.255 私有网络 (B 类) 用于中型网络的内部网络。 192.168.0.0 - 192.168.255.255 私有网络 (C 类) 最常见，用于家庭、小型办公室的路由器和内部设备。 127.0.0.0 - 127.255.255.255 环回地址 (Loopback) 主要使用 127.0.0.1，代表“本机”或 localhost，用于在本机上进行网络测试。 100.64.0.0 - 100.127.255.255 运营商级 NAT (CGNAT) ISP 用来给大量用户分配的“大内网”地址，以缓解 IPv4 地址耗尽问题。 169.254.0.0 - 169.254.255.255 链路本地地址 (APIPA) 当设备无法从 DHCP 服务器获取 IP 地址时，会自动配置一个此范围内的地址，用于临时局域网通信。 0.0.0.0 未指定地址 通常用作源地址，表示“任何 IP”或“本机”。 224.0.0.0 - 239.255.255.255 组播地址 (Multicast) 用于一对多的通信，如网络电视直播。 255.255.255.255 广播地址 向本地网络中的所有设备发送消息。 抛去这些保留的 ip 地址，剩余的 ip 地址大约是 37 亿个。这 37 亿个看似很多，但是我说几个数据： 2007 年第一代 iPhone 发布，2011 年，全球智能手机出货量已接近 5 亿部。 2000 年，全球互联网用户大于 3.6 亿；2011 年，全球互联网用户超过 20 亿。 如果考虑到一部设备有一个 ip 的话，那么 37 亿个 IP 地址是远远不够用的。事实也是：2011 年 2 月 3 日，IANA（互联网号码分配局）宣布它已经将所有的 IPv4 地址都分配出去了，也就是说所有可进行分配的 IPv4 地址都有了主人。 IPv4 地址分配的逻辑是这样的：有一个全球总库（IANA），它负责将 IPv4 地址分配给五个区域性注册机构（RIR），分别是：亚太地区、欧洲地区、拉丁美洲和加勒比地区、北美地区以及非洲地区。之后再进行一层一层的分配。 那么既然 IPv4 被分配完了，后续如果想要增加新的设备又该怎么办呢？解决方案有两个： IPv6 内网 IP + NAT 这两个方案分别从两个角度出发，试图解决 IPv4 地址太少的问题。 IPv6 IPv6 的解决方案最为根本，也最为直接：既然 IPv4 太短，那么把它变长就可以了！于是 IPv6 诞生了。 IPv6 使用 128 位来定义一个地址，足足是 IPv4 的 4 倍！不止如此，它还将 “点分十进制” 升级为了 “冒号十六进制” 的形式。所以它的理论范围为： 从 0000:0000:0000:0000:0000:0000:0000:0000 到 ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff 足足有 2¹²⁸ ≈ 3.4 x 10³⁸ (340 万亿亿亿亿个)！这又是一个什么概念呢？一个常见的比喻是：地球上每一粒沙子都可以被分配一个独立的 IPv6 地址，而且还绰绰有余。可见它有多么的长。 IPv6 长度变长了，相比较 IPv4，写起来多少有些复杂，于是它也设计了一些简化规则： 每组中开头的 0 可以省略：0db8 -&gt; db8。 连续的多组 0000 可以用双冒号 :: 代替，但一次只能用一次。比如说： 缩写后：2001:db8:85a3::8a2e:370:7334 展开后：2001:db8:85a3:0000:8a2e:370:7334 IPv6 同样有特殊地址，比如 ::1 就是它的环回地址（相当于 IPv4 的 127.0.0.1），但由于其地址空间极其庞大，不再需要像 IPv4 那样抠出大段的 “私有地址” 来节约使用。 IPv4 升级到 IPv6 的挑战 IPv6 看着 “哪儿哪儿都好”，但是推广 IPv6 的过程可谓十分坎坷：IPv6 是不兼容 IPv4 的。 IPv4 地址的耗尽是点燃 IPv6 项目的导火索，但是在 “必须开发一个新协议” 这个大前提被确立下来时，IETF (互联网工程任务组) 的开发者忽然意识到：这是一个千载难逢的好机会，可以去修复 IPv4 运行了几十年后暴露出来的各种设计缺陷： 路由效率低下：全球 IPv4 路由表的条目过于庞大和碎片化，导致骨干网路由器压力巨大。 缺乏内置安全：IPsec (用于网络层加密和认证) 是后来给 IPv4 打的“补丁”，而在 IPv6 中是强制要求、原生集成的。 配置复杂：IPv4 依赖 DHCP 进行自动配置。IPv6 设计了更强大的 “无状态地址自动配置 (SLAAC)”，设备可以自己生成地址，即插即用。 没有为移动性优化：Mobile IP for IPv4 也是一个笨拙的补丁。IPv6 在设计时就考虑了移动设备的需求。 他们自问：“既然我们无论如何都要进行一次伤筋动骨的 “大手术”，为什么不趁这个机会，把身体里其他所有已知的小毛病也一并治好呢？” 于是，在 “扩大地址空间” 这个核心目标之上，又增加了一系列重要的设计目标： 简化报头，提升路由效率：移除了一些不常用或有问题的字段，让路由器硬件能更快地处理数据包。 原生集成安全性 (IPsec)：将安全变成了协议的“标配”，而不是“选装”。 改进服务质量 (QoS) 支持：引入“流标签”，让路由器能更好地识别和处理对延迟敏感的应用（如视频会议、在线游戏）。 实现真正的即插即用：通过无状态地址自动配置 (SLAAC)，让设备接入网络后的配置过程大大简化。 为移动性而设计：更好地支持移动设备在不同网络间切换时保持连接。 于是乎：一个 不兼容 IPv4 的新协议：IPv6 诞生了。但这也成为了它最大的缺点。 初识这个概念的你不知道有没有思考过：为什么人们常用 “迁至 IPv6” 这个说法，而不是 “升级到 IPv6” 呢？这正是因为 IPv6 不兼容 IPv4。 一个网络路由器的核心工作是 以线速 (Line Rate) 转发数据包。这意味着一个拥有 10Gbps 端口的路由器，必须有能力在 1 秒内处理掉 10G 比特的数据，不能有任何延迟。这种高性能的实现，严重依赖专门的硬件芯片——ASIC (专用集成电路)。路由器内部的数据包转发路径，大部分是由硬件写死的，而不是由通用的 CPU 来处理的。 这就导致了虽然我们可以通过软件升级，让路由器使用 CPU 来处理 IPv6 数据包，但是它的处理速度和 ASIC 相比可谓有着天壤之别，所以它也被称为 “慢速路径” (Slow Path)。 所以如果用户想要 “升级到 IPv6”，那么它必须要要将自己所有的网络设备进行升级，升级到支持 IPv6 的设备，或者至少是同时支持 IPv4 和 IPv6 的设备。这就带来了很大的迁移成本。 公网 IP 和内网 IP 那么既然 IPv4 迁移到 IPv6 注定是缓慢的，不可一蹴而就的，当下有什么办法能先临时解决 IPv4 地址耗尽的问题吗？答案就是 “内网 IP + NAT”。 其实内网 IP 最初被设计出来，并不是为了解决全球地址耗尽这个宏大问题，而是为了解决一个更早期、更具体、更实际的工程问题：简化和隔离大型组织的内部网络管理。 在 20 世纪 90 年代初的时候，虽然全球地址耗尽的危机还未迫在眉睫，但一些大型企业、大学和政府机构已经开始构建自己庞大的内部网络。它们遇到了几个头疼的问题： 内部网络重构的麻烦 想象一下，一个大公司最初从 IANA 申请了一块公网 IP 地址块用于内部网络。后来公司发展壮大，需要连接到另一家 ISP，或者要更换 ISP。 新的 ISP 可能会要求公司使用由新 ISP 分配的另一块完全不同的公网 IP 地址块。 这意味着公司内部成千上万台电脑、服务器、打印机的 IP 地址 全都需要重新编号和配置。这在当时是一个极其痛苦、繁琐且容易出错的工作，被称为“重新编号地狱”。 路由表的混乱 如果一个公司的内部网络结构非常复杂，它可能会把自己的内部路由信息“泄露”到全球互联网的路由表中。 这会造成全球路由表的“污染”，增加了骨干网路由器的负担，也可能导致路由错误。 安全性的初步考量 当时的网络安全概念还很初级，但人们已经意识到，让公司内部的每一台打印机、文件服务器都拥有一个可以直接从外部访问的公网 IP，似乎不是一个好主意。需要一种方式将内部网络与外部互联网进行逻辑上的隔离。 为了解决上述这些问题，IETF 在 1994 年发布了 RFC 1597 (后来在 1996 年被更完善的 RFC 1918 取代)，正式提出了 “私有地址空间 (Private Address Space)” 的概念。 RFC 1918 的核心思想是： 划定专用地址块： 从现有的 IPv4 地址空间中，专门划出三块地址（10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16）作为“私有地址”。 可重复使用： 明确规定，任何组织都可以在自己的内部网络中自由地使用这些地址，而无需向 IANA 申请。你公司可以用 10.0.0.1，我公司也可以用 10.0.0.1，互不影响。 禁止路由： 严格规定，所有互联网骨干网上的路由器，绝对不能转发源地址或目标地址是私有地址的数据包。 这完美地解决了最初的问题： 简化管理： 公司内部可以随意规划自己的私有地址。即使更换 ISP，也只需要更改边界路由器 (NAT 设备) 的公网 IP 配置即可，内部上万台设备的 IP 地址完全不需要变动。 隔离网络： 私有地址天生无法在公网上路由，从而实现了内部网络和外部互联网的天然隔离。 最终，随着 90 年代中后期互联网的爆炸式普及，人们很快发现，这个 “无心插柳” 的设计，恰好是解决日益严峻的地址耗尽危机的 “神丹妙药”。 NAT NAT 的全称是 “Network Address Translation”，即 “网络地址转换”，它负责在 “内网 IP” 和 “公网 IP” 之间进行 “转换”。它的工作原理是这样的： 出站（内网设备访问公网）： 手机（内网 IP 192.168.1.101 访问 Google 网站） 它会准备一个“数据包”，包含以下内容： 目标 IP: Google 服务器的公网 IP。 源 IP: 192.168.1.101 (手机的内网 IP)。 源 端口 数据包到达路由器 执行 NAT： 路由器中会有一个 “NAT 转换表” (NAT Translation Table)，这是一个内部记账本。它执行以下操作： 使用家庭的 “公网 IP” 替换数据包中的内网 IP。同时分配一个新的端口。 在 NAT 转换表中记录内网 IP 和公网 IP 的映射，比如：“192.168.1.101:49152 &lt;-&gt; 203.0.113.50:30001” 数据包发往互联网: 从 Google 的服务器拉看，这个请求就是从 203.0.113.50:30001 发出的，它完全不知道手机的存在。 响应服务器返回的数据就是上述步骤的反面，即根据 “公网IP+端口”（起到主要作用的就是端口，毕竟公网IP 都是一致的），找到“内网IP+端口”，最终将相应数据包发送到请求数据的手机上。下面的序列图描绘了这一来一回的完整过程： 总结 NAT 的核心机制： 地址转换： 将数据包的源 IP 地址从 内网 IP 替换为 公网 IP (出站时)，或者将目标 IP 地址从 公网 IP 替换为 内网 IP (入站时)。 端口转换 (PAT - Port Address Translation)：这是现代 NAT 的精髓，也称为 NAPT。通过 修改源端口号，使得同一个公网 IP 地址可以同时为多个内网设备提供服务。路由器通过维护 (内网IP:内网端口) &lt;-&gt; (公网IP:公网端口) 这个映射表，来确保返回的数据能够被准确地分发给正确的设备和应用。 巨大的 “局域网” APNIC (Asia-Pacific Network Information Centre)，即亚太网络信息中心，它负责亚太地区 IP 地址分配的区域互联网注册机构 (RIR)。根据 APNIC 截至目前的公开统计数据，分配给中国 (国家代码: CN) 的公网 IPv4 地址总数大约为 3.45 亿 个。我们也可以通过 ipinfo 查到这个数据。 根据这个数据可想而知，3.45 亿是远远不够用的，那么常见的方法就是使用上面提到的 “内网 IP” 方案，通过运营商架设基础设施，来实现一张巨大的 “局域网”，以此来解决公网 ip 不足的问题。 端口 说到 IP，就不得不提到 “端口” 这个概念。其实在上文中也已经多次涉及到这个概念了。那么 “端口” 又是什么呢？ IP 地址的作用是为了找到设备，而端口的作用则是为了找到设备中的服务。在技术细节方面： 端口是一个 16 位 (bit) 的无符号整数 取值范围从 0 到 2¹⁶ - 1，即 0 到 65535 以上面 NAT 的场景为例，对于公网 IP 而言，每个服务发出的数据包都需要自己独一无二的端口。如果端口重复了，NAT 就不知道该把公网中服务器响应的数据发送给哪个内网设备。 端口的分类 端口也像 IP 一样，有一些硬性分配的保留项。 知名端口 范围: 0 - 1023 特点与用途: 这类端口由 IANA (互联网号码分配局) 进行严格的 硬性分配，它们是全球统一的互联网标准。这些端口被永久地分配给了互联网上最核心、最基础的服务，就像是公开的、全世界都知道地址的“政府办事大厅” 或 “公共服务窗口”，确保了全球网络服务的互操作性。在大多数操作系统中，应用程序需要拥有管理员（root）权限才能绑定和监听这些端口。 常见协议与服务示例: 20/21: FTP (文件传输协议) 22: SSH (安全外壳协议)，用于安全的远程登录。 23: Telnet (远程终端协议)，一个早期的、不安全的远程登录协议。 25: SMTP (简单邮件传输协议)，用于发送电子邮件。 53: DNS (域名系统)，用于将域名解析为 IP 地址。 80: HTTP (超文本传输协议)，用于访问普通网页。 110: POP3 (邮局协议版本3)，用于接收电子邮件。 143: IMAP (互联网消息访问协议)，也是用于接收邮件，功能更强大。 443: HTTPS (安全的超文本传输协议)，用于访问加密的网页。 注册端口 范围: 1024 - 49151 特点与用途: 这些端口由 IANA 进行 半官方分配，供特定的应用程序、协议或商业公司注册使用，目的是为了避免不同软件间的端口冲突。虽然它们不像知名端口那样是强制性的互联网标准，但业界普遍会遵守这些约定。普通用户权限的程序就可以绑定这些端口，无需特殊的管理员权限。 常见协议与服务示例: 1433: Microsoft SQL Server 数据库服务 1521: Oracle 数据库服务 3306: MySQL 数据库服务 3389: RDP (Windows 远程桌面协议) 5432: PostgreSQL 数据库服务 5900: VNC (虚拟网络计算)，一个常用的远程控制软件。 6379: Redis 键值数据库服务 8080: 常被用作 Web 服务器的备用端口，或用于运行 HTTP 代理服务器。 动态/私有端口 范围: 49152 - 65535 特点与用途: 这些端口 不分配给任何固定的服务，它们是完全自由使用的。它们的主要作用是作为客户端程序（例如浏览器、游戏、邮件客户端）在发起对外连接时，由操作系统 随机选择 一个作为本次通信的 源端口 (Source Port)。当通信会话结束后，这个端口就会被释放，可以被其他应用重新使用。因为它们是临时分配的，所以也被称为 临时端口 (Ephemeral Ports)。 常见协议与服务示例: 这类端口没有固定的服务对应，它们的使用场景是动态的。例如： 当用浏览器访问 google.com 时，电脑可能会随机选择 51234 作为本次通信的源端口。 当启动一个在线游戏时，游戏客户端可能会随机选择 62000 作为与游戏服务器通信的源端口。 当使用邮件客户端收取邮件时，它可能会随机选择 58341 作为连接邮件服务器的源端口。 端口映射 端口映射（或者更广义地称为“端口转发”，Port Forwarding）是一个非常基础且应用广泛的网络技术。这项技术可以将一个端口映射成另外一个端口，比如说我们经常使用的 Docker。 想象一下这个场景：你需要用 Docker 部署一个 Gitlab 服务。等到服务部署好了之后，Gitlab 服务会监听 443 这个 https 端口。但是用户无法从主机上直接访问 Docker 容器内的 443 端口，因为众所周知 Docker 的环境和主机环境是互相隔离的。所以我们要是用 Docker 的 端口映射 （-p 参数）来说实现端口转发。 Docker 环境和主机环境互相隔离，这也意味着 Docker 内部有一套自己的端口，从 0 到 65535。 通过端口转发，我们可以将主机上的任意一个非保留端口端口，比如说 4343，映射到 Docker 容器的 443 端口。之后任何访问主机 4343 端口的请求，都会被转发给 Docker 的 443 端口，也就是 Gitlab 服务。 端口映射还有一个作用，那就是绕开 ISP 屏蔽。上文中也有提到 ISP 这个概念，它的全称是 Internet Service Provider，中文翻译是 互联网服务提供商，也就是电信呀，联通、移动这些运营商。 根据中国工信部的规定，所有向公众提供服务的网站都必须进行 ICP 备案。为了落实这一政策，ISP 会直接屏蔽家庭宽带的 80 端口，从技术上阻止个人用户在未经备案的情况下，私自搭建对外的 Web 服务器。其实不止 80 端口，很多端口可能都会因为各种各样的原因导致被 ISP 屏蔽。这个时候就需要通过端口映射功能来 “绕开” ISP 的屏蔽。 有些时候 ISP 屏蔽端口是为了保护主机的安全，比如 25、 445、135、137、138、139 等端口。这些端口经常会成为黑客的攻击对象。 ","link":"https://blog.rakuyoo.top/computer-network-ip-and-port/"},{"title":"Surge For Mac 的一些注意事项","content":"记一些日常使用时的小细节吧，也谈不上使用教程。 增强模式下终端不走代理 我在官方论坛上提过一个同名问题：Surge5 Mac 增强模式下终端不走代理，最后发现是 pfctl 的问题。 在我现在使用的电脑上它是开机自启的，并且在我调试 Surge 期间并没有关闭它，最终导致了这个问题。 可以通过在命令行中执行 sudo pfctl -d 来临时关闭它。 托管配置 官方文档见 托管配置。 Surge 的配置直接托管到服务器上，或者如果你使用某个机场，那么 Surge 也可以自动从 URL 更新配置文件。 只需要在你的配置文件最前端增加下面内容即可（记得把 url 替换成你自己的 url）： #!MANAGED-CONFIG http://test.com/surge.conf interval=60 strict=true 配置分离 当你的配置设置了从 url 自动加载，那么你就无法再在本地修改你的配置了，此时可以通过 配置分离 来解决这个问题。 单独建立一个配置文件，随后通过 #!include 引用你的远端配置。这样就可以做到既自动拉取机场配置，又能享受本地自定义配置。 一个注意点是，上面的文档中有这么一段话： The filename suffix is not required, if it is a complete profile, you can continue to use the conf suffix, if it is not a complete profile, it is recommended to use another suffix to avoid being displayed in the configuration list. 也就是说如果你的子配置文件不完整，那么可以不加后缀，避免该文件出现在 Surge 的配置文件列表里。 一般子配置文件的 [Rule] 不会包含也不能包含 FINAL，所以如果加载到 Surge 里之后会报错提示 “非法的配置文件，需要以 FINAL 结尾”（但是不会影响功能，仅仅是提示错误）。这个时候就可以通过去掉后缀或者修改后缀的方式将该文件从配置列表中 “隐藏” 掉。 抓包 - “wifi 无网络链接” 主包用的 Surge v6，其实从 Surge 5.8.1 版本开始就可能会遇到这个问题：在 iPhone wifi 上设置了 “配置代理” 后提示 “无互联网访问”，mac surge 上也不显示设备。 Surge 版本 5.8.1 中新增了一个配置 proxy-restricted-to-lan，就是它导致了这个问题。 这个配置默认启用，在配置文件中将其设置为 false 即可解决问题。 ","link":"https://blog.rakuyoo.top/notes-about-surge-for-mac/"},{"title":"从使用 Combine 拦截 delegate 看 Runtime 消息转发","content":"CombineCocoa 已有一段时间未更新，但其中许多代码仍能正常工作，比如 Delegate 的这部分。 今天我们先从这部分代码入手，看看如何使用 Combine 拦截 UIKit 中的各类 delegate 对象，再深入探讨其背后的 Runtime 实现。 使用 Combine 拦截 delegate 下面的代码和 CombineCocoa 中的不完全一致，我会做一些调整以及格式上的修改。 首先，我们需要明确最终想要实现的效果，这是做框架时常用的逆向思考方法。比如我们想要一个这样的方法去调用： // 代替 `UITableViewDelegate.tableView(_:willDisplay:forRowAt:)` tableView.willDisplayCellPublisher.sink { ... } /// Combine wrapper for tableView(_:willDisplay:forRowAt:)` public var willDisplayCellPublisher: AnyPublisher&lt;(cell: UITableViewCell, indexPath: IndexPath), Never&gt; { return /* 一个 Publisher 对象 */ } willDisplayCellPublisher 的定义位于 CombineCocoa 仓库的 UITableView+Combine.swift 文件。关键代码如下所示： @available(iOS 13.0, *) public extension UITableView { /// Combine wrapper for `tableView(_:willDisplay:forRowAt:)` var willDisplayCellPublisher: AnyPublisher&lt;(cell: UITableViewCell, indexPath: IndexPath), Never&gt; { let selector = #selector(UITableViewDelegate.tableView(_:willDisplay:forRowAt:)) return delegateProxy.interceptSelectorPublisher(selector) .map { ($0[1] as! UITableViewCell, $0[2] as! IndexPath) } .eraseToAnyPublisher() } override var delegateProxy: DelegateProxy { TableViewDelegateProxy.createDelegateProxy(for: self) } } @available(iOS 13.0, *) private class TableViewDelegateProxy: DelegateProxy, UITableViewDelegate, DelegateProxyType { func setDelegate(to object: UITableView) { object.delegate = self } } 实现看起来稍显复杂，我们逐步分析，先来看 willDisplayCellPublisher 这个计算属性。从 interceptSelectorPublisher 方法的字面含义上来看，猜测它是拦截了 #selector(UITableViewDelegate.tableView(_:willDisplay:forRowAt:)) 方法，将相关调用封装为 Publisher 供外部订阅。 delegateProxy 属性本质上是 TableViewDelegateProxy 类型，其直接父类为 DelegateProxy，而它的父类最终是 ObjcDelegateProxy，这是一个纯 Objective‑C 类型。 让我们先跳过 DelegateProxy，看看 ObjcDelegateProxy 都干了些什么。 在看之前我们应该就能想到，它里面一定涉及到了 ObjC runtime 的使用，否则没有必要特地使用 ObjC 相关代码位于 Runtime 目录下。 // Runtime/include/ObjcDelegateProxy.h #import &lt;Foundation/Foundation.h&gt; @interface ObjcDelegateProxy: NSObject @property (nonnull, strong, atomic, readonly) NSSet &lt;NSValue *&gt; *selectors; /// 当拦截到对应选择子时被调用，参数为方法名和参数列表 - (void)interceptedSelector:(SEL _Nonnull)selector arguments:(NSArray * _Nonnull)arguments; - (BOOL)respondsToSelector:(SEL _Nonnull)aSelector; - (BOOL)canRespondToSelector:(SEL _Nonnull)selector; @end 从接口设计上来说这个类型暴露了比较多的方法，其中 interceptedSelector 方法是在 willDisplayCellPublisher 中用到的那个，一会儿重点关注一下。 再看看 .m 文件中的实现： 我省略了查找方法选择子相关方法的实现，本文重点关注方法转发相关的部分。 /// Runtime/ObjcDelegateProxy.m #import &lt;Foundation/Foundation.h&gt; #import &quot;include/ObjcDelegateProxy.h&quot; #import &lt;objc/runtime.h&gt; /// 辅助宏：将对象包装成 NSValue，以非保留方式持有 #define OBJECT_VALUE(object) [NSValue valueWithNonretainedObject:(object)] /// 全局静态字典：Key 为代理类，Value 为该类声明的所有拦截选择子集合 static NSMutableDictionary&lt;NSValue *, NSSet&lt;NSValue *&gt; *&gt; *allSelectors; @implementation ObjcDelegateProxy - (NSSet *)selectors { return allSelectors[OBJECT_VALUE(self.class)]; } + (void)initialize { @synchronized (ObjcDelegateProxy.class) { if (!allSelectors) { allSelectors = [NSMutableDictionary new]; } // 获取本类及其父类中所有 returnType 为 void 的方法选择子 allSelectors[OBJECT_VALUE(self)] = [self selectorsOfClass:self withEncodedReturnType:[NSString stringWithFormat:@&quot;%s&quot;, @encode(void)]]; } } - (BOOL)respondsToSelector:(SEL _Nonnull)aSelector { return [super respondsToSelector:aSelector] || [self canRespondToSelector:aSelector]; } - (BOOL)canRespondToSelector:(SEL _Nonnull)selector { for (id current in allSelectors[OBJECT_VALUE(self.class)]) { if (selector == (SEL) [current pointerValue]) { return true; } } return false; } - (void)interceptedSelector:(SEL _Nonnull)selector arguments:(NSArray * _Nonnull)arguments { // 默认实现，子类可重写以处理拦截到的方法 } - (void)forwardInvocation:(NSInvocation *)anInvocation { NSArray * _Nonnull arguments = unpackInvocation(anInvocation); [self interceptedSelector:anInvocation.selector arguments:arguments]; } /// 从一个 NSInvocation 对象中提取出参数值，并包装为一个 NSArray&lt;id&gt; 返回 NSArray * _Nonnull unpackInvocation(NSInvocation * _Nonnull invocation) { NSUInteger numberOfArguments = invocation.methodSignature.numberOfArguments; NSMutableArray *arguments = [NSMutableArray arrayWithCapacity:numberOfArguments - 2]; // 忽略前两个参数：self 和 _cmd（Objective-C 的默认前两个参数） for (NSUInteger index = 2; index &lt; numberOfArguments; ++index) { const char *argumentType = [invocation.methodSignature getArgumentTypeAtIndex:index]; // Skip const type qualifier. if (argumentType[0] == 'r') { argumentType++; } // 辅助宏：判断参数类型 #define isArgumentType(type) \\ strcmp(argumentType, @encode(type)) == 0 // 辅助宏：提取基础数值类型参数，并转为 NSNumber #define extractTypeAndSetValue(type, value) \\ type argument = 0; \\ [invocation getArgument:&amp;argument atIndex:index]; \\ value = @(argument); \\ id _Nonnull value; // 引用类型或 block 类型，直接解包 if (isArgumentType(id) || isArgumentType(Class) || isArgumentType(void (^)(void))) { __unsafe_unretained id argument = nil; [invocation getArgument:&amp;argument atIndex:index]; value = argument; } // 以下全部是基本类型，统一转为 NSNumber else if (isArgumentType(char)) { extractTypeAndSetValue(char, value); } else if (isArgumentType(short)) { extractTypeAndSetValue(short, value); } else if (isArgumentType(int)) { extractTypeAndSetValue(int, value); } else if (isArgumentType(long)) { extractTypeAndSetValue(long, value); } else if (isArgumentType(long long)) { extractTypeAndSetValue(long long, value); } else if (isArgumentType(unsigned char)) { extractTypeAndSetValue(unsigned char, value); } else if (isArgumentType(unsigned short)) { extractTypeAndSetValue(unsigned short, value); } else if (isArgumentType(unsigned int)) { extractTypeAndSetValue(unsigned int, value); } else if (isArgumentType(unsigned long)) { extractTypeAndSetValue(unsigned long, value); } else if (isArgumentType(unsigned long long)) { extractTypeAndSetValue(unsigned long long, value); } else if (isArgumentType(float)) { extractTypeAndSetValue(float, value); } else if (isArgumentType(double)) { extractTypeAndSetValue(double, value); } else if (isArgumentType(BOOL)) { extractTypeAndSetValue(BOOL, value); } else if (isArgumentType(const char *)) { extractTypeAndSetValue(const char *, value); } // 如果不是任何已知类型，就用 NSValue 包裹原始数据，比如 CGRect、CGSize else { NSUInteger size = 0; NSGetSizeAndAlignment(argumentType, &amp;size, NULL); NSCParameterAssert(size &gt; 0); // 使用栈内存来存储未知数据 uint8_t data[size]; [invocation getArgument:&amp;data atIndex:index]; value = [NSValue valueWithBytes:&amp;data objCType:argumentType]; } [arguments addObject:value]; } return arguments; } + (NSSet &lt;NSValue *&gt; *) selectorsOfClass: (Class _Nonnull __unsafe_unretained) class withEncodedReturnType: (NSString *) encodedReturnType { // 省略相关实现 } + (NSSet &lt;NSValue *&gt; *) selectorsOfProtocol: (Protocol * __unsafe_unretained) protocol andEncodedReturnType: (NSString *) encodedReturnType { // 省略相关实现 } + (NSSet &lt;NSValue *&gt; *) selectorsOfProtocolPointer: (Protocol * __unsafe_unretained * _Nullable) pointer count: (NSInteger) count andEncodedReturnType: (NSString *) encodedReturnType { // 省略相关实现 } + (NSString *)encodedMethodReturnTypeForMethod: (struct objc_method_description) method { return [[NSString alloc] initWithBytes:method.types length:1 encoding:NSASCIIStringEncoding]; } @end 从源码得知，ObjcDelegateProxy 及其子类在初始化类型时调用 initialize 方法，方法内会以类型为 key，将所有返回类型为 void 的方法存储到 allSelectors 这个私有静态全局属性中。 respondsToSelector: 会先调用 canRespondToSelector:，后者会在 allSelectors 中查找该 selector。如果找到了，就返回 YES，使得运行时认为此方法已被实现，从而跳过消息转发的后续流程。 实际情况肯定是不会实现相关方法，接着就会通过消息转发流程执行 forwardInvocation 方法，将通过 unpackInvocation 方法拿到的参数，和方法选择子一起交给 interceptedSelector 方法，供子类覆写使用。 大致的流程如下所示： [系统调用 delegate 方法] ↓ [调用 respondsToSelector:] ↓ ObjcDelegateProxy 判断是否支持该 selector（从协议中收集） ↓ 系统继续调用该方法，但 proxy 未实现 ——&gt; ↓ 触发 forwardInvocation ↓ unpackInvocation -&gt; 拿到参数 ↓ interceptedSelector -&gt; 子类处理 ↓ 发出事件或自定义逻辑 相信现在你应该能明白为什么特地使用 ObjC 来实现了：Swift 不支持 NSInvocation，也无法重写 forwardInvocation 进行完整的消息转发。 回到 TableViewDelegateProxy，它除了间接继承自 ObjcDelegateProxy 之外，还遵循了 DelegateProxyType 协议，这个 swift 协议的定义就比较简单常见了： import Foundation private var associatedKey = &quot;delegateProxy&quot; public protocol DelegateProxyType { associatedtype Object func setDelegate(to object: Object) } @available(OSX 10.15, iOS 13.0, tvOS 13.0, watchOS 6.0, *) public extension DelegateProxyType where Self: DelegateProxy { static func createDelegateProxy(for object: Object) -&gt; Self { // 确保线程安全，避免在多线程环境下重复创建/状态竞争。 objc_sync_enter(self) defer { objc_sync_exit(self) } let delegateProxy: Self if let associatedObject = objc_getAssociatedObject(object, &amp;associatedKey) as? Self { delegateProxy = associatedObject } else { delegateProxy = .init() objc_setAssociatedObject(object, &amp;associatedKey, delegateProxy, .OBJC_ASSOCIATION_RETAIN) } delegateProxy.setDelegate(to: object) return delegateProxy } } 通过这个协议，我们可以便捷地为 DelegateProxy 子类添加 createDelegateProxy(_:) 方法，方法内部使用懒加载，创建了一个关联对象，随后调用 setDelegate(to:) 方法，将这个关联对象设置为 object 的代理。 看完这些前置条件，我们可以看一看 DelegateProxy 这个类型了（DelegateProxy.swift）： @available(OSX 10.15, iOS 13.0, tvOS 13.0, watchOS 6.0, *) open class DelegateProxy: ObjcDelegateProxy { private var dict: [Selector: [([Any]) -&gt; Void]] = [:] private var subscribers = [AnySubscriber&lt;[Any], Never&gt;?]() ... public override func interceptedSelector(_ selector: Selector, arguments: [Any]) { dict[selector]?.forEach { handler in handler(arguments) } } public func intercept(_ selector: Selector, _ handler: @escaping ([Any]) -&gt; Void) { if dict[selector] != nil { dict[selector]?.append(handler) } else { dict[selector] = [handler] } } public func interceptSelectorPublisher(_ selector: Selector) -&gt; AnyPublisher&lt;[Any], Never&gt; { DelegateProxyPublisher&lt;[Any]&gt; { subscriber in // 为了在 deinit 中释放，需要先存储起来 self.subscribers.append(subscriber) return self.intercept(selector) { args in _ = subscriber.receive(args) } }.eraseToAnyPublisher() } deinit { subscribers.forEach { $0?.receive(completion: .finished) } subscribers = [] } } 现在我们可以明白，在 willDisplayCellPublisher 中调用的 interceptSelectorPublisher(_:) 方法，将 #selector(UITableViewDelegate.tableView(_:willDisplay:forRowAt:)) 作为 key，用于发送数据的闭包作为 value，存储到 dict 这个字典属性中。这个过程可以看做一个 “注册” 流程。 当消息转发被触发时，interceptedSelector(_: _:) 方法会被调用，通过参数 selector 从 dict 中取出数据发送事件，再将参数作为 value 发送出去。 DelegateProxyPublisher 是一个比较简单的自定义 Publisher 对象，它被定义在 DelegateProxyPublisher，这里就不对其做过多的描述，读者可以翻阅源码自行查看。 总结 至此，相信大家已经清楚了 CombineCocoa 如何利用 Combine 封装 Objective‑C 消息转发来拦截 delegate 调用，它的本质就是使用 Combine 框架，封装消息转发的结果。至于关联对象的使用则是锦上添花。 而且我们也可以明白，为什么 CombineCocoa 这套框架只能拦截没有返回值的代理方法，因为它的 allSelectors 属性里只存储了返回值为 void 的方法。 说到这里，再看看标题，既然提到了 Runtime 的消息转发，就让我们再回顾一下消息转发的相关知识。 Runtime 消息转发 在 ObjC 中，方法的调用都是通过 objc_msgSend 来进行，我们一般称之为 “发送消息”。整个发送消息的流程大致如下所示： ↓ [1] objc_msgSend(obj, sel) ↓ [2] 方法未找到 -&gt; runtime 会调用： +resolveInstanceMethod: 或 +resolveClassMethod: ↓ [3] 仍未处理？调用： - forwardingTargetForSelector: （提供另一个对象转发 selector） ↓ [4] 还不行？runtime 调用： - methodSignatureForSelector: ↓（你必须返回一个 NSMethodSignature） [5] 然后调用： - forwardInvocation: ↓ [6] 如果 forwardInvocation 没处理，会 crash: unrecognized selector sent to instance 这也就是我们常说的 “使用 ObjC 调用一个未实现的方法时，未必会触发崩溃”，因为我们可以在运行时动态添加实现，或者使用消息转发。 动态方法解析 第一阶段被称之为 “动态方法解析”（Dynamic Method Resolution），即 +resolveInstanceMethod: 或 +resolveClassMethod: 方法中，在运行时动态地添加一个方法。 快速转发 第二阶段被称之为 “快速转发”（Fast Forwarding），即 -forwardingTargetForSelector: 方法中把这次函数调用转发给另外一个对象。 完整消息转发 如果这两步都没处理，则会触发第三阶段：“完整消息转发”（Full Message Forwarding），或者叫 “标准消息转发”（Standard Message Forwarding），也就是上文提到的 -methodSignatureForSelector: + -forwardInvocation: 这两个方法。 -methodSignatureForSelector: 是调用 -forwardInvocation: 的前提，该方法负责返回方法的签名信息，也就是参数数量、类型、返回值类型等，为 NSMethodSignature 类型。 NSObject 有一个默认的 -methodSignatureForSelector: 方法实现，默认实现会去当前类的 method list 中查找是否存在名为 selector 的方法，如果找到了，就创建并返回一个对应的 NSMethodSignature；否则返回 nil。如果你不重写它，而 selector 确实不存在，默认实现就会返回 nil，这会直接导致 runtime 抛出 unrecognized selector sent to instance 异常，不会进入 -forwardInvocation:。 在 CombineCocoa 的场景中，ObjcDelegateProxy 没有覆写 -methodSignatureForSelector: 方法，但是因为其子类实现了对应的代理协议，比如 UITableViewDelegate，所以相关的 @objc 方法会被添加到方法列表中，所以 -methodSignatureForSelector: 能够生成正确的 NSMethodSignature 对象，进而调用到 -forwardInvocation: 方法。 ","link":"https://blog.rakuyoo.top/combine-intercept-delegate-objc-runtime/"},{"title":"通过覆写方法看 Swift 方法派发","content":"在 Swift 开发中，我们常常利用 extension 和 // MARK: 来划分代码逻辑，比如划分出 “Config”、“Life cycle” 等模块。然而，在使用扩展时却常常会遇到方法覆写的问题，尤其是在定义基类的“框架方法”后，子类无法正确覆写的问题。 本文将通过覆写方法，深入探讨 Swift 中的方法派发机制，并讨论如何在代码设计时规避常见坑点。 本文基于 Swift 5 与 Xcode 16.2 编写。 何为 Swift 方法派发 首先，为不了解 “方法派发” 的读者做个简短的概念解释： 方法派发简单地讲，指的是在调用一个方法时，如何执行该方法的机制。也可以理解为 “去哪里找到该方法的实现” 的机制。 广义上说 Swift 的方法派发分以下两种： 派发类型 定义 底层机制 静态派发 在编译期就确定调用目标 直接内联调用，不参与虚函数表 动态派发 在运行时根据对象的实际类型确定调用目标 依赖虚函数表（vtable）或 Objective‑C 消息发送机制 方法覆写与方法派发 在很多项目中，我们会在基类中定义一些“框架方法”，由子类覆写这些方法来实现统一的代码结构。比如下面的类型： open class BaseCollectionViewCell: UICollectionViewCell { override public init(frame: CGRect) { super.init(frame: frame) config() } public required init?(coder: NSCoder) { super.init(coder: coder) config() } } extension BaseCollectionViewCell { open func config() { // ⚠️ Non-'@objc' instance method in extensions cannot be overridden; use 'public' instead addSubviews() addInitialLayout() } open func addSubviews() { } // ⚠️ Non-'@objc' instance method in extensions cannot be overridden; use 'public' instead open func addInitialLayout() { } // ⚠️ Non-'@objc' instance method in extensions cannot be overridden; use 'public' instead } config()、addSubviews() 和 addInitialLayout() 被我称为 “框架方法”，用来约束子类，统一管理某一类的代码。 写完方法后，Xcode 给了我们三个警告：Non-'@objc' instance method in extensions cannot be overridden; use 'public' instead。提示我们在 extension 中定义的非 @objc 方法不能被覆写的，所以我们使用的 open 是没有意义的，建议我们换成 public。 这是为什么呢？ Swift 中的方法覆写 我们先来了解一下 Swift 方法覆写的实现原理：在 Swift 中，方法覆写本质上是依赖 “动态派发机制” 来实现的。 对于一个纯 Swift 类而言，在类的主要声明部分定义的方法会被加入到虚函数表（vtable table）中。这意味着在运行时，根据对象的实际类型可以找到正确的实现，从而支持子类覆写。 在 Swift 中，虚函数表被称为 “witness table”，下文将会使用 ““witness” 这个单词代表 “虚函数表”。 而在 extension 中定义的方法，默认采用的是 “静态派发”，静态派发会在编译时直接绑定调用目标，不会被加入到 witness table 里，也不暴露在 Objective‑C 运行时中。 上一节中提到，子类覆写方法需要依靠虚函数表或 Objective‑C 运行时，所以定义在 extension 中的方法无法被子类继承。 改写 知道错误原因的你可能想到了两种解决方案： 可以将方法移动到主要声明部分，也就是通过虚函数表实现方法覆写。 或者按照警告的提示，添加 @objc，借助 Objective-C 运行时实现方法覆写。 我们先采用第一种方案，看看如何实现： open class BaseCollectionViewCell: UICollectionViewCell { ... open func config() { ... } ... } class SubCollectionViewCell: BaseCollectionViewCell { } extension SubCollectionViewCell { override func config() { } // ❌ Non-@objc instance method 'config()' declared in 'BaseCollectionViewCell' cannot be overridden from extension } 当我们尝试继承 BaseCollectionViewCell 去定义 SubCollectionViewCell 时，问题更严重了，Xcode 报了个错误： Non-@objc instance method 'config()' declared in 'BaseCollectionViewCell' cannot be overridden from extension 这其实也很好理解，根据虚函数表的原理，如果父类函数被覆写，那么表中只会保存被覆写之后的函数。所以尽管父类的 config() 方法在虚函数表中，但是子类的方法不在，这样在运行时就无法使用子类的实现替代父类的实现。 既然第一条路走不通，那么第二条路呢？比如下面这样： open class BaseCollectionViewCell: UICollectionViewCell { ... @objc open func config() { ... } ... } class SubCollectionViewCell: BaseCollectionViewCell { } extension SubCollectionViewCell { override func config() { } // ❌ Cannot override a non-dynamic class declaration from an extension } 很好，报错变了，证明有效，但不完全有效。在这个例子中，我们已经使用 @objc 将父类方法暴露给 Objective-C，但是这实际上并不意味着方法派发方式改为了 “Objective-C 消息派发”。 在 Swift 3 时代存在 @objc 的隐式推断，而在发布 Swift 4 时候，这个隐式推断被取消了。开发者在类的主要声明（请先留意这一点，后文中还会提及）中，需要手动添加 dynamic 关键字，才能告诉编译器，将这个方法改为使用 “Objective-C 消息派发”。如果只添加 @objc，那么方法仅仅是被暴露给 Objective-C 而已，实际上还是被添加到虚函数表中，最终使用函数表派发。 所以正确的做法是为父类的方法定义添加 dynamic 关键字，就像下面这样： open class BaseCollectionViewCell: UICollectionViewCell { ... @objc dynamic open func config() { ... } ... } class SubCollectionViewCell: BaseCollectionViewCell { } extension SubCollectionViewCell { override func config() { } // ✅ } 好奇的你可能会注意到一个小细节：为什么子类方法光秃秃的，既不用添加 @objc，又不用添加 dynamic？ 这是因为尽管 Swift 4 已经取消了 @objc 隐式推断，但是在继承时这一推断被保留了下来，参考：Constructs that (still) infer @objc。所以在这个例子中，为了保持一致性，子类自动继承了父类方法的修饰。 extension 狂人 好了，现在你跟我说你是 extension 的狂热粉丝，你想将父类的框架方法也移到 extension 去定义，于是你将代码改成了这样： open class BaseCollectionViewCell: UICollectionViewCell { } extension BaseCollectionViewCell { @objc // ⚠️ 因为你的粗心，你遗漏了 `dynamic` 关键字。 open func config() { } } class SubCollectionViewCell: BaseCollectionViewCell { } extension SubCollectionViewCell { override func config() { } // ✅ } 让我们假设你是一个粗心的人（对不起），你在移动代码时丢失了 config() 方法上的 dynamic 关键字。此时你会发现子类的 config() 没有报错！等等，先别着急欢呼 “extension 万岁！”，让我们来看看为什么不需要 dynamic？ 答案是因为：在 extension 中定义方法并使用 @objc 修饰时，将自动转换为 Objective-C 消息派发。所以就不需要显式添加 dynamic 了。 再深入思考一下，“在 extension 中定义方法并使用 @objc 修饰时，将自动转换为 Objective-C 消息派发” 这么设计的原因是什么呢？ 我觉得是因为 “extension 中的方法无法被加入虚函数表”。使用 @objc 修饰后肯定无法使用静态派发，所以最终只能使用 Objective-C 消息派发了。 再再刨根问底一下，为什么 “extension 中的方法无法被加入虚函数表”？根据我目前的知识，这是 Swift 编译器做的硬性规定，并且与 Swift 的设计理念有关。本文就不展开讨论了。 纯 Swift 类 上面的示例代码中，所有的类都是继承自 NSObject，那么如果是一个纯 Swift 类呢？比如说下面这个类型： class Foo { func config() {} } class SubFoo: Foo { } extension SubFoo { override func config() {} // 🩺 这里会报错吗？报什么错？ } 结论和上面一样。不论是纯 Swift 类，还是一个继承自 NSObject 的类，在本文所讨论的场景中行为是一致的。 有朋友会提到 Swift 代码跨平台时的表现：添加了 @objc 和 dynamic 的纯 Swift 类在 Windows 或 Linus 平台上可以正常编译吗？答案肯定是否定的，Objective-C 运行时只存在于 Apple 平台，在上面两个平台中是没有的，所以代码无法通过编译。 总结 通过本文的讨论，相信你已经掌握了如何利用 extension 正确覆写父类的方法。 在实际开发中，虽然 Swift 的方法派发常被视作面试“八股文”般的基础知识，但是理解这些细微差别不仅能帮助你在设计类结构和方法定义时做出更明智的选择，还能在保证高效运行的同时，为后续的灵活扩展打下坚实的基础。 那么留一个课后思考题： class Foo { func config() {} } class SubFoo: Foo { } extension SubFoo { @objc override func config() {} // ❌ Non-@objc instance method 'config()' declared in 'Foo' cannot be overridden from extension } 上面这个代码中，父类和子类中的 config() 方法分别是哪种派发方式呢？欢迎在评论区留下你的答案。 写在最后 其实本篇文章的场景 “extension 的设计初衷是为类添加新的功能，而非修改已有方法” 是不符合 Swift 语言的设计初衷的。官方文档中是这么说的： Extensions can add new functionality to a type, but they can’t override existing functionality. 所以你可能会在看到文章开头时就说：“根本就不应该在 extension 中覆写方法！”。这也确实，Apple 并不推荐我们借助 ObjC 的方式在 extension 中覆写方法。 但是既然 Apple 允许开发者借助 ObjC 运行时的特性来实现这一功能，那么我们就姑且允许它存在吧，相信你有能力把握一个度，什么时候可以用这个功能，什么时候应尽量避免。 ","link":"https://blog.rakuyoo.top/swift-method-dispatch-via-overriding/"},{"title":"List files by size","content":"问 ChatGPT 写了一个 shell 脚本，可以在命令行中按照从大到小，递归地输出除了 .json 文件外的文件，包含文件大小和文件名。可用于辅助筛查项目中哪些图片过大。 单独脚本文件就不存到 GitHub 上了，在这里记录一下吧。 #!/bin/bash # 定义一个函数将字节大小转为人类可读格式 human_readable_size() { num=$1 # echo &quot;Converting size: $num bytes&quot; # Log: 转换字节大小 if [ &quot;$num&quot; -lt 1024 ]; then echo &quot;${num}B&quot; elif [ &quot;$num&quot; -lt 1048576 ]; then echo &quot;$((num / 1024))K&quot; elif [ &quot;$num&quot; -lt 1073741824 ]; then echo &quot;$((num / 1048576))M&quot; else echo &quot;$((num / 1073741824))G&quot; fi } # 临时文件存储所有文件信息 temp_file=$(mktemp) # echo &quot;Temporary file created: $temp_file&quot; # Log: 临时文件创建 # 使用 find 查找所有文件，排除 .json 文件 find . -type f ! -name &quot;*.json&quot; -print0 | while IFS= read -r -d '' file; do # 获取文件大小（以字节为单位，macOS 使用 stat -f%z） size=$(stat -f%z &quot;$file&quot;) # echo &quot;File: $file, Size: $size bytes&quot; # Log: 文件和大小 # 输出字节大小和文件路径到临时文件 echo &quot;$size $file&quot; &gt;&gt; &quot;$temp_file&quot; done # Log: 排序之前的临时文件内容 # echo &quot;Contents of temp file before sorting:&quot; # cat &quot;$temp_file&quot; # 按字节数降序排序，并输出格式化结果 sort -k1,1nr &quot;$temp_file&quot; | while read -r size file; do # echo &quot;Processing sorted file: $file, Size: $size bytes&quot; # Log: 处理排序后的文件 # 获取人类可读格式 readable_size=$(human_readable_size &quot;$size&quot;) # 使用 printf 来确保输出格式对齐 printf &quot;%-10s %-10s %s\\n&quot; &quot;$readable_size&quot; &quot;$size&quot; &quot;$file&quot; done # 删除临时文件 # echo &quot;Deleting temporary file: $temp_file&quot; # Log: 删除临时文件 rm &quot;$temp_file&quot; ","link":"https://blog.rakuyoo.top/list_files_by_size/"},{"title":"String Catalog 实践","content":"最近项目有了国际化（i18n）的需求，正好 WWDC23 上 Apple 推出了 String Catalog 功能。趁此机会尝试一下，并将一些经验记录下来。 如果你对 String Catalog 不太熟悉，可以先参考 老司机技术 WWDC 23 内参，或者直接观看 WWDC 的 官方视频 和 文档 来了解更多。 好了废话不多说，开始今天的内容。 开发环境 Xcode：16.2 Swift：5.9 框架：UIKit 最低支持版本：iOS 13 组件化工具：CocoaPods Xcode 15 及以上版本支持 String Catalog，版本对功能无特别限制。 但由于项目最低支持版本为 iOS 13，而 String(localized:) 是 iOS 15 引入的 API，因此需要额外处理低版本兼容的问题。 此外，关于 String Catalog 在 CocoaPods 组件化项目中的使用，相关资料较少，因此这部分内容需要实际摸索一下。 项目准备工作 首先，在项目的 PROJECT 设置中添加需要支持的语言： 接着可以（可选）调整 Scheme 的模拟器设置，将语言和地区设置为待测试的环境： 例如将语言设置为英语、地区为美国。此操作便于验证翻译效果，但地区设置通常不会影响文案展示。 String Catalog + CocoaPods 在主项目中使用 String Catalog 可以算作在 CocoaPods 中使用的一个子集，所以我们直接讲解如何在 CocoaPods 中使用 String Catalog。 新建文件 通过 Command+N 新建 String Catalog 文件，效果如下： 提示： 文件里显示的语言，是根据所属 .xcodeproj 设置里添加的语言自动生成的。 Pods.xcodeproj 默认包含多种语言，所以 Pods 中的 .xcstrings 文件才会显示这么多的语种。 但这不会影响最终的 App。鉴于我们平时在使用时，不会将 Pods.xcodeproj 提交到 git，同时 App 的多语言设置不受 Pods 的影响，所以作为开发者你完全可以无视这一点，只从 String Catalog 挑选你关注的语种即可。 这里我新建了一个文件，叫 HomeLocalizable.xcstrings，并加了两个文案。 修改 Podspec 文件 接着，我们需要修改 podspec，把 .xcstrings 文件加到资源文件中。 说到这里，聪明的你肯定可以直接猜到，是不是可以仿照 .xcassets 类型的文件，直接将 .xcstrings 文件放到 resource_bundle 的自定义 Bundle 中。没错，确实是这样： s.resource_bundle = { 'HomeBundle' =&gt; [ 'Sources/Resource/**/*.xcstrings', 'Sources/Resource/**/*.xcassets', ] } 注意： 如果只将 .xcstrings 放入 Bundle 中的话会有一个问题，详见 Pod 与 String Catalog 自动化 这一小节。 然后执行 pod install，这样 .xcstrings 文件就会被包含到 HomeBundle 里了。 添加新的文言 选择我们的默认语种，点击加号即可添加新的文言。此后同样的操作在需要翻译的语种中，设置对应文言的翻译内容即可。 需要注意的是，默认语种中文言的状态（State）默认是不展示，即 Reviewed 已审核过的，和其他语种中的绿色对勾是一样的。而当我们新建一个文言后，其他语种中文言的状态则是NEW。 我实际测试了一下，当我们向 NEW 状态的文言添加对应的翻译，或者修改了 NEEDS REVIEW 状态的文言所对应的翻译后，该条文言的状态将自动变为 Reviewed。 另外，就算文言处于 NEEDS REVIEW 状态，只要包含了对应的翻译，那么该翻译也是可以正常显示的。不过从字面上来看这一行为有些危险，稳妥起见建议开发者确保所有文言无误后，将其标记为 Reviewed 后再使用。 展示国际化文本 这一节我们就要提到 String(localized:) 的问题了。网上大部分介绍 String Catalog 的文章都会使用 String(localized: ) 这一个新的 api，然而当我实际使时才发现，它仅限于 iOS 15 + 的版本。那么该怎么办呢？ 其实我们还可以使用老的 NSLocalizedString 方法。 let text = NSLocalizedString( &quot;扫码或长按小程序码&quot;, tableName: &quot;HomeLocalizable&quot;, bundle: BundleToken.bundle, comment: &quot;&quot; ) private final class BundleToken { static let bundle: Bundle = { #if SWIFT_PACKAGE return Bundle.module #else guard let resourcePath = Bundle(for: BundleToken.self).resourcePath, let bundle = Bundle(path: resourcePath + &quot;/HomeBundle.bundle&quot;) // ⬅️ Watch this. else { fatalError(&quot;Bundle not found, please make sure the Bundle name is correct!&quot;) } return bundle #endif }() } tableName 是 .xcstrings 文件名，bundle 是在 podspec 里定义的 HomeBundle。 这里的 BundleToken 参考了 SwiftGen 的实现，做了一些适配 CocoaPods 的改动：增加 HomeBundle。 如果你好奇为什么使用老的 NSLocalizedString api 也可以，那么我们可以先 build 编译一下项目，然后在 Products 目录找到编译好了之后的应用，最后找到 HomeBundle.bundle。 此时你就会发现，.xcstrings 虽然不像是 .bundle 一样是一个 “包”，但是在编译时，它会被编译为和 .xcstrings 同名的 .strings 文件，所以我们依然可以使用 NSLocalizedString api 来读取相关设置，这点在 WWDC 的视频中也有相关说明： SwiftGen 截止文章更新（2025.1.14），SwiftGen 最新版为 6.6.3，尚未正式支持 String Catalog。（SwiftGen 目前好像进入停更的状态） 不过有一个相关的 pr：Xcode 15 String catalog support #1124。 这个 pr 对 .xcstring 文件进行解析，解析后生成的内容和老版本 .string 解析后生成的内容类似。 顺带一提，这似乎也影响到了 Tuist 对于 String Catalog 的支持。 你可以考虑使用我 fork 的这个版本：6.6.4-alpha.0。通过 mise 安装时，可以按照下面的格式修改 mise.toml 文件： [tools] &quot;ubi:rakuyoMo/SwiftGen&quot; = { version = &quot;6.6.4-alpha.0&quot;, matching = &quot;swiftgen-6.6.4-alpha.0-macos.zip&quot;, exe = &quot;swiftgen&quot; } 如果你不想分叉自编译 SwiftGen 可以考虑尝试一个名为 xcstrings-tool 的新工具，Swift 开发，不支持 brew，但是支持通过 mise 安装 cli 单独使用，详见文档 一些注意 因为这是我第一次总结 i18n 相关的内容，所以也顺带记录一些问题或者注意事项。可能比较常见，比较基础。 换行符 如果在 .xcstrings 文件中直接输入 \\n 代表换行，在实际展示时是起不到换行的效果，而是会将 \\n 展示出来。从 json 文件或Git 中查看会发现，实际内容是 \\\\n，即转义过后的内容。 正确的方式是使用 option + return 组合键在 .xcstrings 文件中输入换行。或者不嫌麻烦的话可以手动修改 json 文件，效果是一样的（手动狗头）。 nslocalizedstring_key SwiftLint 中包含着一条名为 nslocalizedstring_key 的规则，文档在这里。 它建议不要动态传入 key，比如这样是不推荐的： // 不推荐 let key = &quot;example_key&quot; let text = NSLocalizedString(key, comment: &quot;&quot;) Pod 与 String Catalog 自动化 在 Xcode 15 中，如果设置了 Use Compiler to Extract Swift Strings 为 YES，那么 String Catalog 文件就可以自动追踪国际化文本，比如下面这个字符串： let test = NSLocalizedString(&quot;Test&quot;, tableName: &quot;TestLocalizable&quot;, comment: &quot;Test&quot;) 编译后会有一个 “Syncing Localizations” 的阶段，在该阶段这个字符串会被自动记录到 TestLocalizable.xcstrings 文件中。 但是在 pod 组件化项目中，如果该字符串在某个 pod 内，则自动追踪不会生效。原因有以下几点： “Syncing Localizations” 阶段仅会扫描当前 Scheme 所包含的 target。一般我们编译时都会选择主项目，此时是不包含 pod target 的，所以不会扫描相应的代码。 Pod 的 Use Compiler to Extract Swift Strings 设置项默认为 NO，需要在 Podfile 中使用钩子手动开启（Key 为 SWIFT_EMIT_LOC_STRINGS）。 前文中，.xcstrings 被添加到 Bundle 中，这意味着它会被归入 Bundle 这个 target，比如 Pod 名为 Home，那么 .xcstrings 文件就在 Home-HomeBundle 这个 target 中。而项目代码则在 Home 这个 target 里。此时编译 Home，因为该 target 实际不包含 .xcstrings，所以字符串也不会同步到 .xcstrings 文件中。 综上，考虑到 CocoaPods 已经停止增加新功能，建议还是尽快迁移到 SPM。 ","link":"https://blog.rakuyoo.top/string-catalog/"},{"title":"HarmonyOS 与 ArkTS 开发注意事项","content":"本文持续记录一些使用 ArkTS 进行 HarmonyOS 开发时遇到的一些问题以及解决方案。以及一些可能称不上是问题，但是在我看来有必要记录一下的点。 本文的主要结构将分为 ArkTS 和 HarmonyOS 两部分。同时内容多与 iOS 开发以及 Swift 语言进行对比。 因为代码高亮限制，本文中涉及到 ArkTS 的代码，都将使用 TypeScript 的语法高亮设置 ArkTS ArkTS 是 TS 的超集，同时也阉割了一些 TS 的用法。 本节所记录的内容是从一个 TS 零基础小白的视角出发所遇到的问题。所以某些内容未必属于 ArkTS 引入，也有可能是 TS 就存在。某些小节可能会指出该小节的内容是属于 ArkTS 独有，还是 TS 就有。但是大多数小节可能并不会做此区分。 async 与 await ArkTS 的异步协程不像是 Swift 那样，需要在方法声明上显式增加 async 关键字。 只要方法返回 Promise 类型，那么在调用时候就可以通过添加 await 关键字，来获取 Promise 里所包含的数据。 比如下面的方法： function getData(): Promise&lt;string&gt; { return new Promise((resolve, reject) =&gt; { setTimeout(() =&gt; { resolve(&quot;Data fetched!&quot;); }, 1000); }); } function main() { const result = await getData(); // 可以使用 await console.log(result); // 输出: &quot;Data fetched!&quot; } 这其实是 TS 的语法。当你在 TypeScript 中声明一个函数为 async 时，该函数会自动返回一个 Promise 对象，而不是直接返回结果。即使函数内部没有显式地返回 Promise，async 函数会将其结果封装在一个 Promise 中。比如下面这样： async function myAsyncFunction() { return &quot;Hello, Async!&quot;; } // 调用方式 myAsyncFunction().then(result =&gt; console.log(result)); // 输出: &quot;Hello, Async!&quot; myAsyncFunction 因为被 async 修饰，所以自动返回了一个 Promise&lt;string&gt; 而不是单纯的 string。 所以如果方法没有标记 async，但是返回了 Promise，其实和标记了但是省略返回类型是一样的，自然也可以在调时直接使用 await 同步获取内容了。 HarmonyOS 环境变量配置 这里说的环境变量是 “正式环境”、“预上线环境” 以及 “测试环境”。包含接口环境配置以及一些 debug 入口等等。 在 iOS 开发中我们有预编译宏，但是 HarmonyOS 这块儿没有这个。HarmonyOS 的方法是在各个子模块中建立 target 的 sourceRoots 字段，将不同环境的代码添加到不同的 target 中，然后 entry 再引用不同的 target，以此来达到不同环境加载不同代码的需求。 本节不详细展开具体的详细内容，详细内容见官方文档。 在做这一项配置的过程中，有一个概念很容易搞错，或者说官方文档的讲解流程和实际上配置的流程有一定的出入。（当然也有可能是我个人能力不足导致理解错误）。 我们来看下面几个 build-profile.json5 文件。首先是一个 feature module 的： // feature/build-profile.json5 { &quot;apiType&quot;: &quot;stageMode&quot;, &quot;buildOption&quot;: {}, &quot;targets&quot;: [ { &quot;name&quot;: &quot;default&quot;, &quot;source&quot;: { &quot;sourceRoots&quot;: [&quot;./src/default&quot;] // 配置target为default的差异化代码空间 } }, { &quot;name&quot;: &quot;custom&quot;, &quot;source&quot;: { &quot;sourceRoots&quot;: [&quot;./src/custom&quot;] // 配置target为custom的差异化代码空间 } } ] } 其包含两个 target，使用 sourceRoots 引用了不同的代码。 然后再看 entry 的： { &quot;apiType&quot;: &quot;stageMode&quot;, &quot;buildOption&quot;: {}, &quot;targets&quot;: [ { &quot;name&quot;: &quot;default&quot;, }, ] } entry 只包含一个 default target。 最后看工程的： { &quot;app&quot;: { &quot;signingConfigs&quot;: [], &quot;products&quot;: [ { &quot;name&quot;: &quot;default&quot;, &quot;signingConfig&quot;: &quot;default&quot;, &quot;compatibleSdkVersion&quot;: &quot;5.0.0(12)&quot;, &quot;runtimeOS&quot;: &quot;HarmonyOS&quot;, }, { &quot;name&quot;: &quot;custom&quot;, &quot;signingConfig&quot;: &quot;default&quot;, &quot;compatibleSdkVersion&quot;: &quot;5.0.0(12)&quot;, &quot;runtimeOS&quot;: &quot;HarmonyOS&quot;, } ], &quot;buildModeSet&quot;: [ { &quot;name&quot;: &quot;debug&quot;, }, { &quot;name&quot;: &quot;release&quot; } ] }, &quot;modules&quot;: [ { &quot;name&quot;: &quot;entry&quot;, &quot;srcPath&quot;: &quot;./entry&quot;, &quot;targets&quot;: [ { &quot;name&quot;: &quot;default&quot;, &quot;applyToProducts&quot;: [ &quot;default&quot;, &quot;custom&quot;, ] }, ] }, { &quot;name&quot;: &quot;feature&quot;, &quot;srcPath&quot;: &quot;./feature&quot;, &quot;targets&quot;: [ { &quot;name&quot;: &quot;default&quot;, &quot;applyToProducts&quot;: [ &quot;default&quot; ] }, { &quot;name&quot;: &quot;custom&quot;, &quot;applyToProducts&quot;: [ &quot;custom&quot; ] }, ] }, ] } 在阅读官方文档后，我首先的想法就是上面这三个配置文件。定义 default 和 custom 两个 product，然后 entry 作为入口，添加到两个 product 中。feature/default 添加到 default；feature/custom 添加到 custom。这样 default product 包含 entry 和 feature/default，custom product 包含 entry 和 feature/custom。思路看着没问题对吧，然而实际运行起来会发现配置不生效：custom product 使用的是 feature/default。 问题在于思路不对，正确的思路应该是：将 entry 中定义的 target，下放给各个 module 使用。也就是说要先在 entry 中定义各个 target，然后再在各个 module 中使用 entry 定义的 target，自定向下使用。 比如有5个子 module，各自都有不同的 target 需求，分别是 1-5，那么 entry 中就应该定义这 1-5个 target，然后5个子 module 从中选取自己需要的那一个。 所以上述例子中的配置文件应该改成下面这样，先看 entry 的 build-profile.json5： { &quot;apiType&quot;: &quot;stageMode&quot;, &quot;buildOption&quot;: {}, &quot;targets&quot;: [ { &quot;name&quot;: &quot;default&quot;, }, { &quot;name&quot;: &quot;custom&quot;, // 增加 custom target }, ] } 然后修改工程级 build-profile.json5： { &quot;app&quot;: { &quot;signingConfigs&quot;: [], &quot;products&quot;: [ { &quot;name&quot;: &quot;default&quot;, &quot;signingConfig&quot;: &quot;default&quot;, &quot;compatibleSdkVersion&quot;: &quot;5.0.0(12)&quot;, &quot;runtimeOS&quot;: &quot;HarmonyOS&quot;, }, { &quot;name&quot;: &quot;custom&quot;, &quot;signingConfig&quot;: &quot;default&quot;, &quot;compatibleSdkVersion&quot;: &quot;5.0.0(12)&quot;, &quot;runtimeOS&quot;: &quot;HarmonyOS&quot;, } ], &quot;buildModeSet&quot;: [ { &quot;name&quot;: &quot;debug&quot;, }, { &quot;name&quot;: &quot;release&quot; } ] }, &quot;modules&quot;: [ { &quot;name&quot;: &quot;entry&quot;, &quot;srcPath&quot;: &quot;./entry&quot;, &quot;targets&quot;: [ { &quot;name&quot;: &quot;default&quot;, &quot;applyToProducts&quot;: [ &quot;default&quot;, ] }, { &quot;name&quot;: &quot;custom&quot;, &quot;applyToProducts&quot;: [ &quot;custom&quot;, ] }, ] }, { &quot;name&quot;: &quot;feature&quot;, &quot;srcPath&quot;: &quot;./feature&quot;, &quot;targets&quot;: [ { &quot;name&quot;: &quot;default&quot;, &quot;applyToProducts&quot;: [ &quot;default&quot; ] }, { &quot;name&quot;: &quot;custom&quot;, &quot;applyToProducts&quot;: [ &quot;custom&quot; ] }, ] }, ] } 这么修改后再运行，custom product 就可以正确地读取到 feature/custom 了。 最后再来说一说官方中的例子。官方上的示例使用 entry 演示 sourceRoots 的用法，其实暗藏了 “entry 也需要定义对应的 target” 这一点，只不过没有明说，让读者忽略了这一关键内容。 ","link":"https://blog.rakuyoo.top/notes-on-harmonyos-and-arkts-development/"},{"title":"Gitlab-CI 与 Shell 会话模式","content":"之前的文章中介绍过，iOS 常用的 Gitlab-CI 执行器是 shell，因为打包时只能使用这个。 今天将包管理工具迁到 mise 时遇到了一些和 shell 有关的问题，涉及到一些之前的知识盲区，遂记录一下。 本文的内容和 mise 无关，只是工具迁移时引出了本文涉及到的问题和知识盲区。对工具感兴趣的朋友可以点击上面的链接查看相关内容。 Shell 会话模式 先介绍一下 Shell 的几种会话模式。 我们都知道 macOS 的 默认终端环境是 zsh，zsh 有多个配套的文件，包含 ~/.zshrc、.zprofile 等。另一个比较常见的是 bash，可使用的配置文件有 ~/.bash_profile 、~/.bashrc 等。 之前我只模糊的知道有这些文件，可以在其中写入一些内容，然后启动终端后就会自动加载这些文件，却从来没有关注过 什么时候会加载哪个文件。这就涉及到 “Shell 会话模式” 这个概念。 “Shell 会话模式”是指用户启动会话时 shell（例如 bash、zsh 或其他命令行解释器）可以运行的不同方式。每种模式都规定了 shell 的行为方式、加载的初始化文件以及处理命令的方式。 此会话模式主要围绕两个概念：“是否是交互式” 以及 “是否登录”。这两种概念两两组合，诞生四种会话模式。 是否是交互式：指定用户是否可以通过键入命令直接与 shell 交互。shell 会等待用户输入并根据输入的命令提供输出。 是否登录：指开启会话时用户是否登录。 举几个例子： 交互式：打开 macOS 中的终端，此时我们可以在里面输入命令，与 shell 交互。 非交互式：Gitlab-CI 执行 script 时，就是非交互式。我们知道执行过程中我们没有办法做任何事，包括输入设备密码，say yes 等。 登录：通过 ssh 连接服务器，此时需要我们输入用户名密码。又或者执行命令时携带了 | /bin/bash --login 参数。 非登录：打开 macOS 的终端，默认是非登录的。又或者从一个已经登录了的终端中，使用 bash 或其他命令开启了新的终端窗口，这些也是非登录式的。 那么在不同的登录模式下，shell 会加载的配置文件其实是不同的： 会话类型 .bash_profile .bashrc .zprofile .zshrc 交互式登录 ✅ ❌ ✅ ❌ 交互式非登录 ❌ ✅ ❌ ✅ 非交互式登录 ✅ ❌ ❌ ❌ 非交互式非登录 ❌ ❌ ❌ ❌ 如果使用 bash，那么还有一个可能会使用的配置文件：~/.profile。它的作用和 ~/.bash_profile 类似，仅当 .bash_profile 不存在时，会尝试加载 .profile。另外，有的时候 .bash_profile 中会有命令，同时加载 .profile。 而 zsh 默认是不会尝试加载 .profile 的。 Gitlab-CI 的 script Gitlab-CI 的文档有列举出它所支持的 shell 种类，这里我们看默认的 bash。 贴一下文档中关于如何执行 script 的示例 # This command is used if the build should be executed in context of another user (the shell executor) cat generated-bash-script | su --shell /bin/bash --login user # This command is used if the build should be executed using the current user, but in a login environment cat generated-bash-script | /bin/bash --login # This command is used if the build should be executed in a Docker environment cat generated-bash-script | /bin/bash 从文档中可以看到，不论哪种方式，Gitlab-CI 都是使用 /bin/bash 环境执行的 script 中定义的脚本，并且在非 Docker 环境下，会携带 --login 参数，也就是说此时使用的是 “非交互式登录 Shell”，回看上面的表格，.bash_profile 文件会被加载。 此时就需要注意了，因为 macOS 默认的终端是 zsh，所以我们在编写脚本的时候，很可能会习惯性地将配置写入到 .zprofile 或者 .zshrc 文件中，但是回到 Gitlab-CI 中就会发现不生效，原因就是脚本的执行环境不同。 ","link":"https://blog.rakuyoo.top/gitlab-ci-and-shell-session-mode/"},{"title":"从源码看如何刷新 Epoxy 列表","content":"如何正确的刷新 Epoxy 列表可以说是熟练使用这个框架的关键，初上手时总会有 “为什么列表刷新后数据没变” 和 “为什么它刷新后数据就变了” 的疑惑。 本文从源码分析的角度，带大家来梳理 Epoxy 列表刷新的原理，并学会如何正确刷新 Epoxy 列表。 Epoxy 列表本质上还是 UICollectionView，只不过实现了 diff 算法，封装了 reuse id 的创建。 对于一个常见的 UICollectionViewCell 或 UITableViewCell，我的习惯是： Cell 初始化时创建 subviews，添加到 contentView 上，并设置布局。 对外暴露 config() 方法，或直接暴露 subviews 在 collectionView(_: cellForItemAt:) 方法中将数据设置到 subviews 上。 如果想要列表上 Cell 中的数据发生变化，无外乎以下几种方法： 创建不同的 Cell。不同的数据对应不同的 Cell，抛弃或部分抛弃重用机制。 在 Cell 创建或复用后，重新设置 subviews 上的数据。这包括两种方法： collectionView(_: cellForItemAt:) 方法中调用 config()，或直接使用 subviews。 使用响应式框架，比如 Combine，将数据绑定到 Cell 的 subviews 上。 那么接下来，我们就从这几种方法上入手，看看如何正确刷新 Epoxy 列表。 Cell 创建流程 首先我们需要知道 Epoxy 是如何创建一个 Cell。在使用框架时，我们没有显式创建过 reuse id，Epoxy 框架是如何创建它的？ 源码分析 在 CollectionView/Internal/CollectionViewDataSource.swift 中，关于 Cell 的创建如下所示： func collectionView( _ collectionView: UICollectionView, cellForItemAt indexPath: IndexPath) -&gt; UICollectionViewCell { guard let item = data?.item(at: indexPath), let section = data?.section(at: indexPath.section), let reuseID = reuseIDStore.registeredReuseID(for: item.viewDifferentiator) else { // The `item(…)` or `registeredReuseID(…)` methods will assert in this scenario. return UICollectionViewCell() } let cell = collectionView.dequeueReusableCell(withReuseIdentifier: reuseID, for: indexPath) if let cell = cell as? CollectionViewCell { self.collectionView?.configure( cell: cell, with: item, at: .init(itemDataID: item.dataID, section: .dataID(section.dataID)), animated: false) } else { EpoxyLogger.shared.assertionFailure( &quot;Only CollectionViewCell and subclasses are allowed in a CollectionView.&quot;) } return cell } 首先关注创建 Cell 时使用的标识符，即 reuseID 这个变量是从何而来的。 let reuseID = reuseIDStore.registeredReuseID(for: item.viewDifferentiator) 这里包含两个需要明白的内容，分别是 reuseIDStore 和 viewDifferentiator。 从后往前看，viewDifferentiator 定义在 ViewDifferentiatorProviding 协议里，它的类型 ViewDifferentiator 是一个遵循 Hashable 的结构体： // MARK: - ViewDifferentiatorProviding public protocol ViewDifferentiatorProviding { var viewDifferentiator: ViewDifferentiator { get } } // MARK: - ViewDifferentiator public struct ViewDifferentiator: Hashable { public init(viewType: AnyClass, styleID: AnyHashable?) { viewTypeDescription = &quot;\\(type(of: viewType.self))&quot; self.styleID = styleID } public var viewTypeDescription: String public var styleID: AnyHashable? } 从源码注释上可以看出来这两个类型就是服务于 reuseID 创建的。ItemModel 是这么实现 ViewDifferentiatorProviding 协议的： extension ItemModel { public var viewDifferentiator: ViewDifferentiator { .init(viewType: View.self, styleID: styleID) } ... } 这里的 View 是一个范型，可以理解为一个实现了 EpoxyableView 的 UIView 对象。styleID 定义在 StyleIDProviding 协议中，是一个扩展出来的存储属性，其类型是 AnyHashable，这里就省略展示它的定义了。 那么使用时它的值是哪儿来的呢？ public struct ItemModel&lt;View: UIView&gt;: ViewEpoxyModeled { ... public init&lt;Params: Hashable, Content: Equatable&gt;( dataID: AnyHashable, params: Params, content: Content, makeView: @escaping (Params) -&gt; View, setContent: @escaping (CallbackContext, Content) -&gt; Void) { self.dataID = dataID styleID = params // ⬅️ 在这里进行赋值 erasedContent = content self.makeView = { makeView(params) } self.setContent = { setContent($0, content) } isErasedContentEqual = { otherModel in guard let otherContent = otherModel.erasedContent as? Content else { return false } return otherContent == content } } ... } 上面这个是 ItemModel 的初始、基本初始化方法，可以看到在初始化时，是使用 params 这个参数初始化 styleID 属性的。但是基本上我们都很少使用这个初始化方法，而是使用下面这个： extension StyledView where Self: BehaviorsConfigurableView &amp; ContentConfigurableView { public static func itemModel( dataID: AnyHashable, content: Content, behaviors: Behaviors? = nil, style: Style) -&gt; ItemModel&lt;Self&gt; { ItemModel&lt;Self&gt;( dataID: dataID, params: style, // ⬅️ 发现 params 的来源 content: content, makeView: Self.init(style:), setContent: { context, content in context.view.setContent(content, animated: context.animated) }) .setBehaviors { context in context.view.setBehaviors(behaviors) } } } 看到这里就明白了：这个 styleID 就是创建 EpoxyableView 时定义的 Style。所以 viewDifferentiator 代表的就是遵循 EpoxyableView 协议的这个类型本身以及它所包含的 Style 对象。当同一个 View 但是 Style 不同时，viewDifferentiator 也就不同。 看完了 viewDifferentiator，让我们再回过头来看 reuseIDStore。 它的类型是一个类 ReuseIDStore，从命名上可以看出这个类是专门用来存储 reuse id 的。通过源码可以看到，这个类内部使用两个 Dictionary 来封装 reuse id 的创建、存储以及读取逻辑。 对外主要提供了两个方法：reuseID(byRegistering:) 和 registeredReuseID(for:)，参数都是 ViewDifferentiator。前者用于存储 reuse id，后者负责查询读取。 内部包含以下两个 Dictionary: private var uniqueViewDifferentiatorCountsForViewTypes = [String: Int]() private var reuseIDsForViewDifferentiators = [ViewDifferentiator: String]() uniqueViewDifferentiatorCountsForViewTypes 负责计数，即一个 CollectionView 注册了几次这个 View（Cell）。 通过追踪调用栈，可以发现每次调用 setSections(_: strategy:) 方法时，如果存在新的 ViewDifferentiator，就会调用 reuseID(byRegistering:) 进行注册。 这里我们结合注册方法 reuseID(byRegistering:) 的实现来一起看： public func reuseID(byRegistering viewDifferentiator: ViewDifferentiator) -&gt; String { if let existingReuseID = reuseIDsForViewDifferentiators[viewDifferentiator] { return existingReuseID } let viewType = viewDifferentiator.viewTypeDescription let uniqueViewDifferentiatorCount = uniqueViewDifferentiatorCountsForViewTypes[viewType] ?? 0 uniqueViewDifferentiatorCountsForViewTypes[viewType] = uniqueViewDifferentiatorCount + 1 let reuseID = &quot;\\(viewType)_\\(uniqueViewDifferentiatorCount)&quot; // ⬅️ 类型加出现次数 reuseIDsForViewDifferentiators[viewDifferentiator] = reuseID return reuseID } ViewDifferentiator 是遵循 Hashable 的，对于同一个 View 类型而言，不同的 Style 实例意味着不同的 ViewDifferentiator，每一种都会调用 reuseID(byRegistering:) 进行注册。通过累加 count，可以保证不同的 ViewDifferentiator 对应不同的 reuse id。 小结 到这里我们可以用一个例子来总结一下 reuse id 的创建过程和创建结果。 假设定义如下一个 View： final class TextRow: UIView, EpoxyableView { enum Style { case small, large } } 使用时如下： // ... other row TextRow.itemModel(dataID: &quot;small&quot;, style: .small) // ... other row TextRow.itemModel(dataID: &quot;large&quot;, style: .large) // ... other row 那么当我们调用 collectionView.setSections(_: strategy:) 时，因为两个 ItemModel 实例对应的 Style 不同，CollectionView 中就会注册两个 Cell，reuse id 分别为 TextRow_0 和 TextRow_1。 所以如果你希望抛弃或部分抛弃 Cell 的重用机制，那么你就需要保证 reuse id 各不相同，即 使用不同的 View，或者不同的 View.Style。 在实际使用时，我们常通过字面含义，将比如 TextColor 等 UI 样式封装在 Style 中。在遇到 “可用时显示黑色，不可用时展示灰色” 这种需求时，动态修改 TextColor，随后调用 setSections(_: strategy:) 刷新列表。这时列表数据发生变动，其实是 reuse id 不同，系统创建了新的 Cell 来展示不同的文字颜色。很多时候这种现象其实违背了开发者的初衷。 更新 Cell 数据 另外一种方法就是更新 Cell 上子视图的数据，比如 Label 的 text 等。在 Epoxy 中，我们常将数据定义在 Content 里，并在 setContent(_: animated:) 方法内将数据渲染到 UI 上，比如这样： extension HeaderRow: ContentConfigurableView { struct Content: Equatable { let title: String let tips: String? } func setContent(_ content: Content, animated: Bool) { titleLabel.text = content.title tipsLabel.text = content.tips } } 那么是不是修改了 Content 之后，再调用 setSections(_: strategy:) 刷新列表即可刷新 UI？让我们来看一下源码。 Content 设置流程 源码分析 首先看一下 itemModel(dataID: content: behaviors: style:) 方法的定义，它是 EpoxyableView 创建 ItemModel 的方法。 extension StyledView where Self: BehaviorsConfigurableView &amp; ContentConfigurableView { public static func itemModel( dataID: AnyHashable, content: Content, behaviors: Behaviors? = nil, style: Style) -&gt; ItemModel&lt;Self&gt; { ItemModel&lt;Self&gt;( dataID: dataID, params: style, content: content, // ⬅️ 传入 makeView: Self.init(style:), setContent: { context, content in // ⬅️ 在闭包内又吐了出来，相当于闭包持有了 Content 实例 context.view.setContent(content, animated: context.animated) }) .setBehaviors { context in context.view.setBehaviors(behaviors) } } } 从实现上看，content 实例被包裹进了 setContent 闭包，延迟到合适的时机再进行调用： extension ItemModel: InternalItemModeling { ... public func configure(cell: ItemWrapperView, with metadata: ItemCellMetadata) { // Even if there's no `setContent` closure, we need to make sure to call `viewForCell` to ensure that the cell is set up. let view = viewForCell(cell) setContent?(.init(view: view, metadata: metadata)) } ... } 这里插一句，在寻找 setContent 闭包的 Caller 时，你有可能会注意到下面这个方法： extension AnyItemModel: InternalItemModeling { ... public func configure(cell: ItemWrapperView, with metadata: ItemCellMetadata) { model.configure(cell: cell, with: metadata) if let view = cell.view { setContent?(.init(view: view, metadata: metadata)) } } ... } 这个方法属于 AnyItemModel，这个类型擦除负责包裹不同的 ItemModel 实例，方法第一行的 model 即是它所包装的 ItemModel 实例。 为什么提到这个方法呢？因为这个方法看上去调用了两次 setContent 闭包。然而实际上，不止 setContent，AnyItemModel 中的其他类似的属性（比如 setBehaviors）都是 nil，所以 setContent 闭包只调用了一次，重点还是关注 ItemModel 的 configure(cell: with:) 方法。 AnyItemModel 的 setContent 是 nil 是设计上决定的，项目中搜索不到对其进行赋值的代码。至于为什么明明没有赋值，却还有调用，猜测是为了架构上的统一？ 继续向上寻找调用方，可以发现是在上文中提到的 collectionView(_: cellForItemAt:) 方法内调用的。方法实现内有一行 self.collectionView?.configure( ... )，该方法内无判断地调用了 model 的 configure(cell: with:) 方法。 小结 总结一下逻辑和调用链： 刷新 CollectionView 时，执行 collectionView(_: cellForItemAt:)，通过层层调用，最终 model.setContent 闭包被调用，View 的 setContent(_: animated) 被调用，新的 Content 被设置到 UI 上。 到这里结束了吗？还不要高兴的太早，不要忘记了 CollectionView 还存在 Diff 的逻辑：更新 Content 实例后调用 setSections(_: strategy:)，CollectionView 一定会被刷新吗？未必。 CollectionView 刷新时机 源码分析 让我们自顶向下看，先看 setSections(_: strategy:) 的实现： open class CollectionView: UICollectionView { ... public func setSections(_ sections: [SectionModel], animated: Bool) { let strategy: UpdateStrategy if configuration.usesBatchUpdatesForAllReloads { strategy = animated ? .animatedBatchUpdates : .nonanimatedBatchUpdates } else { strategy = animated ? .animatedBatchUpdates : .reloadData } setSections(sections, strategy: strategy) } public func setSections(_ sections: [SectionModel], strategy: UpdateStrategy) { EpoxyLogger.shared.assert(Thread.isMainThread, &quot;This method must be called on the main thread.&quot;) epoxyDataSource.registerSections(sections) apply(.make(sections: sections), strategy: strategy) } ... } registerSections() 上文有提到，负责注册 Section 以及 Cell。重点关注下面的 apply(_: strategy:) 方法。查看该的实现，可以看到其内部调用了下面这个方法： open class CollectionView: UICollectionView { ... private func updateView(with data: CollectionViewData, strategy: UpdateStrategy) { updateState = .preparingUpdate let performUpdates = { self.performBatchUpdates({ self.performUpdates(data: data, animated: strategy.animated) }, completion: { _ in if let nextUpdate = self.queuedUpdate, self.window != nil { self.queuedUpdate = nil self.updateView(with: nextUpdate.newData, strategy: nextUpdate.strategy) } else { self.completeUpdates() } }) } // There's two cases in which we should always have a strategy of `.reloadData`: // - The first update, since we're going from empty content to non-empty content, and we don't want to animate that update. // - Before the first layout of this collection view when `bounds.size` is still zero, since there's no benefit to doing batch updates in that scenario. let override = (epoxyDataSource.data == nil || bounds.size == .zero) ? .reloadData : strategy switch override { case .animatedBatchUpdates: performUpdates() case .nonanimatedBatchUpdates: UIView.performWithoutAnimation { performUpdates() } case .reloadData: let result = epoxyDataSource.applyData(data) updateState = .updating(from: result.oldData) reloadData() completeUpdates() } } ... } 通过上面的实现我们可以得到第一个刷新方案：如果 strategy 是 .reloadData，那么会直接调用 UICollectionView 的 reloadData 方法（即不使用 Epoxy 内置的 Diff 算法），此时 collectionView(_: cellForItemAt:) 方法必执行，新的 Content 可以被正确地更新。 之所以保留注释，是因为这里的注释比较关键。通过注释我们可以得知：初次加载时，或者对一个尚未具有尺寸的 CollectionView 调用刷新时，将忽略外部参数，永远使用 .reloadData 方式进行刷新。 那如果使用其他的 strategy 呢？就需要关注 performUpdates 闭包里调用的 performUpdates(data: animated:) 方法了。 这里稍微提一下 UICollectionView 的 performBatchUpdates(_: completion:) 方法，它是用来将多个操作（update、insert、delete 等），合并至一个动画内进行展示。 因为 performUpdates(data: animated:) 里使用了 diff 算法，该算法会同时计算出上述多种操作，所以为了更好的 UI 交互展示，需要将这些操作合并至一个动画内展示。 performUpdates(data: animated:) 方法实现比较多，这里挑一些我们关注的贴一下： open class CollectionView: UICollectionView { ... private func performUpdates(data: CollectionViewData, animated: Bool) { let result = epoxyDataSource.applyData(data) updateState = .updating(from: result.oldData) for (fromIndexPath, toIndexPath) in result.changeset.itemChangeset.updates { if let cell = cellForItem(at: fromIndexPath) as? CollectionViewCell, let item = epoxyDataSource.data?.item(at: toIndexPath) { let metadata = ItemCellMetadata( traitCollection: traitCollection, state: cell.state, animated: animated) item.configure(cell: cell, with: metadata) // ⬅️ 熟悉的老朋友 item.configureStateChange(in: cell, with: metadata) } } ... deleteSections(result.changeset.sectionChangeset.deletes) deleteItems(at: result.changeset.itemChangeset.deletes) for (fromIndex, toIndex) in result.changeset.sectionChangeset.moves { moveSection(fromIndex, toSection: toIndex) } for (fromIndexPath, toIndexPath) in result.changeset.itemChangeset.moves { moveItem(at: fromIndexPath, to: toIndexPath) // ⬅️ moveItem 未必会触发 cell 加载 } insertSections(result.changeset.sectionChangeset.inserts) insertItems(at: result.changeset.itemChangeset.inserts) // ⬅️ insertItems 也会触发 cell 加载 } ... } 方法实现的第一行，epoxyDataSource.applyData(data) 就是应用 diff 算法。之前写的 Epoxy 源码分析系列文章中，第一篇就是有关 Epoxy 的 Diff 算法的，这里就不再赘述算法原理。 在这个方法实现中可以说有两个刷新逻辑： 熟悉的朋友：item.configure(cell: cell, with: metadata)。也就是说如果上面的条件都满足，那么 configure(cell: with:) 方法就会被调用，Content 就会被更新。 又或者归到 inserts 里，作为一个新的 Cell 插入到列表中。 IndexPathChangeset 包含以下几种情况： public struct IndexPathChangeset { ... /// The inserted `IndexPath`s needed to get from the old collection to the new collection. public var inserts: [IndexPath] /// The deleted `IndexPath`s needed to get from the old collection to the new collection. public var deletes: [IndexPath] /// The updated `IndexPath`s needed to get from the old collection to the new collection. public var updates: [(old: IndexPath, new: IndexPath)] /// The moved `IndexPath`s needed to get from the old collection to the new collection. public var moves: [(old: IndexPath, new: IndexPath)] ... } 这两个方案中，inserts 的逻辑很简单。让我们忽略 diff 算法的逻辑，直接说结论：只要 dataID 发生变化，它就是一个新的 ItemModel，就会执行 delete &amp; insert。所以第二个刷新方案就是：修改前后，ItemModel 的 dataID 不同。 那 configure(cell: with:) 的前置条件都有哪些呢？首先是 for 循环的数组不为空，其次是两个 if。聪明的你应该可以发现，这两个 if 不是逻辑重点，第一个 if 只是为了满足 Swift 语法的类型转换，而第二个 if 是为了判断 Cell 是否展示出来，避免 configure(cell: with:) 多次调用。所以重点就在于 result.changeset.itemChangeset.updates 是否为空。 所以我们关注的重点就是：如何修改我们的数据源，可以让其归到 updates 集合里。条件有以下几点： 这个 ItemModel 在刷新前已经存在于列表中，在列表中具有 Index。 ItemModel 的 isDiffableItemEqual(to:) 方法返回 false。 相关方法的实现如下： extension ItemModel: Diffable { ... public func isDiffableItemEqual(to otherDiffableItem: Diffable) -&gt; Bool { guard let other = otherDiffableItem as? Self else { return false } return isErasedContentEqual?(other) ?? true } } isErasedContentEqual 在 ItemModel 初始化时进行设置： public struct ItemModel&lt;View: UIView&gt;: ViewEpoxyModeled { ... public init&lt;Content: Equatable&gt;( dataID: AnyHashable, content: Content, setContent: @escaping (CallbackContext, Content) -&gt; Void) { self.dataID = dataID erasedContent = content // ⬅️ erasedContent 就是 content self.setContent = { setContent($0, content) } isErasedContentEqual = { otherModel in guard let otherContent = otherModel.erasedContent as? Content else { return false } return otherContent == content // ⬅️ Content 遵循 Equatable 的目的就在此 } } ... } 从上面两个方法实现中我们可以得到结论： 如果 EpoxyableView 没有定义 Content，那么也就不存在 updates 的逻辑。 更新前后，两个 ItemModel 的 Content 不相等，EpoxyableView 的 setContent(_: animated:) 方法才会被调用。 那么第三种刷新方案就是：确保修改前后的 Content 不相等。 小结 现在我们知道了，在调用 setSections(_: strategy:) 方法之后，Style 不变的前提下，以下几种情况 EpoxyableView 的 setContent(_: animated) 方法将被调用： strategy 为 .reloadData 时。 包括空 CollectionView 初次加载数据，或者 CollectionView 尚没有 Size 时。 修改数据前后，ItemModel 的 dataID 发生了改变。 修改数据前后，Content 的 == 返回 false，即两个 Content 不相等。 比如下面的例子就是有效的： private var count = 0 { didSet { setItems(items, animated: true) } } @ItemModelBuilder private var items: [ItemModeling] { TextRow.itemModel( dataID: DataID.row, content: .init( title: &quot;Count \\(count)&quot;, body: &quot;Tap to increment&quot;), style: .large) .didSelect { [weak self] _ in self?.count += 1 } } 总结 到这里让我们总结一下，借助 Epoxy 的机制我们应该如何刷新列表。 方法 描述 推荐指数 使用不同的 dataID 比如声明一个 enum DataID { case some(value: String) }，每次刷新时修改 value 的值。 或者将 dataID 和数组的下标做对应，这样在移动 Cell 时，相当于同一个 ItemModel 的 Content 发生了变化。 🌟 使用不同的 Style 将需要变化的值定义在 Style 里，刷新 Style 后刷新列表 🌟🌟 利用 Equatable，修改 Content 确保 Content 的判等逻辑符合您的要求，修改 Content 后刷新列表 🌟🌟🌟🌟 有一个刷新之外的引申话题，上文也提到过，那就是什么样的数据该放到 Style 中。希望看完这篇文章后，对于这个问题你也能有新的思考和理解。 想必对于聪明的你来说，上述分析过程有点太简单了，只要稍微深入追一下代码，就能梳理清楚这些情况。甚至 Epoxy 的 demo 和文档中已经介绍的很清楚了。 但是有的时候，生活就是这么诡异。不把这些过程写下来的话，对于使用还不熟练的我而说，经常会陷入文章开头的疑问：“为什么列表刷新后数据没变” 和 “为什么它刷新后数据就变了”。也就是说即使我们知道了 “当 Content 不一致时刷新列表，列表数据会发生变化” 并按之付出实践时，依然会有不生效的情况发生。 这篇文章还不是完全体，日后当我再次发现刷新失效的情况时，我将完善这篇文章，补充对应的解决方案。 ","link":"https://blog.rakuyoo.top/refresh-epoxy-list/"},{"title":"Lazy var 的线程安全问题","content":"Swift 的 lazy 关键字一直有一个很容易被忽略的问题，那就是它不是线程安全的。 在官方文档中有这么一段描述： Note If a property marked with the lazy modifier is accessed by multiple threads simultaneously and the property hasn’t yet been initialized, there’s no guarantee that the property will be initialized only once. 官方自己也说了，如果多线程同时访问，可能导致初始化两次。 一般情况下我们不太会遇到这种问题，都是在 viewDidLoad 中添加、布局子视图，但是少数情况还是有可能会碰到这个问题，所以需要注意。 lazy var void 与 dispatch_once Xcode 16 &amp; Swift 6 以前 除了用 lazy 修饰 UI 视图之外，另一个可能比较常见的用法是用 lazy var void 代替 dispatch_once。 在中文互联网上能搜到很多这种用法，但是读到这里你应该能发现，这种用法其实也是线程不安全的，因为它用的是线程不安全的 lazy 关键字修饰。 在《Migrating to Swift 3》，官方明确提到了代替 dispatch_once 的办法： The free function dispatch_once is no longer available in Swift. In Swift, you can use lazily initialized globals or static properties and get the same thread-safety and called-once guarantees as dispatch_once provided. Example: let myGlobal = { … global contains initialization in a call to a closure … }() _ = myGlobal // using myGlobal will invoke the initialization code only the first time it is used. 请注意，示例中使用的是 let，并且是 lazily initialized globals 或者 static properties。众所周知，static 修饰或者顶层属性的初始化都是延迟初始化，再配合上线程安全的 let 定义，这才可以达到和 dispatch_once 相同的效果。 所以综上，建议使用 static let void 来代替 lazy var void。 <!-- ### Xcode 16 & Swift 6 那么在最新的 Xcode 16 中呢？ Xcode 16 会自动添加 `@MainActor`， ->","link":"https://blog.rakuyoo.top/thread-safety-with-lazy/"},{"title":"在 Xcode playground 中预览 View 视图","content":"记录一个总是忘记的操作： import UIKit import PlaygroundSupport let image = UIImage(named: &quot;winnie.png&quot;) let imageView = UIImageView(image: image) PlaygroundPage.current.liveView = imageView 在 Xcode playground 中，如果引入 PlaygroundSupport 库，赋值 PlaygroundPage.current.liveView，可以将视图显示在右侧的预览框，也就是 Live View 中。 另外不只是 UIView，UIViewController 及其子类也是可以的，PlaygroundPage.current.liveView = controller 即可。 ","link":"https://blog.rakuyoo.top/preview-in-xcode-playground/"},{"title":"epoxy源码笔记2：在扩展中定义存储属性","content":"常规的在扩展中定义存储属性的方法是借助 Objective-C 的 runtime 进行属性关联。但是这个方法仅限于 ObjC 类，那么纯 Swift 类，比如结构体该怎么办呢？ 在 epoxy 这个库中发现了解决办法。 Objective-C runtime 先说一下用 Objective-C runtime 的实现方式，比如下面的代码： private var loadingViewKey: Void? extension HUDProtocol { public var loadingView: LoadingView { get { if let view = objc_getAssociatedObject(self, &amp;loadingViewKey) as? LoadingView { return view } loadingView = LoadingView() return loadingView } set { objc_setAssociatedObject(self, &amp;loadingViewKey, newValue, .OBJC_ASSOCIATION_RETAIN_NONATOMIC) } } } 使用 loadingViewKey 关联 loadingView 属性，同时 get 方法内还做了一次懒加载。对于 Key 的定义形式多种多样，这里就不做过度赘述。 值得一提的是，翻看 epoxy 代码中发现，在使用关联属性给 ObjC 类增加存储属性时，还是用到了 @nonobjc。因为它是一个纯 Swift 库，所以使用 @nonobjc 来避免生成 objc 接口也是很合理的优化。 Epoxy 中的做法 让我们先看一下源码，比如 DidChangeStateProviding.swift 这个文件，其中内容如下： 我会省略掉一些和本文无关的注释和内容，完整的内容请点击上方链接查看原始代码。 extension CallbackContextEpoxyModeled { public typealias DidChangeState = (CallbackContext) -&gt; Void public var didChangeState: DidChangeState? { get { self[didChangeStateProperty] } set { self[didChangeStateProperty] = newValue } } private var didChangeStateProperty: EpoxyModelProperty&lt;DidChangeState?&gt; { .init(keyPath: \\Self.didChangeState, defaultValue: nil, updateStrategy: .chain()) } } 可以看到上述代码扩展了 CallbackContextEpoxyModeled，并在其中定义了存储属性 didChangeState。 细看 didChangeState，发现用到了 subscript 语法。接着查看 CallbackContextEpoxyModeled 的声明： public protocol CallbackContextEpoxyModeled: EpoxyModeled { associatedtype CallbackContext } 发现仅仅是一个协议。接着往下看，它所遵循的 EpoxyModeled 在这里进行定义。 关键代码如下： public protocol EpoxyModeled { /// The underlying storage of this model that stores the current property values. var storage: EpoxyModelStorage { get set } } extension EpoxyModeled { /// Stores or retrieves a value of the specified property in `storage`. public subscript&lt;Property&gt;(property: EpoxyModelProperty&lt;Property&gt;) -&gt; Property { get { storage[property] } set { storage[property] = newValue } } } 从这里感觉EpoxyModeled就是一个壳，用来封装 storage 这个属性。上面提到的 subscript 语法其实也是对 storage 的 subscript 的调用。 但是这里就复杂了一些，同时出现了 EpoxyModelStorage 和 EpoxyModelProperty 两个类型。让我们先看一下 EpoxyModelStorage。 EpoxyModelStorage 该类型在这里进行定义。 依然贴出来一些关键代码： public struct EpoxyModelStorage { public init() { } /// Stores or retrieves the value of the specified property. public subscript&lt;Property&gt;(property: EpoxyModelProperty&lt;Property&gt;) -&gt; Property { get { guard let propertyStorage = storage[property.keyPath] else { return property.defaultValue() } // This cast will never fail as the storage is only settable via this subscript and the `KeyPath` key is unique for any provider and value type pair. // swiftlint:disable:next force_cast return propertyStorage.value as! Property } set { // We first update the value without using the `updateStrategy` since the likely scenario // is that there won't be a collision that requires the `updateStrategy`, and we'll be able to // return without incurring the cost of another write. let propertyStorage = PropertyStorage(value: newValue, property: property) guard var replaced = storage.updateValue(propertyStorage, forKey: property.keyPath) else { return } // This cast will never fail as the storage is only settable via this subscript and the // `KeyPath` key is unique for any provider and value type pair. // swiftlint:disable:next force_cast replaced.value = property.updateStrategy.update(replaced.value as! Property, newValue) storage[property.keyPath] = replaced } } // MARK: Private /// The underlying storage for the properties, with a key of the `EpoxyModelProperty.keyPath` and /// a value of the property's `PropertyStorage`. /// /// Does not include default values. private var storage = [AnyKeyPath: PropertyStorage]() } // MARK: - PropertyStorage /// A value stored within an `EpoxyModelStorage`. private struct PropertyStorage { /// The type-erased value of the `EpoxyModelProperty`. var value: Any /// The property's corresponding `EpoxyModelProperty`, erased to an `AnyEpoxyModelProperty`. var property: AnyEpoxyModelProperty } 可以看到在 EpoxyModelStorage 中定义了一个关键的存储属性 storage，该属性是字典类型，使用 KeyPath 做 Key，Value 是一个自定义属性。但是这里其实 PropertyStorage 不是重点，因为用到 PropertyStorage.property 的代码并不在本文的范围之内，所以我们可以将 storage 进行简化，就当作 [AnyKeyPath: Any] 类型来对待。 再回看 EpoxyModelStorage 的 subscript 实现，至此其实我们已经能明白 Epoxy 在 extension 中定义存储属性的原理：其实就是用 KeyPath 做 key，将存储属性对应的值存储到内部的一个字典属性中。 看到这里会不会有点失望？有的人还会说这是脱裤子放屁，理由如下： 在需要 extension 定义存储属性的类型里手动定义一个 [String: Any]，然后在 extension 里操作这个字典不也能达到一样的效果吗？还至于费这么大劲写这么多封装？ 而且一点也不灵活，不像是ObjC一样随写随用，无需更改原类型的定义。 还有，因为 protocol 是 public 的，那么 storage 必然会被暴露给外界，造成一定程度的隐患。 疑问暂且按下不表，再来看看 EpoxyModelProperty 这个类型。 EpoxyModelProperty 先看定义： public struct EpoxyModelProperty&lt;Value&gt; { /// Creates a property identified by a `KeyPath` to its provided `value` and with its default value if not customized in content by consumers. /// /// The `updateStrategy` is used to update the value when updating from an old value to a new value. public init&lt;Model&gt;( keyPath: KeyPath&lt;Model, Value&gt;, defaultValue: @escaping @autoclosure () -&gt; Value, updateStrategy: UpdateStrategy) { self.keyPath = keyPath self.defaultValue = defaultValue self.updateStrategy = updateStrategy } /// The `KeyPath` that uniquely identifies this property. public let keyPath: AnyKeyPath /// A closure that produces the default property value when called. public let defaultValue: () -&gt; Value /// A closure used to update an `EpoxyModelProperty` from an old value to a new value. public let updateStrategy: UpdateStrategy } // MARK: EpoxyModelProperty.UpdateStrategy extension EpoxyModelProperty { /// A closure used to update an `EpoxyModelProperty` from an old value to a new value. public struct UpdateStrategy { public init(update: @escaping (Value, Value) -&gt; Value) { self.update = update } /// A closure used to update an `EpoxyModelProperty` from an old value to a new value. public var update: (_ old: Value, _ new: Value) -&gt; Value } } // MARK: Defaults extension EpoxyModelProperty.UpdateStrategy { /// Replaces the old value with the new value when an update occurs. public static var replace: Self { .init { _, new in new } } /// Chains the new closure value onto the old closure value, returning a new closure that first calls the old closure and then subsequently calls the new closure. public static func chain() -&gt; EpoxyModelProperty&lt;(() -&gt; Void)?&gt;.UpdateStrategy { .init { old, new in guard let new = new else { return old } guard let old = old else { return new } return { old() new() } } } /// Chains the new closure value onto the old closure value, returning a new closure that first calls the old closure and then subsequently calls the new closure. public static func chain&lt;A&gt;() -&gt; EpoxyModelProperty&lt;((A) -&gt; Void)?&gt;.UpdateStrategy { .init { old, new in guard let new = new else { return old } guard let old = old else { return new } return { a in old(a) new(a) } } } // Add more arities as needed } EpoxyModelProperty 有三个属性：keyPath、defaultValue 和 updateStrategy。 keyPath 和 defaultValue 配合范型可以很好的解决 objc_getAssociatedObject 中类型转换的问题，我们可以回看上面提到过的这个代码片段： // This cast will never fail as the storage is only settable via this subscript and the `KeyPath` key is unique for any provider and value type pair. // swiftlint:disable:next force_cast return propertyStorage.value as! Property 这也就解答了上面提到的第一个问题：如果仅仅定义一个 [String: Any]，那么 String 类型的 Key 和 Any 类型的 Value 明显无法保证对齐。 那么你可能想说改成 [KeyPath: Any] 呢？答案也是不行的。因为在调用 subscript 时，如果直接使用 KeyPath，则会触发系统默认的通过 KeyPath 取值的方法，不会走自定义的 subscript，所以 Map 的 Key 无论如何都需要自定义类型来包一层，既然要包一层，何不直接连 defaultValue 也封装一下呢？ 至于 updateStrategy 定义了属性赋值时的方法，是替换原值？还是两个值都执行？虽然它和本文内容无关，但是这个实用的属性确实也是 EpoxyModelProperty 的必要性之一。 解决疑问 针对上述三个问题： 为何不手动定义 [String: Any]？epoxy 属于过度封装 缺乏灵活性，而且需要更改原类型的定义。对于无法更改定义的类型，则无法使用这个方法。 因为 protocol 是 public 的，导致 storage 会被暴露给外界，造成一定程度的隐患。 针对第一个问题，用 protocol 规范属性定义在 Swift 中绝对不算是过度封装，而是很常见的方法；因为无法直接使用 KeyPath 类型作为 Map 的Key，为了保持类型安全，在上层调用中也无法直接使用 AnyKeyPath，所以必然需要自定义一个类型来做调用链中的 “索引”。 而且其他两个问题确实是实际存在的。特别是第二个问题，除非使用 objc_getAssociatedObject 来实现相关协议，否则则无法使用这套方法。 至于第三个问题，考虑到外部对该类型的扩展性，暴露 storage 属性给外界是必然的，否则外部如何给这个类型增加新的存储属性呢？除非你明确不允许外部给这个类型增加存储属性，那么你可以使用一个 internal type 包一层 Map。 简化代码 现在让我们学以致用，考虑一下如何简化代码。 关键类型是三个协议：EpoxyModeled、EpoxyModelStorage 和 EpoxyModelProperty，简化肯定也是针对这三个类型进行简化。 但是其实可以删减的空间并不多，仅仅在你不需要 updateStrategy 时，可以将实现简化为下面的这样： public struct ModelProperty&lt;Value&gt; { public init&lt;Model&gt;( keyPath: KeyPath&lt;Model, Value&gt;, defaultValue: @escaping @autoclosure () -&gt; Value ) { self.keyPath = keyPath self.defaultValue = defaultValue } public let keyPath: AnyKeyPath public let defaultValue: () -&gt; Value } public struct ModelStorage { private var storage = [AnyKeyPath: Any]() public init() { } public subscript&lt;Property&gt;(property: ModelProperty&lt;Property&gt;) -&gt; Property { get { guard let propertyStorage = storage[property.keyPath] else { return property.defaultValue() } return propertyStorage as! Property } set { storage[property.keyPath] = newValue } } } public protocol Modeled { var storage: ModelStorage { get set } } extension Modeled { public subscript&lt;Property&gt;(property: ModelProperty&lt;Property&gt;) -&gt; Property { get { storage[property] } set { storage[property] = newValue } } } 其实上述代码也就相当于 updateStrategy = .replace 的情况。不过，查看这份简化的代码是不是感觉整个逻辑都更清晰了呢😏 在调用上和原本实现也没有区别： struct Action: Modeled { var storage = ModelStorage() } extension Action { var testString: String { get { self[testStringProperty] } set { self[testStringProperty] = newValue } } private var testStringProperty: ModelProperty&lt;String&gt; { .init(keyPath: \\Self.testString, defaultValue: &quot;default&quot;) } } var testAction = Action() print(testAction.testString) // &quot;default&quot; testAction.testString = &quot;123&quot; print(testAction.testString) // &quot;123&quot; 对了，Modeled 中的 subscript 其实属于简化代码的操作，如果去掉这部分代码，那么在 testString 的 get 和 set 中，使用 self.storage[testStringProperty] 也是可以的，不影响主要逻辑。 ","link":"https://blog.rakuyoo.top/epoxy-source-code-notes-2/"},{"title":"Github Action：在步骤之间共享数据","content":"Github Action 各个 Setp 之间是互相独立的，所以假如我们在 A Step 中定义了一个环境变量，在接下来的 Step 中是无法使用的。 那么怎么共享呢？在最新的 Github Action 中，可以通过 Environment files 来实现。 Environment files 它的做法是通过一个环境变量配置文件，在各个Step之间共享自定义环境变量。 官方的例子如下所示： steps: - name: Set the value id: step_one run: | echo &quot;action_state=yellow&quot; &gt;&gt; &quot;$GITHUB_ENV&quot; - name: Use the value id: step_two run: | printf '%s\\n' &quot;$action_state&quot; # This will output 'yellow' echo &quot;action_state=yellow&quot; &gt;&gt; &quot;$GITHUB_ENV&quot; 这段就是关键代码了。 被抛弃的 set-env 如果你在 Google 上搜索过这个问题的话，你可能会搜到这篇文章：GitHub Actions 第15天：在步骤之间共享数据。 这篇文章中提到的 set-env 方法，在后续的 Github Action 中被禁止使用了，详见官方博文：GitHub Actions: Deprecating set-env and add-path commands 在本文撰写时，如果在 Github Action 中没有经过任何配置地去使用 set-env 的话，会直接报错，Step 执行失败。 “Environment files” 的做法也并不麻烦，反而觉得比 set-env 简单很多，所以就使用上面的方法吧。 ","link":"https://blog.rakuyoo.top/share-env-between-steps-in-github-action/"},{"title":"UIImage.size And CGImage.size","content":"记录一个最近发现的小细节：UIImage.size 和 CGImage.size 在一些情况下是不相等的。 如果你的图片是添加到 Assets.xcassets 并设置了 1x、2x 等不同规格，那么 UIImage.size 的大小将始终等于 1x 时候的尺寸，不论当前机型实际使用了哪种规格的图片。 而 CGImage.size 返回的则是 UIImage.size * ImageScale 之后的结果。也就是说如果该图片是 2x 图，那么 CGImage.size 就会是 UIImage.size 的二倍。 这一问题在使用 CGImage 画图会暴露出来，很多时候会使用 UIImage.size 或者 UIImageView.size 设置画布，然而因为忽略了 CGImage.size，就会导致画出来的图片尺寸偏大，出现显示的图片被剪裁的问题。 ","link":"https://blog.rakuyoo.top/uiimage-size-and-cgimage-size/"},{"title":"加速 git submodule","content":"iOS 中有一个第三方数据库封装：GRDB。该组件使用了 git submodule 来依赖 SQLiteLib。 这个时候如果我们只使用之前介绍过的 设置 SPM Mirror，那么这个子模块就会是一个漏网之鱼。 那么怎么为这个子模块设置镜像呢？我们可以使用 git config --global url.[New].insteadOf [Old] 命令来替换 url 中的某个部分。可以参考：Easier Git Repository Cloning with insteadOf --global 参数是必须的，spm 的加载读取的是 ~/.gitconfig 文件，不会读取项目根目录下的 .git/config 文件。所以这个替换必须设置为全局替换的。 New 是镜像 url，而 Old 是 github url，设置后我们可以通过 git config --global -l 或者 git config --global -e 命令来查看是否设置成功。 设置成功后不论是执行 git clone 或者 git submodule update --init，都会通过镜像 url 去执行了。 如果你想在团队项目中共享这份配置，那么可以将地址存储到一个文件中，比如像下面这样： # .gitconfig-insteadof [url &quot;https://github.com/swiftlyfalling/SQLiteLib.git&quot;] insteadOf = https://your-organize-git/SQLiteLib.git 然后通过 git config --global include.path .gitconfig-insteadof 将这个配置文件添加到 ~/.gitconfig 文件中即可。 添加后 ~/.gitconfig 内容会如下所示： ... [include] path = /Users/.../project/.gitconfig-insteadof ","link":"https://blog.rakuyoo.top/speed-up-git-submodule/"},{"title":"提取ipa文件","content":"有的时候我们想要看看某个 App 的资源文件，或者单纯看看它引用了哪些三方库等。这些内容只要有一个 ipa 文件就可以看，也无需砸壳。 本文就记录一下如何简单的提取手机中 App 的 ipa 文件。 首先确保手机上已经安装了该 app。 随后在 mac 上下载 Apple 的官方软件 Apple Configurator 安装后打开，手机连接Mac后会在软件上显示当前设备，如下所示： 选中设备，随后点击右上角的 “添加” 按钮，选择 App： 在弹出的列表中选择要提取 ipa 的 App，随后点击 “添加” 按钮： 这一步有可能会提示你要登陆 Apple ID。出现这种情况要么是你的 Mac 没有登录，要么是你的手机和 Mac 登录的不是同一个 ID。按照提示登录即可。 因为我们的手机上已经安装了该 app，所以 Apple Configurator 肯定是装不上的，会弹出下面的弹窗： 此时不要操作 Apple Configurator！打开 Finder 访达 App，使用快捷键 Command + Shift + G 打开跳转弹窗，在弹窗中输入下面的路径： ~/Library/Group Containers/K36BKF7T3D.group.com.apple.configurator/Library/Caches/Assets/TemporaryItems/MobileApps/ 进到路径下，一层层点到最里面，就可以找到我们需要的 ipa 包了： Apple Configurator 弹窗关闭后，该 ipa 就会自动删除。所以请在关闭软件之前将 ipa 拷贝出来。 ","link":"https://blog.rakuyoo.top/extract-ipa-file/"},{"title":"Tuist 注意事项","content":"最近在学习使用 Tuist 生成项目，摆脱烦人的 .xcodeproj。但是 Tuist 好用虽然好用，但是因为最近文档正在迁移，加之一些东西只能从示例中发掘，整个学习过程有点费劲。所以开一篇文章记录一下。 UIKit 模版 在有的地方你可以看到 “通过 tuist init --template swiftui 创建 SwiftUI 项目”。也就是说默认是创建 UIKit 项目。 但是！UIKit 的模板已经在这个 PR 中被删除精简掉，并跟随 4.0.0 版本一起发布了。所以如果你使用的是 4.0+ 版本的 Tuist，那么执行 init 操作后默认生成的就是 SwiftUI 项目了。 官方认为大多数人已经迁移到SwiftUI了... 所以如果你想创建 UIKit 项目，那么你有两种（或者更多）选择： 手动把 UIKit 模板从该 PR 中找回来。 每创建一个项目都手动修改一下 Info.plist 以及添加 AppDelegate 等文件。 因为我是第一次使用，所以是手动添加的相关文件。 如果采用这种方法，那么有可能会遇到这个问题：解决从SwiftUI迁回UIKit时，SceneDelegate不执行的问题。之前已经为这个问题单独发过一片文章了，这里再重复提一下。 资源生成 Tuist 集成了 SwiftGen 来实现资源生成，所以 SwiftGen 的模板可以直接拿到 Tuist 里使用。 Tuist 的默认模块在这里：Templates。 以文件资源的生成为例，这个默认模板 和 SwiftGen 的 structured-swift5.stencil 模板几乎一样，只是多了一些 SwiftFormat 和 SwiftLint 的内容。 不包含目录层级 SwiftGen 对于一种资源有多个默认模版，比如文件资源还有 flat-swift5.stencil 这个不包含文件夹层级的模版。单独使用 SwiftGen 的话我们可以通过 --templateName 参数来使用该模板，但是 Tuist 里每种类型的资源只有一种模板。 所以如果我们想生成的资源不包含文件夹目录层级，只能参考Tuist官方文档的内容自定义模板： If you want to provide your own templates to synthesize accessors to other resource types, which must be supported by SwiftGen, you can create them at Tuist/ResourceSynthesizers/{name}.stencil, where the name is the camel-case version of the resource. Resource Template name strings Strings.stencil assets Assets.stencil plists Plists.stencil fonts Fonts.stencil coreData CoreData.stencil interfaceBuilder InterfaceBuilder.stencil json JSON.stencil yaml YAML.stencil files Files.stencil 注意，官方的这个方法其实是替换了默认实现，相关代码应该是这里。 如果你不想覆盖默认实现，想要自定义一个模板，可以参考app_with_plugins这个示例。 自定义解析类型 文档最后的部分有提到，可以通过 Project.resourceSynthesizers 属性来设置本项目自动为哪些类型的资源生成相应的代码。 这个属性是有默认值的，其默认值定义在这里： extension [ResourceSynthesizer] { public static var `default`: Self { [ .strings(), .assets(), .plists(), .fonts(), ] } } 可见使用的是默认模板，而且不包含 .files。如果有需要的话则需要手动添加。 禁用资源生成 虽然你大概率不需要，不过还是提一嘴。文档中也有说明，那就是可以通过设置 Project.Options.disableSynthesizedResourceAccessors 属性，来禁用资源的自动生成。 组件化 Tuist 应该是实施组件化的好手。 目录结构 首先让我们来讨论一下目录结构，这关系到我们如何组织主项目和各个组件。 在官方文档里我们能看到如下目录结构： Tuist/ Config.swift Package.swift ProjectDescriptionHelpers/ Projects/ App/ Project.swift Feature/ Project.swift Workspace.swift Tuist 文件夹是 Tuist 的一些配置，Workspace.swift 用来包含各个 Projects。 但是如果你仔细翻看过他们的示例的话，比如 ios_app_with_static_frameworks 和 ios_app_with_framework_and_resources，就会发现其实根目录下的内容是多变的。 如果你跟我一样好奇 “什么是最佳实践”，可以看这个讨论。然后结论就是：没有什么最佳实践，完全取决于你的个人用法和习惯。 Tuist 真的非常灵活，提供了很多便捷的写法，不同人用 Tuist 会写出不同的配置文件，也会有不同的用法。 Tuist vs SPM 对目录结构有了一些概念之后，就该考虑如何组织各个组件了。 官方文档里有一篇文章：Migrate local Swift Packages 讲的是如何将本地的 SPM 组件迁移到 Tuist，改为使用 Tuist 管理。 国外应该是已经没有什么人用 CocoaPods 做组件化管理了？... 这里我也推荐使用 Tuist + Multiple Projects 来做子组件。首先 SPM 要比 CocoaPods 更现代更官方，但是 SPM 本身有性能问题，再加上国内访问问题，所以真要实际用起来很麻烦。另外使用 Tuist 来管理项目，可以使用自带的 SwiftGen，不用再在 SPM 里进行相关配置。最后不得不提的是，用 Tuist 加载依赖是真的快... 单独运行组件 我个人对组件化的标准是每个子组件应该都能独立编译，进一步能独立运行。基于这个标准的话，会有一些问题。 善用软链接 项目根目录下的 Tuist/ProjectDescriptionHelpers 是不能被子项目读取的，比如前文提到的目录结构中： Tuist/ Config.swift Package.swift ProjectDescriptionHelpers/ Projects/ App/ Project.swift Feature/ Project.swift Tuist/ Config.swift Package.swift Workspace.swift 在 Projects/Feature 中执行 tuist generate 命令时，是读取不到根目录中 Tuist/ProjectDescriptionHelpers 里的公共方法的。 一个可行的方法是使用软链接，将 ProjectDescriptionHelpers 文件夹软链到 Projects/Feature/Tuist 里。 同理 Tuist/.swiftpm/configuration/mirrors.json 文件也可以这么做。 是否需要多个 Config.swift 经过进一步实践，以及相关 讨论，发现其实不建议 有多个 Config.swift 文件。所以 Feature/Tuist/Config.swift 文件应该是没有必要的。 我现在使用的目录结构如下所示： Tuist/ Config.swift Package.swift ProjectDescriptionHelpers/ Projects/ App/ Project.swift Components/ FeatureA/ Project.swift FeatureB/ Project.swift Workspace.swift 也就是说各个组件内完全没有 Tuist 文件夹（依赖部分在 Tuist/ProjectDescriptionHelpers 进行了封装） 这时假如我们在 Projects/Components/FeatureA 目录下执行 tuist generate，其实读取的是根目录中 Tuist 的配置，生成的是 App 项目而不是组件项目。 这个时候肯定是不能满足 “组件独立运行” 的，因为现在这个组件算是一个 lib，没有前端 AppDelegate 等相应入口去运行 App。 那么为什么还要改成这种方式呢？一是基于官方推荐的不要使用多个 Config.swift 的建议；二是和群友讨论后，发现一般测试一个 lib 时会单独建一个测试项目，比如 FeatureATest，该测试项目不在 FeatureA 下；三是理想很丰满现实很骨干，话是这么说，但是实际上几乎不会单独运行某个 lib。 所以基于以上三个理由，如果未来需要单独测试某个 lib，那么最佳实践应该是新建一个相关的 Test 项目。 ","link":"https://blog.rakuyoo.top/tuist-notes/"},{"title":"解决从SwiftUI迁回UIKit时，SceneDelegate不执行的问题","content":"最近在研究 Tuist，一个用来管理 Xcode 项目的工具。这个工具好像从 4.0 版本开始，创建新项目默认就是 SwiftUI 模版了，所以需要手动再从 SwiftUI 改回 UIKit。 在改动过程中发现 SceneDelegate 不执行，搜索了一番后找到了解决方案，在此记录一下。 具体的改动方法和流程可以参考：How to migrate from SwiftUI to UIKit App Delegate Life Cycle in Xcode 解决方案出自这里：App migrated to UIKit lifecycle doesn't call SceneDelegate 插一嘴，真没想到还会有其他人也遇到了这个问题... 如上述问题里所讲的那样，这个 bug 的根本问题可能是： do not turn off support multiple windows. It seems that disabling multiple windows prevents UIKit from launching a new scene when the previous scene is no longer available. 也就是说 SwiftUI 的 @main 入口没了，同时因为不支持多窗口，所以导致在当前场景不可用时，无法创建 SceneDelegate 的新场景。 解决方案除了把设备上的 app 删除重新安装之外，还可以把 Info.plist 中的 scene configurations manifest 删除，然后手动创建它： class AppDelegate: NSObject, UIApplicationDelegate { func application(_ application: UIApplication, configurationForConnecting connectingSceneSession: UISceneSession, options: UIScene.ConnectionOptions) -&gt; UISceneConfiguration { let configuration = UISceneConfiguration(name: nil, sessionRole: connectingSceneSession.role) if connectingSceneSession.role == .windowApplication { configuration.delegateClass = SceneDelegate.self } return configuration } } 注意，如果不删那么这个方法也是不会调用的，所以需要删除然后手动创建。 最后吐槽一下，真的没想到 iOS 就连这个也会有类似 “缓存” 的问题... 想到了 LaunchScreen ... ","link":"https://blog.rakuyoo.top/jie-jue-cong-swiftui-qian-hui-uikit-shi-scenedelegate-bu-zhi-xing-de-wen-ti/"},{"title":"CSS选择器","content":"CSS 有多种选择器，可以帮助我们针对不同的 HTML 标签设置样式。 选择器 通配符选择器 通配符选择器（*）可以选择任何元素，通常用于设置默认样式。 * { margin: 0; padding: 0; } 类型选择器 类型选择器（Type Selectors）直接使用 HTML 标签来选择选择元素： p { color: blue; } 上面的代码将所有的 &lt;p&gt; 标签文本渲染为蓝色。 class 选择器 个人理解 class 选择器代表着 “一类有着相同样式的标签”，所以该选择器可以用于多个 HTML 标签。 类选择器使用 . 开头，后接类名： &lt;style&gt; .first { font-weight: bold; text-decoration: line-through; } &lt;/style&gt; &lt;p class=&quot;first&quot;&gt;超越技术&lt;/p&gt; &lt;div&gt;哈哈.&lt;/div&gt; &lt;p&gt;一起学前端!&lt;/p&gt; 一个标签也可以包含多个 class，多个 class 之间使用空格分隔： &lt;!-- 包含两个 class：info 和 highlight --&gt; &lt;p class=&quot;info highlight&quot;&gt;...&lt;/p&gt; 此外，当你在不同的 HTML 标签中使用了相同的 class，然后还想选择某个具体的标签时，你可以像下面这样组合使用两个选择器： &lt;style&gt; p.first { color: red; } &lt;/style&gt; &lt;p class=&quot;first&quot;&gt;该颜色会是红色&lt;/p&gt; &lt;span class=&quot;first&quot;&gt;该颜色不会是红色&lt;/span&gt; id 选择器 个人理解这里的 id 在使用场景上就和数据库中的主键一样，可以用来定位某个唯一的 HTML 标签，所以该选择器或者说 id 属性的值，在 HTML 文档中需要是唯一的，不能出现在多个元素上。 id 选择器使用 # 号来定义： &lt;style&gt; #my-id { background-color: yellow; } &lt;/style&gt; &lt;p id=&quot;my-id&quot;&gt;超越技术&lt;/p&gt; &lt;div&gt;哈哈.&lt;/div&gt; &lt;p&gt;一起学前端!&lt;/p&gt; 因为 id 选择器的唯一性，所以就没必要像上面提到的 class 选择器一样，通过标签再进一步缩小选择范围了。 属性选择器 我们都知道可以在 html 标签上自定义任意的属性，比如下面的例子： &lt;!-- `data-rakuyo` 不是很好，因为它不明所以； 但是它又很好，可以一眼让你看出它是一个自定义属性。 --&gt; &lt;div data-rakuyo&gt;...&lt;/div&gt; 属性选择器使用 [] 中括号包裹属性名称。针对上面的代码，我们可以通过属性选择器来选择该 div 标签： /* 选择所有带有 `data-rakuyo` 属性的元素，不论其所属哪个 html 标签 */ [data-rakuyo] { font-weight: bold; } 再进一步缩小范围，属性选择器可以通过指定标签来筛选 “具有某些属性的标签”： /* 仅限带有 `data-style` 属性的 &lt;button&gt; 标签 */ button[data-style] { font-weight: bold; } 可以通过具体的属性值来做条件筛选： &lt;style&gt; [data-style=&quot;cancel&quot;] { color: gray; } [data-style=&quot;done&quot;] { color: black; } &lt;/style&gt; &lt;button data-style=&quot;cancel&quot;&gt;取消&lt;/button&gt; &lt;button data-style=&quot;done&quot;&gt;下单&lt;/button&gt; 属性选择器还可以借助一些匹配操作符，比如 $= 和 *= 来实现特定的功能： /* 选择 `src` 属性以 `https` 开头的所有图片 */ img[src^=&quot;https&quot;] { color: red; } /* 选择 `src` 属性以 `.jpg` 结尾的所有图片 */ img[src$=&quot;.jpg&quot;] { border: 3px solid black; } /* 选择 `href` 属性包含 `external` *字符串* 的所有链接 */ a[href*=&quot;external&quot;] { color: green; } /* 选择 `href` 属性包含 `external` *单词* 的所有链接 */ a[href~=&quot;external&quot;] { color: green; } 注意 *= 和 ~= 的区别。前者匹配字符串，而后者匹配的是一个完整的单词。 伪选择器 伪选择器包含 “伪类选择器” 和 “伪元素选择器” 两种。 伪类选择器 伪类选择器用于定义元素在特定状态下的样式，比如鼠标悬停时的样式。或者用于选择第n个子元素。 /* 设置表格中某一列的文字不换行 */ .nowrap-column { white-space: nowrap; } 对于特定状态，有以下几种： 名称 说明 示例 :hover 用于在用户将鼠标悬停在元素上时应用样式 a:hover { color: red; } :active 用于选取被用户激活（例如鼠标点击）的元素 button:active { background-color: gray; } :visited 用于选取用户已访问过的链接 a:visited { color: purple; } :focus 用于选取当前获取了焦点的元素通常在用户通过键盘或者鼠标进行交互时出现 input:focus { border-color: blue; } 对于选择子元素，有以下几种用法： 名称 说明 示例 :nth-of-type(n) 该选择器允许你选择相同类型的且特定位置的子元素请注意，n从1开始，且不能是负数 /* 这会使每隔两个段落文字变为红色 */ p:nth-of-type(2n) { color: red; } :nth-child(n) 该选择器允许你选择特定位置的子元素 /* 列表中的奇数项背景色变为浅灰色 */ li:nth-child(odd) { background-color: lightgray; } :nth-last-child(n) 该选择器与:nth-child(n)类似，但是它从元素的末尾开始计数。 /* * 这会使每个无序列表（ul）的 * 倒数第二个列表项（li）的文字颜色变为蓝色 */ ul li:nth-last-child(2) { color: blue; } :first-child“nth-child(1)” 该选择器选取某个元素的第一个子元素 /* * 这会使每个无序列表（ul）的第一个列表项（li）的文字加粗 */ ul li:first-child { font-weight: bold; } :last-child“nth-last-child(1)” 该选择器选取某个元素的最后一个子元素 /* * 这会使每个无序列表（ul） * 的最后一个列表项（li）的文字变为斜体 */ ul li:last-child { font-style: italic; } 我觉得 :nth-of-type 和 :nth-child 的概念比较接近，同时容易混淆，所以在这里特意将他们两个单独拿出来，用一个例子来看看这两个选择器之间的区别。 &lt;div class=&quot;container&quot;&gt; &lt;p&gt;Paragraph 1&lt;/p&gt; &lt;div&gt;Div 1&lt;/div&gt; &lt;p&gt;Paragraph 2&lt;/p&gt; &lt;p&gt;Paragraph 3&lt;/p&gt; &lt;div&gt;Div 2&lt;/div&gt; &lt;/div&gt; 针对上面这段 html，两个选择器的结果为： 选择器 结果 说明 .container p:nth-of-type(2) 选择到 &lt;p&gt;Paragraph 2&lt;/p&gt; 这个选择器将选择容器 .container 内的第二个 &lt;p&gt; 元素。在给定的 HTML 结构中，第二个 &lt;p&gt; 元素是 &quot;Paragraph 2&quot;，因此该选择器会影响到这个段落元素。 .container p:nth-child(2) 不会选择任何元素 这个选择器将选择容器 .container 内所有的 &lt;p&gt; 元素中的第二个子元素。在给定的 HTML 结构中，第二个子元素是 &quot;Div 1&quot;，而不是 &lt;p&gt; 元素。因此，这个选择器不会选择任何元素，因为在该结构中，第二个子元素不是 &lt;p&gt; 元素。 简而言之，nth-child(n) 会先按照顺序选择父视图的子元素，再看该子元素是否符合限制条件，如果符合则样式生效，否则不会生效；而 nth-of-type(n) 则相反，会先选择符合要求的元素，再按照顺序进行选择。 同理可得，因为 :first-child、:last-child 和 :nth-child(1)、:nth-last-child(1) 相同，所以假如使用 p:fist-child，但是第一个标签不是 &lt;p&gt;，那么该样式也不会生效。 伪元素选择器 伪元素选择器用于选择元素的特定部分而不是元素本身。伪元素选择器以双冒号 :: 开头。 下面是一些常见的伪元素选择器： /* 选择元素的第一行文本 */ p::first-line { font-weight: bold; } /* 选择元素的第一个字母 */ p::first-letter { font-size: 150%; } /* 在元素内容之前插入内容 */ p::before { content: &quot;前置内容 &quot;; } /* 在元素内容之后插入内容 */ p::after { content: &quot; 后置内容&quot;; } /* 选择用户选中的文本部分 */ ::selection { background-color: yellow; color: black; } /* 选择输入框的占位符文本 */ input::placeholder { color: gray; } 除此之外还有一些不太常见的伪元素选择器： /* 选择列表项的标记部分（通常是列表项前面的符号，如圆点或数字） */ li::marker { color: red; } /* 选择元素的背景层，用于处理全屏元素的背景样式 */ dialog::backdrop { background-color: rgba(0, 0, 0, 0.5); } /* 选择拼写错误的文本部分 */ ::spelling-error { text-decoration: underline wavy red; } /* 选择语法错误的文本部分 */ ::grammar-error { text-decoration: underline dashed blue; } 组合选择器 组合选择器是 CSS 中的一种选择器，允许你针对同时满足多个条件的元素应用样式。 后代选择器 后代选择器（Descendant Selector）允许你选择某个元素内部的所有后代元素。 &lt;!DOCTYPE html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Descendant Selector Example&lt;/title&gt; &lt;style&gt; /* 选择 .container 内部的所有 p 元素 */ .container p { color: blue; } &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;div class=&quot;container&quot;&gt; &lt;p&gt;This is a paragraph.&lt;/p&gt; &lt;div&gt; &lt;p&gt;This is another paragraph.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt; 在这个例子中，.container p 选择器会选择所有嵌套在 .container 内部的 &lt;p&gt; 元素，并将它们的颜色设为蓝色。即两个 &lt;p&gt; 标签都会变成蓝色。 子选择器 子选择器（Child Selector）用于选择某个元素的直接子元素。 &lt;!DOCTYPE html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Child Selector Example&lt;/title&gt; &lt;style&gt; /* 选择 .container 下的直接子元素 p */ .container &gt; p { font-weight: bold; } &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;div class=&quot;container&quot;&gt; &lt;p&gt;This is a paragraph.&lt;/p&gt; &lt;div&gt; &lt;p&gt;This is another paragraph.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt; 这个例子中，.container &gt; p 选择器只会选择 .container 直接子元素中的 &lt;p&gt; 元素，并将它们的字体加粗。而里层的 This is another paragraph. 并不会被加粗。 对比 后代选择器 来看，后代选择器会一直查询到该叶子节点的最末端，渲染所有满足条件的标签，不管中间是否还有其他标签；而 子选择器 只会渲染下一个层级（直接）的子节点，并不会进一步深入。 相邻兄弟选择器 相邻兄弟选择器（Adjacent Sibling Selector）选择紧接在指定元素后的兄弟元素。 &lt;!DOCTYPE html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Adjacent Sibling Selector Example&lt;/title&gt; &lt;style&gt; /* 选择 .container 后面紧邻的 p 兄弟元素 */ .container + p { color: green; } &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;div class=&quot;container&quot;&gt; &lt;p&gt;This is a paragraph.&lt;/p&gt; &lt;/div&gt; &lt;p&gt;This is another paragraph.&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; 在这个例子中，.container + p 选择器会选择紧接在 .container 元素后面的 &lt;p&gt; 元素，并将它们的颜色设为绿色。即 This is another paragraph. 会被渲染为绿色。 有以下几点需要注意： “相邻兄弟选择器” 强调 相邻。如果我们将代码进行修改： &lt;div class=&quot;container&quot;&gt; &lt;p&gt;This is a paragraph.&lt;/p&gt; &lt;/div&gt; &lt;span&gt;123&lt;/span&gt; &lt;p&gt;This is another paragraph.&lt;/p&gt; 此时 .container 不再有任何相邻的 &lt;p&gt; 节点，所以 .container + p 不会选择任何的标签进行渲染。 “相邻兄弟选择器” 只会渲染相邻的 一个 节点。如果我们将代码进行修改： &lt;div class=&quot;container&quot;&gt; &lt;p&gt;This is a paragraph.&lt;/p&gt; &lt;/div&gt; &lt;p&gt;This is another paragraph. 1&lt;/p&gt; &lt;p&gt;This is another paragraph. 2&lt;/p&gt; 那么只有相邻的第一个 &lt;p&gt; 标签，即 This is another paragraph. 1 会被渲染为绿色，另外一个 &lt;p&gt; 标签仍为黑色。 通用兄弟选择器 通用兄弟选择器（General Sibling Selector）选择与指定元素相邻的所有兄弟元素。 &lt;!DOCTYPE html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;General Sibling Selector Example&lt;/title&gt; &lt;style&gt; /* 选择 .container 后的所有 p 兄弟元素 */ .container ~ p { font-style: italic; } &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;div class=&quot;container&quot;&gt; &lt;p&gt;This is a paragraph.&lt;/p&gt; &lt;/div&gt; &lt;p&gt;This is another paragraph.&lt;/p&gt; &lt;p&gt;This is yet another paragraph.&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; 在这个例子中，.container ~ p 选择器会选择紧跟在 .container 元素后的所有 &lt;p&gt; 元素，并将它们的字体样式设为斜体。很容易看出来，该选择器和 相邻兄弟选择器 互为一对。 组合选择器总结 后代选择器（ ） 和 子选择器（&gt;） 互为一对：同样是选择子元素，前者会深入到层级的最末端，而后者只会选择下一个层级，点到为止。 相邻兄弟选择器（+） 和 通用兄弟选择器（~） 互为一对：同样是选择相邻元素，前者只选择同层级内相邻的节点，而后者会选择同层级内所有符合条件的节点。 课后思考 现在我们提出来这么一个需求：从接口处获取商品的价格（忽略接口请求步骤），得到一个价格字符串，比如 &quot;¥35.66&quot;。针对该字符串进行渲染，要求如下： 文字颜色均为红色。 ¥ 和 .66 部分样式一致：15号字。但是需要注意 ¥ 符号并不一定存在。 35 部分加粗，18号字。 使用 js+html+css，留几秒钟思考给出答案。 ... ... ... ... ... 参考答案如下： &lt;!DOCTYPE html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;style&gt; /* 样式1：¥符号 15 号字，红色 */ .currency-sign, .after-dot { font-size: 15px; } /* 样式2：¥ 和 . 之间的内容，18号字，加粗，红色 */ .between { font-size: 18px; font-weight: bold; } .currency-sign, .after-dot, .between { color: red; } &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;span class=&quot;currency-value&quot;&gt;&lt;/span&gt; &lt;!-- 使用 JavaScript 定义变量 --&gt; &lt;script&gt; // 定义变量，代替接口请求的步骤 const currencyValue = &quot;¥35.66&quot;; // 获取 span 标签 const spanElement = document.querySelector('.currency-value'); // 检查 span 标签是否存在 if (spanElement) { // 检查 currencyValue 是否包含 &quot;¥&quot; 符号 if (currencyValue.includes('¥')) { // 渲染 ¥ 符号 const currencySignSpan = document.createElement('span'); currencySignSpan.innerText = '¥'; currencySignSpan.classList.add('currency-sign'); spanElement.appendChild(currencySignSpan); } // 删去 &quot;¥&quot; 符号后的值 let valueWithoutSign = currencyValue.replace('¥', ''); // 按 &quot;.&quot; 分隔值 const [beforeDot, afterDot] = valueWithoutSign.split('.'); // 渲染 &quot;.&quot; 之前的内容 const beforeDotSpan = document.createElement('span'); beforeDotSpan.innerText = beforeDot; beforeDotSpan.classList.add('between'); spanElement.appendChild(beforeDotSpan); // 渲染 &quot;.&quot; 符号 和 &quot;.&quot; 之后的内容 const afterDotSpan = document.createElement('span'); afterDotSpan.innerText = '.' + afterDot; afterDotSpan.classList.add('after-dot'); spanElement.appendChild(afterDotSpan); } &lt;/script&gt; &lt;/body&gt; &lt;/html&gt; 然后我们可以再进一步，如果改成 vue3 + TypeScript 呢？ &lt;template&gt; &lt;span class=&quot;currency-value&quot;&gt; &lt;span v-if=&quot;hasCurrencySign&quot; class=&quot;currency-sign&quot;&gt;¥&lt;/span&gt; &lt;span class=&quot;between&quot;&gt;{{ beforeDot }}&lt;/span&gt; &lt;span class=&quot;after-dot&quot;&gt;.{{ finalAfterDot }}&lt;/span&gt; &lt;/span&gt; &lt;/template&gt; &lt;script setup lang=&quot;ts&quot;&gt; import { ref } from 'vue'; const currencyValue = ref&lt;string&gt;(&quot;¥35.66&quot;); const hasCurrencySign = currencyValue.value.includes('¥'); const [beforeDot, afterDot] = currencyValue.value.replace('¥', '').split('.'); const finalAfterDot = afterDot || '00'; &lt;/script&gt; &lt;style scoped&gt; /* 样式1：¥符号 15 号字，红色 */ .currency-sign, .after-dot { font-size: 15px; } /* 样式2：¥ 和 . 之间的内容，18号字，加粗，红色 */ .between { font-size: 18px; font-weight: bold; } .currency-sign, .after-dot, .between { color: red; } &lt;/style&gt; 可见因为 vue 中的模版可以直接引用变量，为我们减少了大量的 DOM 操作，代码量得以减少了不少。 ","link":"https://blog.rakuyoo.top/css-selector/"},{"title":"HTML 元素显示模式","content":"记录前端的学习过程。 本篇文章是最基础的 HTML 部分。 元素显示模式 HTML 元素一般分为 “块元素” 和 “行内元素”。 块元素 &lt;div&gt; 是最典型的块元素。其余的还有 &lt;h1&gt; ~ &lt;h6&gt;、&lt;p&gt;、&lt;ul&gt;、&lt;ol&gt;、&lt;li&gt; 等。 特性： 独占一行 高度、宽度、外边距和内边距都可以控制 宽度默认是容器（父级宽度）的 100% 是一个容器及盒子，里面可以放行内或者块级元素 注意： 文字类元素内部不能使用块级元素 &lt;p&gt; 标签主要用于存放文字，因此 &lt;p&gt; 里面不能放块级元素，特别是不能放 &lt;div&gt; 同理，&lt;h1&gt; ~ &lt;h6&gt; 等都是文字类块级标签，里面也不能放其他块级元素 行内元素 是最典型的行内元素。其余的还有 &lt;a&gt;、&lt;strong&gt;、&lt;b&gt;、&lt;em&gt;、&lt;i&gt;、&lt;del&gt;、&lt;s&gt;、&lt;ins&gt;、&lt;u&gt;等。 有的地方也将行内元素成为内联元素。 特性： 相邻行内元素在一行上，一行可以显示多个 宽、高直接设置是无效的 默认宽度就是它本身内容的宽度 行内元素只能容纳文本或其他行内元素。 注意： 链接里不能再放链接 特殊情况链接 &lt;a&gt; 里面可以放块级元素，但是给 &lt;a&gt; 转换成块级模式最安全 示例 下面这段代码可以作为一个简单的示例。 &lt;body&gt; &lt;h1&gt;《前端小课》&lt;/h1&gt; &lt;h2&gt; 一本帮你入门与进阶的前端书 &lt;!-- h2 和 p 都是块级元素，内部可以直接包含其他元素，比如 &lt;hr&gt; --&gt; &lt;!-- html 标签之间，默认好像是会有一些间距的。所以这个 &lt;hr&gt; 如果写在 &lt;p&gt; 外层，会有不一样的 ui 效果 --&gt; &lt;hr&gt; &lt;/h2&gt; &lt;!-- 用 div 这个块级元素包一下，可以保证代码占整行 不用 div 包裹，那么这个元素的大小就是 code 的大小 --&gt; &lt;div class=&quot;code-bg&quot;&gt; &lt;code&gt; const p = document.querySelector('p'); p.onclick = function() { alert('噢，噢，噢，别点我了。'); } &lt;/code&gt; &lt;/div&gt; &lt;/body&gt; ","link":"https://blog.rakuyoo.top/html-element-display-mode/"},{"title":"Jellyfin + tinymediamanager + TMM 刮削","content":"使用 tinymediamanager 配合 TMM（themoviedb）实现 Jellyfin 刮削。 网上现有的教程都比较老了，所以开一篇帖子记录一下自己配置的过程。 TMM API Key 因为是使用 TMM 来实现刮削，所以我们先上官网 申请一下 API Key。 首先你需要注册一个账号，注册账号比较简单，本篇不作介绍。 在初次登录，未配置过 API 的情况下，有两种方法让我们进入到 API 申请页面： 直接拉到页面最底部，点击 API 。 点击自己的头像，进入账户设置页面，然后点击列表左侧的 API 选项。 注意 申请过 API 之后就不能使用第一种方法了，会进入文档页面。只能使用第二种方法查看已经申请的 API。 在 API 选项页面，我们选择 Developer 类型，创建 API Key。 进入到信息填写页面，这里我借用一张别人的图做示例： 经过实际测试，有以下几点需要注意： 应用信息部分直接按照上图所示填写即可，一模一样就行。 姓名必须要使用英文。 地址信息找一个随机地址生成器生成一下即可。注意 地址1 和 地址2 都要有，可以是中文，两个一样也可以。 点击提交后即可完成申请，申请是秒过的，在新页面可以立刻查看到我们申请的 key： 有的文章附图里，标题显示的是 “API密钥（v3 auth）”。在 TMM 最新版里直接显示 “API 密钥” 了，其实是一回事。 NAS 配置 回到群晖 NAS 上，首先需要通过 docker 安装 tinymediamanager docker-compose 省流版，我做了一份 docker-compose 文件，如果您有编程经验可以直接参考使用： version: '3.8' services: tinymediamanager: container_name: tinymediamanager-tinymediamanager image: tinymediamanager/tinymediamanager:latest command: /app/tinyMediaManager -Dtmm.contentfolder=/data extra_hosts: - &quot;www.themoviedb.org:8.67.111.128&quot; - &quot;www.themoviedb.org:18.154.144.22&quot; - &quot;www.themoviedb.org:99.86.199.23&quot; - &quot;image.tmdb.org:84.17.46.53&quot; - &quot;image.tmdb.org:89.187.162.242&quot; - &quot;image.tmdb.org:138.199.46.66&quot; - &quot;api.themoviedb.org:13.33.33.60&quot; - &quot;api.themoviedb.org:13.224.167.10&quot; - &quot;api.themoviedb.org:99.84.192.81&quot; - &quot;api.thetvdb.org:192.241.234.54&quot; ports: - 4000:4000 volumes: - /volume1/video:/media:rw - /volume1/docker/tiny-media-manager/config:/config:rw - /volume1/docker/tiny-media-manager/data:/data:rw environment: PATH: &quot;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot; LANG: &quot;en_US.UTF-8&quot; LC_ALL: &quot;en_US.UTF-8&quot; USER_ID: &quot;0&quot; GROUP_ID: &quot;0&quot; APP: &quot;tinyMediaManager&quot; UMASK: &quot;0022&quot; ALLOW_DIRECT_VNC: &quot;true&quot; LC_TIME: &quot;C.UTF-8&quot; ENABLE_CJK_FONT: &quot;1&quot; restart: always privileged: true network_mode: bridge 该文件设置了以下内容： 容器名：tinymediamanager-tinymediamanager 配置了 hosts，解决 TMDB 访问问题 启用 “自动重新启动” 使用高级权限执行容器 端口号 4000 挂载以下路径，均为读写权限 本地 /volume1/video，挂载到容器内 /media 本地 /volume1/docker/tiny-media-manager/config，挂载到容器内 /config 本地 /volume1/docker/tiny-media-manager/data，挂载到容器内 /data 手动安装 如果不想使用 docker-compose 安装，可以手动安装 安装 tinymediamanager 在注册表中搜索 tinymediamanager，这里我们选择第二个官方镜像。 大部分教程不是选择第一个 star 最多的，就是选择第三个所谓 “中文更友好” 的。其实在 2024 年，官方的用起来才是最简单的。 下载完成之后，我们先不要打开，进入后续的配置流程。 创建 tinymediamanager 容器 首先通过 ssh 登录群晖，然后通过下面的命令运行 tinymediamanager docker。 这一步可以直接添加 host，解决 tinymediamanager 无法访问的问题。避免我们在后续进入 docker 容器内部修改 host。 但是主要注意一点的是，如果在 DSM 里手动关闭容器再重新启动，那么 host 配置就失效了。因为 TMM 容器内没有 vi，同时考虑以后再次修改，个人还是建议将 /etc/hosts 文件挂载出来，然后在文件内添加下列 host。 docker run \\ --name=tinyMediaManager \\ --add-host=www.themoviedb.org:8.67.111.128 \\ --add-host=www.themoviedb.org:18.154.144.22 \\ --add-host=www.themoviedb.org:99.86.199.23 \\ --add-host=image.tmdb.org:84.17.46.53 \\ --add-host=image.tmdb.org:89.187.162.242 \\ --add-host=image.tmdb.org:138.199.46.66 \\ --add-host=api.themoviedb.org:13.33.33.60 \\ --add-host=api.themoviedb.org:13.224.167.10 \\ --add-host=api.themoviedb.org:99.84.192.81 \\ --add-host=api.thetvdb.org:192.241.234.54 \\ tinymediamanager/tinymediamanager:latest 运行到下面这一步就 ok 了： 此时容器已经创建好，我们可以 ctrl+c 停止运行，然后回到 NAS 上停止该容器，准许后续的编辑。 配置 tinymediamanager 容器 在 docker 文件夹下新建一个 TinyMediaManager 目录，然后新建 data 文件夹。 在容器内找到刚才新创建好的容器，点击 详情-&gt;设置 进入配置。 端口号使用默认或根据你的实际情况填写。 文件夹挂载需要挂载两个，一个是刚才创建的 data 文件夹，另一个是你当前 存储影片 的文件夹。 环境变量需要注意以下几点： USER_ID 和 GROUP_ID 修改为 0（root用户）。 PASSWORD 可以直接删掉。 增加 ENABLE_CJK_FONT 这个 key，值为 1。 对于 ENABLE_CJK_FONT，老版的教程里都需要添加，为了切换为中文后正常显示。我在装最新版的时候直接加上了，没有测试不加可以不可以。 填完后我们就可以运行容器了。 官方镜像较其他镜像的优势 截止到本文发布，官方镜像已经来到了 5.x 的版本，而其他两个镜像还是 3.x 的版本。 相比较老版，新版的挂载目录从 /config 变为了 /data，默认端口从 5800 变为了 4000。 在对中文的支持上，最新的官方镜像对中文的支持已经很好了，不需要额外的配置。而其他两个镜像版本对中文的支持比较麻烦： 教程中往往提到我们需要将 /etc/cont-init.d/10-cjk-font.sh 文件中的 http://dl-cdn.alpinelinux.org 替换为 http://mirrors.tuna.tsinghua.edu.cn/。 此举是为了下载中文字体，然而如果我们使用的是 romancin 的镜像，那么就算替换了也没用，设置为中文依然会乱码。这是因为该字体在清华源下已经失效了（阿里云好像也失效了）。有网友提到华为云是有效的，经过测试确实可以。 最后，在刮削效率和结果上，也是新版更为出色。 tinymediamanager 配置 打开浏览器，输入 http://nas-ip:4000/ 进入 tinymediamanager。初始是一个英文的引导页面，我们一般直接 next 即可。 进入主页面后，我们进入设置，修改语言为中文。 ","link":"https://blog.rakuyoo.top/jellyfin-scraper-with-tmm/"},{"title":"群晖NAS公网访问配置（四）：配置云服务器","content":"假如我们没有公网 IP，或者公网 IP 出现故障暂时无法访问，那么我们可以借助云服务器+WireGuard的形式实现内网穿透。 本文为《群晖NAS公网访问配置》系列文章的第四篇。全部文章请参考： 群晖NAS公网访问配置（一）：配置DDNS 群晖NAS公网访问配置（二）：配置WireGurad 群晖NAS公网访问配置（三）：配置Nginx访问Docker服务 群晖NAS公网访问配置（四）：配置云服务器 我在腾讯云上有一台 Ubuntu 轻量级服务器，本文将以其为基础进行各种配置。 云服务器配置 增加记录 首先我们需要增加一条新的记录值，作为创建 WireGuard 隧道的入口。这里可以参考之前的流程：添加域名记录。 注意这里的记录值需要填写你服务器的公网 IP。 开放端口 接着去到服务器管理后台，需要在防火墙中开放 NAS 中 WireGuard 的端口。在前文示例中，该值为 51820。 在云服务器上配置 WireGuard 安装 WireGuard 服务 首先我们 ssh 到远端云服务器，然后安装 WireGuard 服务： apt update &amp;&amp; apt install wireguard -y # 先更新再安装 该过程可能会超时，可以试着修改 sources.list 源解决。 创建配置 然后我们创建并进入 wireguard 文件夹。 mkdir /etc/wireguard cd /etc/wireguard 接着按照 配置 WireGuard 服务端 的方式，创建公私钥。 注意这里我们需要创建两套客户端配置。因为现在我们是要将该云服务器作为 WireGuard 的服务端，NAS 和我们的操作终端（比如 Mac）都是客户端。 接着我们创建 wg0.conf 配置文件。 客户端配置 NAS 配置 通过 SSH 连接 NAS，进入 /etc/wireguard 文件夹，创建一个新的 wg1.conf 配置文件。 保存文件后，通过 wg-quick up wg1 命令启动配置。或者参考前文设置开机自启动。 客户端配置 客户端配置就简单很多了，和前文中的操作是一模一样的。 注意端口、IP 即可。 ","link":"https://blog.rakuyoo.top/synology-with-cloud/"},{"title":"群晖NAS公网访问配置（三）：配置Nginx访问Docker服务","content":"当我们配置玩 DDNS 和 WireGuard 之后，我们已经可以安全的访问我们 NAS 上的文件以及各种服务了。 但是如果你觉得通过端口访问 Docker 中各个服务太繁琐，或者 Docker 服务太多，要记的端口太多。那么我们可以通过配置 Nginx 的方式，来给各个 Docker 服务绑定一个域名。 本文为《群晖NAS公网访问配置》系列文章的第三篇。全部文章请参考： 群晖NAS公网访问配置（一）：配置DDNS 群晖NAS公网访问配置（二）：配置WireGurad 群晖NAS公网访问配置（三）：配置Nginx访问Docker服务 群晖NAS公网访问配置（四）：配置云服务器 在云平台添加记录值 前面几篇文章中，我们在腾讯云上设置过多条主机记录，这里我们需要再添加一条作为我们 Docker 服务的入口域名。 同样是在腾讯云 我的域名 页面，点击已注册域名，进入“记录管理”页面。 单击添加记录，根据你服务的需求添加一条主机记录，比如我们为 calibre-web 添加域名，那么主机记录可以写 calibre。 记录值和你 NAS 的内网 IP 相同。比如我们在上一篇文章中设置的 192.168.47.29。 配置 NAS 的 Nginx DSM 本身自带 Nginx，所以我们可以通过修改自带 Nginx 的方式来为 Docker 服务增加配置。 DSM 的 Nginx 每次启动时会根据模板文件重新创建，不过这里我们不修改文件本身，所以无碍。 通过 ps aux|grep nginx 命令我们可以看到 DSM 加载的是 /etc/nginx/nginx.conf.run，在该文件末尾有一行 include sites-enabled/*。这代表着我们可以将新增的配置放到 /etc/nginx/sites-enabled 文件夹下。 进入 sites-enabled 文件夹，然后通过 vim 新建一个配置文件： cd /etc/nginx/sites-enabled vim calibre.baidu.com.conf # 可使用你要配置的服务的域名作为文件名 在文件内填入下面的内容： server { listen 80; server_name calibre.baidu.com; # 服务的域名，根据实际情况替换 #charset koi8-r; #access_log /var/log/nginx/host.access.log main; location / { proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; proxy_redirect off; proxy_pass http://172.17.0.1:8083; # 填写群晖容器详情中设置的网关/IP地址+配置的端口 # 如果您要使用本地存储策略，请将下一行注释符删除，并更改大小为理论最大文件尺寸 client_max_body_size 2000m; } } 保存后，使用下面的命令告诉 Nginx 使用指定的配置文件启动，然后重载 Nginx 的配置文件。 /usr/bin/nginx -c /etc/nginx/nginx.conf.run -t /usr/bin/nginx -c /etc/nginx/nginx.conf.run -s reload 现在您应该已经可以使用 calibre.baidu.com 代替 sana.baidu.com:8083 来访问您的 calibre-web 服务了。 ","link":"https://blog.rakuyoo.top/synology-docker-public-access/"},{"title":"群晖NAS公网访问配置（二）：配置WireGurad","content":"因为配置了公网 ip，为了安全，使用 WireGuard 控制访问。 本文参考自：群晖DS220+安装 WireGuard 保姆级教程 本文为《群晖NAS公网访问配置》系列文章的第二篇。全部文章请参考： 群晖NAS公网访问配置（一）：配置DDNS 群晖NAS公网访问配置（二）：配置WireGurad 群晖NAS公网访问配置（三）：配置Nginx访问Docker服务 群晖NAS公网访问配置（四）：配置云服务器 安装 WireGuard 套件 开启 SSH 因为 WireGuard 安装后需要通过 SSH 执行相关命令，所以我们需要先开启群晖的 SSH 功能。 在群晖 “终端机和SNMP -&gt; 终端机” 中勾选 启动SSH功能，端口号使用默认即可。然后点击应用即可完成开启。 随后您可以尝试在终端中通过内网或公网ip/域名访问群晖。 如果您还没有公网ip，那么您需要在内网环境下完成后续操作，因为群晖的 QC 是不支持 SSH 访问的。 安装 WireGuard 套件 首先在 “套件中心 -&gt; 设置 -&gt; 套件来源” 中添加 spk7.imnks.com/ 矿神源。随后搜索 WireGuard，找到对应套件后点击安装。 安装完成后我们通过 SSH 访问群晖NAS。 我这里为了方便，首先执行了 sudo su 命令，避免后续权限问题。 SSH 登录群晖 NAS 后，我们执行下面的命令： # 赋予Wireguard套件权限，在安装群晖套件时，我们在简介处也看到了该命令。 sudo sed -i 's/package/root/g' /var/packages/WireGuard/conf/privilege 至此 WireGuard 套件安装完成。 配置 WireGuard 服务端 依次执行下面的命令进入 WireGuard 配置文件夹： # 创建相关目录，该目录可能已经存在，无视相关报错即可。 mkdir /etc/wireguard/ # 进入Wireguard文件夹，后续操作都将在该文件夹中进行。 cd /etc/wireguard/ 创建公私钥 在该文件夹中，我们需要生成服务端公私钥以及客户端公私钥： #生成服务器端私钥 wg genkey &gt; server_privateKey #生成服务器端私钥对应的公钥 wg pubkey &lt; server_privateKey &gt; server_publicKey # 生成客户端私钥及对应的公钥 wg genkey | tee client_privateKey | wg pubkey &gt; client_publicKey # 使用 cat 命令查看对应内容。 cat server_privateKey cat server_publicKey cat client_privateKey cat client_publicKey 复制好这些秘钥，后面配置文件时需要用到。 创建服务端配置文件 按照注释，将下面配置中的内容替换为您在上一步中生成的内容。 [Interface] PrivateKey = serverprivatekey # 替换为服务器私钥 Address = 10.8.0.1/24 PostUp = iptables -A FORWARD -i %i -j ACCEPT; iptables -A FORWARD -o %i -j ACCEPT; iptables -t nat -A POSTROUTING -o eth+ -j MASQUERADE PostDown = iptables -D FORWARD -i %i -j ACCEPT; iptables -D FORWARD -o %i -j ACCEPT; iptables -t nat -D POSTROUTING -o eth+ -j MASQUERADE ListenPort = 51820 # 端口默认 51820，也可以自定义 MTU = 1420 [Peer] PublicKey = clientpublickey # 替换为客户端公钥 AllowedIPs = 10.8.0.2/32 说明： 上述配置文件中的 Address 和 AllowedIPs，前三位需要在同一局域网内对齐。最后一位详见后文 添加新的客户端 一节。 编辑后内容后回到终端，使用 vim 或其他手段在 /etc/wireguard/ 路径下创建 wg0.conf 文件，其内容为上述配置。 启动服务 先回到群晖套件中心，启动 WireGuard 套件。 之前启动了也无所谓，顺序不重要 再回到SSH终端，使用下面的命令加载配置文件： # 使用 `up` 命令加载 wg0 配置 sudo wg-quick up wg0 # 使用 `down` 命令停止 wg0，当前不执行 # sudo wg-quick down wg0 随后我们使用 wg 命令即可查看当前状态： 说明： 有客户端连接的时候 peer 才会显示 keepalive，此时我们还没有配置客户端连接，能够展示出来对应的配置即可。 最后我们为 wg0 服务添加开机自启动： sudo wg-autostart enable wg0 # 开启自启动 # sudo wg-autostart disable wg0 # 关闭自启动 备注：群晖的 WireGuard 套件无法通过 systemctl 来配置开启自启动，不知道是 DSM 的问题还是套件的问题，这里没有进一步深究。 路由器设置监听端口号转发 为了更好的理解后面的操作，我们先大致讲一下借助 WireGuard 要做的事情： 我们之前在 群晖NAS配置DDNS 一文中配置了 DDNS，在后面的操作中，我们将借助上面的域名来构建 WireGuard 隧道。等到隧道建立好了之后，我们再借助端口转发，通过隧道走内网服务。 腾讯云上的配置 因为之前我们创建的记录被我们用来建立隧道，所以我们需要新建一条记录来作为 NAS 真正的入口。 所以这里我们新建一条记录，主机记录随你定，注意记录值需要填写为 NAS 在内网中的 IP 地址。 后文我们使用以下域名作为示例： 用于创建 WireGuard 隧道的域名：gwa.baidu.com，记录值为公网 ip（39.156.66.10） 作为 NAS 入口的域名：sana.baidu.com，记录值为 NAS 内网 ip（192.168.47.29） 路由器静态 IP 分配 因为路由器重启会导致 ip 变化，而我们需要 NAS 内网的 IP 值。好在现在大部分路由器都支持 DHCP 静态 IP 分配。 登录路由器后台，根据你的路由器的设置，将群晖对应设备的 IP 添加到 DHCP 静态 IP 列表，使其拥有固定的 IP 值。 通过 NAS 设置端口转发 我这里选择的方案是通过群晖设置端口转发，而不是通过路由器。如果你希望通过路由器进行转发，则可以在路由器后台上进行设置。 先留在路由器管理后台，我们需要开启路由器的 UPnP 功能，这样我们才能使用 NAS 来设置端口转发。 开启路由器的 UPnP 之后，我们回到 NAS，来到 “控制面板 -&gt; 外部访问 -&gt; 路由器配置” 页面，如下图所示： 点击新增按钮，选择 “自定义端口”，点击下一步。 在接下来的步骤中，通讯协议我们选择 UDP，端口设置为 创建服务端配置文件 一节中，ListenPort 的值。 最后点击完成。完成后我们可以回到路由器的 UPnP 页面，刷新看看是否有对应的记录。 配置 WireGuard 客户端 下载客户端 WireGuard 客户端下载参考：WireGuard客户端。Apple 平台均需要国外的 Apple ID。 创建客户端配置文件 安装客户端的间隙，我们先准备好客户端配置文件： [Interface] PrivateKey = clientprivatekey # 替换为客户端私钥 Address = 10.8.0.2/32 # 这里需要对应上文 [创建服务端配置文件] 一节中，peer.AllowedIPs 里对应的值 MTU = 1420 [Peer] PublicKey = serverpublickey # 替换为服务端公钥 Endpoint = gwa.baidu.com:51820 # 用于创建 WireGuard 隧道的域名，加上上文设置的 ListenPort 端口 AllowedIPs = 10.8.0.1/24, 192.168.47.29/24 # 分别是 [创建服务端配置文件] 一节中,Interface.Address，以及 NAS 内网 IP PersistentKeepalive = 25 将上述内容保存为 wgnas.conf（文件名随意）。之后我们打开 WireGuard 客户端，选择通过文件创建隧道，选择 wgnas.conf，即可完成配置设置。 至此，您的域名配置应该符合下面的情况： WireGuard 开启状态 gwa.baidu.com sana.baidu.com 开启 无法访问 能访问 NAS 关闭 无法访问 无法访问 即只有在开着 WireGuard 的前提下，可以通过 sana.baidu.com 访问您的 NAS 服务。 添加新的客户端 上面的步骤里我们只添加了一个客户端，因为同一时间 WireGuard 只能识别一个隧道，如果您有多台设备，或者多人公用同一台 NAS，则需要设置多个 WireGuard 客户端配置，不能多台设备同时使用同一个配置文件。 让我们打开 创建服务端配置文件 中的创建的配置文件。 其中 Peer 的部分代表着每个客户端。当我们需要新增客户端时，服务端需要添加一套新的 Peer 配置。 [Interface] PrivateKey = serverprivatekey Address = 10.8.0.1/24 ... ListenPort = 51820 MTU = 1420 # 上述内容保持不变 [Peer] PublicKey = clientpublickey AllowedIPs = 10.8.0.2/32 [Peer] # 新的 Peer 配置 PublicKey = clientpublickey01 # 一套新的客户端公钥 AllowedIPs = 10.8.0.3/32 # 注意这里需要修改 ip 地址最后一位，一般+1即可 随后按照 创建客户端配置文件 中的方法，在新的客户端中加载新的配置文件即可。 ","link":"https://blog.rakuyoo.top/synology-with-wireguard/"},{"title":"群晖NAS公网访问配置（一）：配置DDNS","content":"本文使用DSM 7.0。请您确保已经获取了公网 ip，并且已经购买了域名。 因为我的域名在腾讯云上，所以为了方便 DDNS 使用腾讯云的服务，步骤参考：群晖（Synology）NAS 启用腾讯云 DDNS 并安装免费证书 本文为《群晖NAS公网访问配置》系列文章的第一篇。全部文章请参考： 群晖NAS公网访问配置（一）：配置DDNS 群晖NAS公网访问配置（二）：配置WireGurad 群晖NAS公网访问配置（三）：配置Nginx访问Docker服务 群晖NAS公网访问配置（四）：配置云服务器 获取腾讯云 API 密钥 登录 腾讯云 API 密钥 ，新建腾讯云 API SecretId 及 SecretKey 密钥信息。如下图所示： 复制好您的 Id 以及 Key，点击确定后 key 将无法查看。 添加域名记录 在 我的域名 页面，点击已注册域名，进入“记录管理”页面。 单击添加记录，添加一条主机记录为任意，记录值为任意 IP 的 A 记录。如下图所示： 说明： 假如您的域名为 baidu.com，主机记录为 abc，那么对应的域名就是 abc.baidu.com。所以本步骤中的主机记录请根据您的实际情况填写。 记录值可以填写为任意 IP 地址（比如0.0.0.0），最后 DDNS 绑定完成后，该值会自动更新为正确的公网 IP 地址，所以现在填的内容无所谓。 群晖配置 DDNS 首先按照下图所示，进入群晖添加 DDNS 的面板 外部地址应当就是您的公网 ip 地址。 其余各字段为： 服务供应商：腾讯云。 主机名称：请填写上一步中您添加的域名名称。比如 abc.baidu.com。 用户名/电子邮件：请填写您获取到的 SecretId 信息。 密码/密钥：请填写您获取到的 SecretKey 信息。 从 Tencent Cloud 获取证书，并将其设置为默认证书：勾选选项后，可自动为您申请腾讯云 TrustAsia SSL 免费证书并替换 NAS 的默认 SSL 证书。 备注： 如果您不需要 https，那么这个证书不勾选也可以。 此时可以单击测试联机，测试是否能联机成功。如状态栏显示为正常，则代表联机成功。 最后单击确定，即可完成设置。等待解析生效后（解析生效时间一般需要 10分钟），即可使用域名（abc.baidu.com:5000，需要携带端口）访问NAS。 手动更新 DDNS（可选） 完成设置后，单击群晖面板上的立即更新，系统会更新最新的 DDNS 解析记录，并确认状态是否显示为正常。如下图所示： 返回 我的域名 页面，单击域名，即可查看记录值是否已变更为公网 IP 地址。 ","link":"https://blog.rakuyoo.top/synology-config-ddns/"},{"title":"Package.swift 降级问题排查","content":"过去 Package.swift 没有好好写，toolchain 版本跟着 Xcode 走，没有兼容不同的版本。最近研究用 SPM 代替 CocoaPods，所以认真的研究折腾了一下。 本文总结把 Package.swift 从 5.9 降至 5.1 过程中遇到的一些问题。 本文不讲解基础语法。 版本号前的空格 之前 5.9 的时候是这么写的： // swift-tools-version: 5.9 直接将 5.9 改成 5.1 之后报错了： horizontal whitespace sequence [U+0020] immediately preceding the version specifier is supported by only Swift ≥ 5.4; consider removing the sequence for Swift 5.1 原因是 version: 和 5.1 之间有一个空格。而这个空格是 Swift 5.4 时才支持的特性... 所以改成下面这样就可以了： // swift-tools-version:5.1 // ⬆️注意这里没有空格 参考自：Swift Package Tools Version 的写法 资源的引用 像我一样没有持续关注 SPM 发展的开发者可能不知道，在 SPM 中引用资源是 5.3 开始才支持的特性。 所以如果你的包使用了 Resource bundle 或者 Binary Framework，那么你就不能降至 5.1 了，最低也要 5.3 版本才行。 省略 target.path 我没能找到这个的提案，可能是 Allow sources anywhere in ./Sources when only one target is present 这个。如果哪位明确知道麻烦在评论区告知。 在 5.9 版本上，如果我们的代码都在 Sources 目录下，且只有一个 Target，那么我们可以不显式指定 target.path 属性。 但是在 5.1 版本上不行，所以我们还是需要显式地指定 path 路径。 ","link":"https://blog.rakuyoo.top/troubleshooting-package_swift-downgrade-issues/"},{"title":"解决重置SPM缓存失败的问题","content":"相信经常使用 SPM 的小伙伴，应该都遇到过使用 Reset Package Caches 时报错：An unknown error occurred. reference 'refs/remotes/origin/main' not found (-1)，或者其他分支。 关于这个问题在 stackoverflow 上有比较多的讨论，例如这个 SPM unknown error reference not found when changing branch。 思来想去决定根据上面的回答，记录一下解决方案。 在我的场景下（Xcode 15），仅仅删除 ~/Library/Caches/org.swift.swiftpm/repositories 下对应的目录即可解决问题，之后重新 Rest 即可。 问题的发生 链接中提到了两种观点。一开始 @mrwest09 提到，如果使用 ssh（git@）引入依赖，则有可能会产生该问题。实际上这也符合我的使用场景（不过我没有试 https）。 但是后来 @Ivan Vavilov 也提到，他使用的 https，但是也依然出现了该问题。 所以怎么说好呢，这个问题的发生未必可以都归结到 ssh，至于真正的原因仅从上述回答中可能暂时无从得知... 问题的解决 直接方式 删除 ~/Library/Caches/org.swift.swiftpm/repositories 下的内容即可。 除此之外，回答中还提到了一个 “Swift package caches both in the derived data directory of your project”。该缓存的实际目录在 ~/Library/Developer/Xcode/DerivedData/$project/SourcePackages/repositories。 我的场景下仅删除 org.swift.swiftpm 中的内容即可。如果你仅删除它无效，可以看再试试 一并 SourcePackages 下的内容。 自动化处理 回答中还有好心的大佬写了2个自动删除缓存的脚本，一个是： #!/bin/bash if [[ $# -eq 0 ]] ; then echo 'Please call the script with the name of your project as it appears in the derived data directory. Case-insensitive.' echo 'For example: ./fix-spm-cache.sh myproject' exit 0 fi # Delete all directories named &quot;remotes&quot; from the global Swift Package Manager cache. cd ~/Library/Caches/org.swift.swiftpm/repositories for i in $(find . -name &quot;remotes&quot; -type d); do echo &quot;Deleting $i&quot; rm -rf $i done # Find derived data directories for all projects matching the script argument, and # delete all directories named &quot;remotes&quot; from source package repositories cache for those projects. cd ~/Library/Developer/Xcode/DerivedData/ for project in $(find . -iname &quot;$1*&quot; -type d -maxdepth 1); do for i in $(find &quot;$project/SourcePackages/repositories&quot; -name &quot;remotes&quot; -type d); do echo &quot;Deleting $i&quot; rm -rf $i done done 这个脚本删除了上面提到的两个路径下的缓存，使用时需要 sh ./fix-spm-cache.sh myproject。 另外一个大佬使用了另外一个思路，编写了一个 python 脚本： # Sometimes Xcode cannot resolve SPM(File -&gt; Packages -&gt; Resolve Package versions) if the dependency url is ssh # This script is a workaround to resolve package versions. # Usage: # python spmResolve.py # or # python3 spmResolve.py import os.path import subprocess import glob import json def main(): package_file = &quot;xcshareddata/swiftpm/Package.resolved&quot; xcodeproj = glob.glob('*.xcodeproj') xcworkspace = glob.glob('*.xcworkspace') spmproj = glob.glob('Package.resolved') package_resolved = &quot;&quot; if xcodeproj: package_resolved = xcodeproj[0] + f&quot;/project.xcworkspace/{package_file}&quot; elif xcworkspace: package_resolved = xcworkspace[0] + f&quot;/{package_file}&quot; elif spmproj: package_resolved = spmproj[0] else: print(f&quot;😱 Cannot find *.xcodeproj, *.xcworkspace or Package.resolved file&quot;) exit(-1) update_package_resolved(package_resolved) def update_package_resolved(package_resolved): if not os.path.exists(package_resolved): print(f&quot;😱 Package.resolved file doesn't exit: {package_resolved}&quot;) exit(-1) print(f&quot;Found: {package_resolved}&quot;) f = open(package_resolved) content = json.load(f) f.close() for pin in content[&quot;pins&quot;]: url = pin[&quot;location&quot;] if &quot;branch&quot; in pin[&quot;state&quot;]: branch = pin[&quot;state&quot;][&quot;branch&quot;] commit_hash = get_git_revision_hash(url, branch) print(f&quot;{url}, {branch}, {commit_hash}&quot;) pin[&quot;state&quot;][&quot;revision&quot;] = commit_hash elif &quot;version&quot; in pin[&quot;state&quot;]: version = pin[&quot;state&quot;][&quot;version&quot;] commit_hash = get_git_revision_by_tag(url, version) print(f&quot;{url}, {version}, {commit_hash}&quot;) pin[&quot;state&quot;][&quot;revision&quot;] = commit_hash with open(package_resolved, &quot;w&quot;) as output: json.dump(content, output, indent=4) # resolve SPM subprocess.run(['xcodebuild', '-resolvePackageDependencies']) print('🎉 Well done') def get_git_revision_hash(url, branch) -&gt; str: command = f'git ls-remote {url} refs/heads/{branch} | cut -f 1' return get_git_command_output(command) def get_git_revision_by_tag(url, version) -&gt; str: command = f'git ls-remote {url} -t {version} | cut -f 1' return get_git_command_output(command) def get_git_command_output(command) -&gt; str: return subprocess.check_output(command, stderr=subprocess.STDOUT, shell=True).decode('ascii').rstrip() if __name__ == '__main__': main() 使用方式为在包含 *.xcodeproj 或 *.xcworkspace 文件的路径下，调用 python spmResolve.py。 ","link":"https://blog.rakuyoo.top/fix-spm-cache/"},{"title":"移除字符串中的�","content":"一个非常非常非常常见的需求，一个限制输入长度的输入框，同时不限制用户输入字符的类型。 这里我们先假定一个需求： 一个最多输入6个字符的输入框，不限制用户输入的字符类型，即可以输入空格、中文英文、标点符号、数字以及 Emoji。 在这个需求下考虑下面这个场景： 输入框上已经有了文字：&quot; g,5就&quot;，用户即将输入一个 🉑️。 这时，你的程序很可能就会出问题了。 字符长度 我们大家应该都知道 Swift.String 的 count 属性和 ObjC 的 length 属性获取的结果不同。 比如下面的代码，实际输出结果如注释所述。 let string = &quot; g,5就🉑️&quot; print(string.count) // 6 print((string as NSString).length) // 8 回到需求和场景上，我相信你早就知道，在做这种截取的时候，不能直接用 string.count 判断长度。 这种时候我们一般会用 utf16 编码来获取长度： let string = &quot; g,5就🉑️&quot; print(string.count) // 6 print(string.utf16.count) // 8 print((string as NSString).length) // 8 直接截取 搞定了长度，剩下的就是截取了： extension String { func prefixed(_ maxLength: Int) -&gt; String { let sequence = utf16 guard sequence.count &gt; maxLength else { return self } let startIndex = sequence.startIndex let endIndex = sequence.index(startIndex, offsetBy: maxLength) let result = String(sequence[startIndex ..&lt; endIndex]) // 以防万一，借助 Objective-C 的能力进行截取 return result ?? (self as NSString).substring(to: maxLength) } } 因为 String 的 init?(_ codeUnits: Substring.UTF16View) 构造器返回可能为空，保险起见最后还是借用了一下 ObjC 的方法来做截取。 好了，到目前为止是我写这篇博客之前的理解和做法。后面开始说问题。 半个 emoji 对上述的场景应用该方法后，我们得到的截取结果其实是有问题的： &quot; g,5就🉑️&quot;.prefixed(6) // g,5就� 6的长度限制正好卡在 🉑️ 的编码上，导致最后遗留下来半个 emoji。 如果仅仅是展示还好，但是如果拿这个字符串在 iOS 13 上，使用 JSONEncoder 去编码，就会造成闪退，而且不论是通过 try catch 也好，try? 也好，都无法捕获到该异常（EXC_BAD_ACCESS）。 解决问题 通过断点可知，原本方法的 result 属性是 nil，最终是通过 ObjC 的 substring 方法得到了目标字符串。 很明显这个问题是 emoji 截取不全导致的，这种情况下我们应该删除整个 emoji，而不是残留半个。 进一步搜索，我找到了这篇文章：Swift 字符串 截取 半个表情emoji \\u0000fffd 的处理 原文解决方法如下所示，通过实践，该方法确实可以解决问题。 // text:Optional(&quot;123456789😒&quot;) var newText = (text as NSString).substring(to: 10) newText = (text as NSString).substring(to: maxLength) // 有可能会截取到半个表情，所以这里剔除掉半个表情的情况 if let data = newText.data(using: .utf8), let temp = NSString(data: data, encoding: String.Encoding.utf8.rawValue), temp.contains(&quot;\\u{0000fffd}&quot;) { newText = temp.replacingOccurrences(of: &quot;\\u{0000fffd}&quot;, with: &quot;&quot;) as String } 进一步优化 其实上面的方法无需将 data 转为 NSString，直接使用 String 去 contains 也是可以的，这个暂且按下不表。 但是对于无法预测的用户输入，每一次截取都要先将 text 转为 data，再转回 string，最后才能做判断，我是不太想接受的。 所以我直接将这个答案丢给了 ChatGPT，结果一系列的调教和实践，最终得到了如下版本的方法： extension String { func prefixed(_ maxLength: Int) -&gt; String { let sequence = utf16 guard sequence.count &gt; maxLength else { return self } let startIndex = sequence.startIndex var endIndex = sequence.index(startIndex, offsetBy: maxLength) while endIndex &gt; startIndex &amp;&amp; UTF16.isTrailSurrogate(sequence[endIndex]) { endIndex = sequence.index(before: endIndex) } let result = String(sequence[..&lt;endIndex]) return result ?? (self as NSString).substring(to: maxLength) } } 一些 UTF-16 的概念 如果你对 “为什么” 不感兴趣，那么现在你已经得到了答案，可以关闭掉这个页面了。 下面的内容将先介绍一些和 UTF-16 有关的概念。 UTF-16 UTF-16编码的字符可以是1个或2个16位码元。 那些只需要1个16位码元的字符被称为BMP（基本多文本平面）字符，如英文字母，数字，标点符号等。 而那些需要2个16位码元的字符被称为非BMP字符，如某些emoji。 Surrogate Pairs 在 Unicode标准中，还包含 &quot;Low surrogate&quot; 和 &quot;High surrogate&quot; 两个概念，或者也称为前导代理（lead surrogate）和 尾随代理（trail surrogate）。 它们共同组成了 &quot;surrogate pairs&quot;，即一个代理对。 它被用来表示在 UTF-16 编码中，不能用单个16位编码单元所表示的字符，取值范围在 U+10000 至 U+10FFFF 之间。 &quot;High surrogate&quot; 是一个代理对中的第一个16位编码单元，其范围从 U+D800 到 U+DBFF。 &quot;Low surrogate&quot; 是一个代理对中的第二个16位编码单元，其范围从 U+DC00 到 U+DFFF。 例如，emoji &quot;🉑️&quot; 在UTF-16编码下，是由两个16位码元组成的：0xD83D 和 0xDD91。这两个码元就是一个代理对，其中 0xD83D 是前导代理，0xDD91 是尾随代理。 回到需求和场景 对于字符串 &quot; g,5就🉑️&quot;，当我们的最大长度为 6 位时，截取的为止恰好在 0xD83D 和 0xDD91 的中间。此时 endIndex 指向的是第7位，即 0xDD91 尾随代理。 所以我们用 UTF16.isTrailSurrogate 方法判断，如果截取的末尾是一个尾随代理，则向前移动一位，将整个 emoji 舍掉。 一些其他的想法 其实其他方法还有很多，比如： 通过文档可知，init?(_ codeUnits: Substring.UTF16View) 当 codeUnits 非法时返回 nil。所以也可以通过这点来编写递归/循环，递减 endIndex。 提前判断所要截取的字符是否是 emoji，如果是，则整个 emoji 截掉。 从 &quot; g,5就�&quot; 中判断是否包含 � 这个字符，如果有就全局替换。 其中第三个方法我还具体实践了一下。 首先我们没有办法通过 utf16 直接得到 String 对象，因为 � 这个符号会导致初始化失败。所以只能通过 NSString 来拿到目标字符串。 其次，当我们拿到了 NSString 对象后，直接调用 .contains(&quot;�&quot;)，返回值居然是 false。 let string = &quot; g,5就🉑️&quot; let original = (string as NSString).substring(to: 6) print(original.contains(&quot;�&quot;)) // false 此时，只要向文中提到的第一种方法那样，对 string 转一次 data 即可判断成功。 let string = &quot; g,5就🉑️&quot; let original = (string as NSString).substring(to: 6) let utf8Data = original.data(using: .utf8)! if let test = String(data: utf8Data, encoding: .utf8) { print(test.contains(&quot;�&quot;)) // true } 但是这一套下来确实有点得不偿失了。 但是，为什么是 false？ 经过翻阅资料，怀疑这是因为 NSString 和 String 的底层处理不同导致的。 对于 NSString，允许存在无效字符，所以 &quot; g,5就�&quot; 中的 � 和我们手写的 � 并非是统一个含义，后者代表一个 “Unicode替换字符”，它用来替换无法识别或无效的Unicode字符。而 NSString 中的 � 就只是一个无效的 Unicode，所以这两个才会不匹配。 这其实也能回答最开始的那个方案，为什么需要先转 data，再转一次 string，最后才能判断成功。 ","link":"https://blog.rakuyoo.top/remove-illegal-characters-from-string/"},{"title":"记 podspec 转 podspec.json","content":"如果你像我一样使用 Xcode 14.3 &amp; CocoaPods 12.4.1，那么恭喜你，你很有可能遇到跟我一样问题：Xcode 14.3 has pod lib lint fail。 如果你正在开发的是一个私有 pod，那还好说，可以直接将 podspec 文件 push 到自己的 git spec 上。 但是如果你像我一样，私有 spec repo 中存储的是 podspec.json，那么接下来的这个命令也许会帮到你： pod ipc spec Name.podspec &gt;&gt; Name.podspec.json 使用该命令可以将 podspec 文件转换为 podspec.json 文件，之后你就可以将 json 文件推到 git spec repo 里了。 ","link":"https://blog.rakuyoo.top/ji-podspec-zhuan-podspecjson/"},{"title":"CGO 与 WireGuardKit 编译","content":"CGO 是一种在 iOS 平台上运行 Go 代码的方案。而 WireGuard 一种 VPN 技术，其中包含了一部分 Go 代码，同时项目提供了一个 Makefile 脚本，使用 CGO 来将相关代码编译成 iOS 静态库。 写这篇文章的契机是，因为希望使用 Mac Catalyst 将内部工具带到 macOS 平台，我又回过头去看一年前（恰巧是22年3月）研究 WireGuard 的过程。 发现虽然当时的研究成功，.xcframewrok 文件还在，但是过程已经模糊不清了。另外还需要针对 Mac Cataglyst 构建出新的静态库，所以又要从头进行梳理不过好在是 “模糊不清”，而不是 “一干二净”，多少还记得一些。 Makefile 我对 Go 语言几乎可以说是一窍不通的。 而对于 Makefile 也只有很少很少的了解。 想要执行 Makefile 文件中的脚本，就在命令行中，在进入对应目录后执行以下命令： # 前提是说，电脑上要安装 make。一般安装了 Xcode 命令行的话都是有的。 make 这里我将该 Makefile 先贴过来： # These are generally passed to us by xcode, but we set working defaults for standalone compilation too. ARCHS ?= x86_64 arm64 PLATFORM_NAME ?= macosx SDKROOT ?= $(shell xcrun --sdk $(PLATFORM_NAME) --show-sdk-path) CONFIGURATION_BUILD_DIR ?= $(CURDIR)/out CONFIGURATION_TEMP_DIR ?= $(CURDIR)/.tmp export PATH := $(PATH):/usr/local/bin:/opt/homebrew/bin export CC ?= clang LIPO ?= lipo DESTDIR ?= $(CONFIGURATION_BUILD_DIR) BUILDDIR ?= $(CONFIGURATION_TEMP_DIR)/wireguard-go-bridge CFLAGS_PREFIX := $(if $(DEPLOYMENT_TARGET_CLANG_FLAG_NAME),-$(DEPLOYMENT_TARGET_CLANG_FLAG_NAME)=$($(DEPLOYMENT_TARGET_CLANG_ENV_NAME)),) -isysroot $(SDKROOT) -arch GOARCH_arm64 := arm64 GOARCH_x86_64 := amd64 GOOS_macosx := darwin GOOS_iphoneos := ios build: $(DESTDIR)/libwg-go.a version-header: $(DESTDIR)/wireguard-go-version.h REAL_GOROOT := $(shell go env GOROOT 2&gt;/dev/null) export GOROOT := $(BUILDDIR)/goroot $(GOROOT)/.prepared: [ -n &quot;$(REAL_GOROOT)&quot; ] mkdir -p &quot;$(GOROOT)&quot; rsync -a --delete --exclude=pkg/obj/go-build &quot;$(REAL_GOROOT)/&quot; &quot;$(GOROOT)/&quot; cat goruntime-*.diff | patch -p1 -f -N -r- -d &quot;$(GOROOT)&quot; touch &quot;$@&quot; define libwg-go-a $(BUILDDIR)/libwg-go-$(1).a: export CGO_ENABLED := 1 $(BUILDDIR)/libwg-go-$(1).a: export CGO_CFLAGS := $(CFLAGS_PREFIX) $(ARCH) $(BUILDDIR)/libwg-go-$(1).a: export CGO_LDFLAGS := $(CFLAGS_PREFIX) $(ARCH) $(BUILDDIR)/libwg-go-$(1).a: export GOOS := $(GOOS_$(PLATFORM_NAME)) $(BUILDDIR)/libwg-go-$(1).a: export GOARCH := $(GOARCH_$(1)) $(BUILDDIR)/libwg-go-$(1).a: $(GOROOT)/.prepared go.mod go build -ldflags=-w -trimpath -v -o &quot;$(BUILDDIR)/libwg-go-$(1).a&quot; -buildmode c-archive rm -f &quot;$(BUILDDIR)/libwg-go-$(1).h&quot; endef $(foreach ARCH,$(ARCHS),$(eval $(call libwg-go-a,$(ARCH)))) $(DESTDIR)/wireguard-go-version.h: go.mod $(GOROOT)/.prepared sed -E -n 's/.*golang\\.zx2c4\\.com\\/wireguard +v[0-9.]+-[0-9]+-([0-9a-f]{8})[0-9a-f]{4}.*/#define WIREGUARD_GO_VERSION &quot;\\1&quot;/p' &quot;$&lt;&quot; &gt; &quot;$@&quot; $(DESTDIR)/libwg-go.a: $(foreach ARCH,$(ARCHS),$(BUILDDIR)/libwg-go-$(ARCH).a) @mkdir -vp &quot;$(DESTDIR)&quot; $(LIPO) -create -output &quot;$@&quot; $^ clean: rm -rf &quot;$(BUILDDIR)&quot; &quot;$(DESTDIR)/libwg-go.a&quot; &quot;$(DESTDIR)/wireguard-go-version.h&quot; install: build .PHONY: clean build version-header install 这个脚本分为以下几个部分： 定义变量：最开始的几行都是定义变量，其中 ?= 语法则是设置默认值，意思是可以从外部设置该值。 定义目标：此后脚本定义了4个目标，分别是 build、version-header、clean 和 install。其中 version-header 目标内还定义了一个函数：libwg-go-a。 目标的执行顺序：最后的 .PHONY 用来定义目标执行顺序，脚本将按照该值的顺序去执行目标。 看完大块我们来看细节。 定义变量 在此不解释每个变量的作用，先重点关注下面几个： ARCHS ?= x86_64 arm64 PLATFORM_NAME ?= macosx SDKROOT ?= $(shell xcrun --sdk $(PLATFORM_NAME) --show-sdk-path) GOARCH_arm64 := arm64 GOARCH_x86_64 := amd64 GOOS_macosx := darwin GOOS_iphoneos := ios 上面已经提到了，这个 Makefile 是用来构建 iOS 静态库的。 那么 ARCHS 这个变量就代表着这个库所支持的架构。默认是 x86_64 和 arm64 两个。 PLATFORM_NAME 和 SDKROOT 要一起看，根据指定的平台获取对应 SDK 的路径，即用何种 SDK 来构建静态库。 PLATFORM_NAME 可选的值包括： 值 含义 iphoneos iOS macosx macOS iphonesimulator iOS 模拟器 后面几个变量则是定义了一些 CGO 的参数，注意这里是没有 iphonesimulator 对应的参数的，只有 iOS 和 macOS。 libwg-go-a 函数 前几行是使用 export 来定义一些环境变量，CGO_CFLAGS 和 CGO_LDFLAGS 算是比较重要的两个 CGO 参数，用来指定 C 编译器和链接器的选项，后面还会提及。 函数内的 $(1) 指的是该方法的第一个参数。在 libwg-go-a 函数的下一行是一个 foreach 语句，它的作用是遍历 ARCHS，再将每一个 ARCH 作为 libwg-go-a 函数的参数传入，来调用 libwg-go-a 函数。所以这里的 $(1) 就是我们在最上面定义的 ARCHS 里的内容。 我们还可以注意到 $(GOOS_$(PLATFORM_NAME)) 这里，代码借助 PLATFORM_NAME 变量套了一层，来拼接获取 GOOS_macosx 或者 GOOS_iphoneos 的值。 其中比较关键的 go build 命令，就是真正负责构建静态库的命令。最后生成的静态库文件名则是 libwg-go-$(1).a。 小结 到这里 Makefile 大致就算是说完了，弄明白了这个文件的大致结构与各部分的作用，后面才好动手对文件进行修改。 通过修改 ARCHS 和 PLATFORM_NAME 的值，如果一切顺利的话，我们可以得到 iOS 和 macOS 平台的静态库，大体上工作也就结束了。 但是往往还会有些额外工作要处理。 编译支持模拟器的静态库 虽然 WireGuard VPN 本身不支持模拟器，但是跳出这个话题，上面有提到，这个脚本现在是不支持 iphonesimulator 的，如果想要支持模拟器该怎么办呢？ 其实不支持的原因很简单：因为不存在 GOOS_iphonesimulator 变量（想一想 $(GOOS_$(PLATFORM_NAME))）。 所以我们可以手动定义该变量。通过查阅资料，模拟器对应的 GOOS 也为 ios，所以变量定义如下： GOARCH_arm64 := arm64 GOARCH_x86_64 := amd64 GOOS_macosx := darwin GOOS_iphoneos := ios +GOOS_iphonesimulator := ios 之后再执行 make 命令，就可以得到支持 iOS 模拟器的静态库了。 编译支持 Mac Cataglyst 的静态库 回归业务，本次我的工作是要编译支持 Mac Cataglyst 的静态库，那么该怎么做呢？ x86_64-apple-ios13.0-macabi 通过查找资料，我了解到 -target x86_64-apple-ios13.0-macabi 这个用于描述特定操作系统和处理器架构的标识符： x86_64 表示处理器架构为 64 位的 Intel 或 AMD 处理器； apple 表示运行在苹果操作系统上； ios13.0 表示操作系统版本为 iOS 13.0； macab 表示使用的是 Mac 上的应用程序二进制接口（Mac Application Binary Interface），即我们想要使用的 Mac Cataglyst。 通过使用这个标识符，我们就可以构建出支持 Mac Cataglyst 的静态库。 接着通过这个回复得知，该值需要赋值给 CGO_CFLAGS 变量。回到脚本中我们可以发现，CGO_CFLAGS 上使用 CFLAGS_PREFIX 进行封装。 在这一步我遇到了2个问题： 不要直接将 -target x86_64-apple-ios13.0-macabi 写在 CFLAGS_PREFIX 的末尾。 省略一些代码后，把两行相关代码放在一起看： CFLAGS_PREFIX := -isysroot $(SDKROOT) -arch $(BUILDDIR)/libwg-go-$(1).a: export CGO_CFLAGS := $(CFLAGS_PREFIX) $(ARCH) 可以看到，实际上 export 的内容是 -isysroot $(SDKROOT) -arch x86_64。 如果我们直接在 CFLAGS_PREFIX 的末尾添加，那么就会把 -arch x86_64 隔开，继而报错。所以添加到 -arch 前即可。 找不到 x86_64-apple-ios13.0-macabi。 相关的问题在 golang/go 的 github 上是可以搜到的，可惜这个问题是针对 go-mobile 的，但是可以注意到回复中的，-iosversion=14 字样。 于是将其改为 x86_64-apple-ios14.0-macabi，果断解决问题。 编译 Mac Cataglyst 虽然操作的是 iOS App，但是最终还是运行在 macOS 上，所以还是要使用 macosx 的 sdk 进行构建。 修改代码如下： ARCHS ?= x86_64 arm64 PLATFORM_NAME ?= macosx SDKROOT ?= $(shell xcrun --sdk $(PLATFORM_NAME) --show-sdk-path) -CFLAGS_PREFIX := $(if $(DEPLOYMENT_TARGET_CLANG_FLAG_NAME),-$(DEPLOYMENT_TARGET_CLANG_FLAG_NAME)=$($(DEPLOYMENT_TARGET_CLANG_ENV_NAME)),) -isysroot $(SDKROOT) -arch +CFLAGS_PREFIX := $(if $(DEPLOYMENT_TARGET_CLANG_FLAG_NAME),-$(DEPLOYMENT_TARGET_CLANG_FLAG_NAME)=$($(DEPLOYMENT_TARGET_CLANG_ENV_NAME)),) -isysroot $(SDKROOT) -target x86_64-apple-ios14.0-macabi -arch 但是运行代码后会发现 arm64 架构的包打不出。 Warning 其实到这里，有关 WireGuard 的相关工作已经 “结束” 了。因为虽然最后我打出了包，但是截止到本文发布，我还未能成功将项目运行起来，所以并不知道接下来的操作是否正确。 通过将 GOOS_macosx 对应的值修改为 ios 可以解决这个问题。所以最终修改为： ARCHS ?= x86_64 arm64 PLATFORM_NAME ?= macosx SDKROOT ?= $(shell xcrun --sdk $(PLATFORM_NAME) --show-sdk-path) -CFLAGS_PREFIX := $(if $(DEPLOYMENT_TARGET_CLANG_FLAG_NAME),-$(DEPLOYMENT_TARGET_CLANG_FLAG_NAME)=$($(DEPLOYMENT_TARGET_CLANG_ENV_NAME)),) -isysroot $(SDKROOT) -arch +CFLAGS_PREFIX := $(if $(DEPLOYMENT_TARGET_CLANG_FLAG_NAME),-$(DEPLOYMENT_TARGET_CLANG_FLAG_NAME)=$($(DEPLOYMENT_TARGET_CLANG_ENV_NAME)),) -isysroot $(SDKROOT) -target x86_64-apple-ios14.0-macabi -arch GOARCH_arm64 := arm64 GOARCH_x86_64 := amd64 -GOOS_macosx := darwin +GOOS_macosx := ios GOOS_iphoneos := ios GOOS_maccatalyst := ios GOOS_iphonesimulator := ios 参考 golang/go with issue #36856 golang/go with issue #47228 Re: Support for M1 MacBook Simulators ","link":"https://blog.rakuyoo.top/cgo-and-wireGuardkit-compilation/"},{"title":"epoxy 源码笔记1：Diffing 算法","content":"最近开始阅读学习 epoxy 的源码，开个新的系列来记录一下学习成功。 本系列文章不对 epoxy 是什么进行讲解，直接讲解其中的部分源码内容。 如果相比较文字，您更喜欢直接阅读代码+注释，那么您可以跳到文章末尾的 总结 一节，该小节包含了一个完整的，带注释讲解的源码。 目录 源码 ContiguousArray 预填充数组 entries 记录新元素 Entry 对象和 trackNewIndex(_:) 方法 记录旧元素 NewRecord &amp; OldRecord 确定元素变动 中场休息 删除、新增与移动 删除 新增和移动 总结 源码 本文主要讲解 Collection+Diff.swift 文件中的 makeChangeset(from:) 方法，该方法可以快速比较出两个集合之间的区别，同时返回增删改的索引集合。 首先贴一下相关的源码： // MARK: - Collection extension Collection where Element: Diffable, Index == Int { /// Diffs two collections (e.g. `Array`s) of `Diffable` items, returning an `IndexChangeset` /// representing the minimal set of changes to get from the other collection to this collection. /// /// - Parameters: /// - from other: The collection of old data. public func makeChangeset(from other: Self) -&gt; IndexChangeset { // Arranging the elements contiguously prior to diffing improves performance by ~40%. let new = ContiguousArray(self) let old = ContiguousArray(other) /// The entries in both this and the other collection, keyed by their `dataID`s. var entries = [AnyHashable: Entry](minimumCapacity: new.count) var duplicates = [Entry]() var newResults = ContiguousArray&lt;NewRecord&gt;() newResults.reserveCapacity(new.count) for index in new.indices { let id = new[index].diffIdentifier let entry = entries[id, default: Entry()] if entry.trackNewIndex(index) { duplicates.append(entry) } entries[id] = entry newResults.append(NewRecord(entry: entry)) } var oldResults = ContiguousArray&lt;OldRecord&gt;() oldResults.reserveCapacity(old.count) for index in old.indices { let id = old[index].diffIdentifier let entry = entries[id] entry?.pushOldIndex(index) oldResults.append(OldRecord(entry: entry)) } for newIndex in new.indices { let entry = newResults[newIndex].entry if let oldIndex = entry.popOldIndex() { let newItem = new[newIndex] let oldItem = other[oldIndex] if !oldItem.isDiffableItemEqual(to: newItem) { entry.isUpdated = true } newResults[newIndex].correspondingOldIndex = oldIndex oldResults[oldIndex].correspondingNewIndex = newIndex } } var deletes = [Int]() var deleteOffsets = [Int]() deleteOffsets.reserveCapacity(old.count) var runningDeleteOffset = 0 for index in old.indices { deleteOffsets.append(runningDeleteOffset) let record = oldResults[index] if record.correspondingNewIndex == nil { deletes.append(index) runningDeleteOffset += 1 } } var inserts = [Int]() var updates = [(Int, Int)]() var moves = [(Int, Int)]() var insertOffsets = [Int]() insertOffsets.reserveCapacity(new.count) var runningInsertOffset = 0 for index in new.indices { insertOffsets.append(runningInsertOffset) let record = newResults[index] if let oldArrayIndex = record.correspondingOldIndex { if record.entry.isUpdated { updates.append((oldArrayIndex, index)) } let insertOffset = insertOffsets[index] let deleteOffset = deleteOffsets[oldArrayIndex] if (oldArrayIndex - deleteOffset + insertOffset) != index { moves.append((oldArrayIndex, index)) } } else { inserts.append(index) runningInsertOffset += 1 } } EpoxyLogger.shared.assert( old.count + inserts.count - deletes.count == new.count, &quot;Failed sanity check for old count with changes matching new count.&quot;) return IndexChangeset( inserts: inserts, deletes: deletes, updates: updates, moves: moves, newIndices: oldResults.map { $0.correspondingNewIndex }, duplicates: duplicates.map { $0.newIndices }) } // MARK: - Entry /// A bookkeeping refrence type for the diffing algorithm. private final class Entry { // MARK: Internal private(set) var oldIndices = [Int]() private(set) var newIndices = [Int]() var isUpdated = false /// Tracks an index from the new indices, returning `true` if this entry has previously tracked /// a new index as a means to identify duplicates and `false` otherwise. func trackNewIndex(_ index: Int) -&gt; Bool { let previouslyEmpty = newIndices.isEmpty newIndices.append(index) // We've encountered a duplicate, return true so we can track it. if !previouslyEmpty, newIndices.count == 2 { return true } return false } func pushOldIndex(_ index: Int) { oldIndices.append(index) } func popOldIndex() -&gt; Int? { guard currentOldIndex &lt; oldIndices.endIndex else { return nil } defer { currentOldIndex += 1 } return oldIndices[currentOldIndex] } // MARK: Private private var currentOldIndex = 0 } // MARK: - OldRecord /// A bookkeeping type for pairing up an old element with its new index. private struct OldRecord { var entry: Entry? var correspondingNewIndex: Int? = nil } // MARK: - NewRecord /// A bookkeeping type for pairing up a new element with its old index. private struct NewRecord { var entry: Entry var correspondingOldIndex: Int? = nil } makeChangeset(from:) 方法是 Collection&lt;Diffable&gt; 的一个扩展，实际使用时你可以简单的理解为一个 [Diffable]。方法接收一个同类型的集合，返回一个 IndexChangeset 类型的结构体。 返回值暂且忽略，咱们直接来看函数体。 ContiguousArray let new = ContiguousArray(self) let old = ContiguousArray(other) 函数体内首先将 self 和 other 转换为 ContiguousArray 类型的数组。从变量名上我们可以得知，self 代表着新的集合，而 other 代表着旧的的集合，也就是被比较的集合。 ContiguousArray 和 Array 类似，区别在于 ContiguousArray 能保证子元素在内存中是连续存储的，也就是说它是一块连续的内存，这就允许 CPU 通过指针进行连续访问，大幅提升性能。而 Array 我们大家都知道，它的内部使用了 “缓冲区”，逻辑上是连续的，但是内存上未必是连续的。 因为在后续的算法中，self 和 other 的大小是固定的，不会发生变化，所以这里将其转换为 ContiguousArray 是更好的选择。 通过源码中的注释我们也能知道，将 self 和 other 转换为 ContiguousArray 类型，可以提升大约 40% 的性能。 后文中，将使用 new 代表 self，使用 old 代表 other。和这里的变量声明保持同步。 预填充数组 毕竟是一个讲究效率的算法，后文中随处可见 .init(minimumCapacity:) 、 reserveCapacity(_:) 方法，这些方法的作用是一样的：设置数组的长度。 我们都知道 Swift 数组扩容通常是直接 *2。上一节中我们有提到，整个算法中数据源的大小是已知的，那么作为一个讲究效率的算法，我们可以在创建数组的同时就设定好数组的大小，避免在后续操作中触发数组的自动扩容，减少因为扩容而带来的性能损耗。 entries var entries = [AnyHashable: Entry](minimumCapacity: new.count) entries 这个字典变量的作用是，将 new 和 old 中，使用相同 id 的元素索引值对应起来。 现在这么讲可能比较抽象，后面结合逻辑一起来谈。 记录新元素 var newResults = ContiguousArray&lt;NewRecord&gt;() newResults.reserveCapacity(new.count) for index in new.indices { let id = new[index].diffIdentifier let entry = entries[id, default: Entry()] if entry.trackNewIndex(index) { duplicates.append(entry) } entries[id] = entry newResults.append(NewRecord(entry: entry)) } 代码首先使用新序列的索引进行遍历，使用对应元素的 id，尝试去 entries 中查找元素。 这里使用了一个小技巧：Dictionay 的 default。当不存在 id 对应的元素时，将返回 default 对应的内容。这里即是返回一个空的 Entry 对象。 稍后讲解 Entry 对象和 trackNewIndex(_:) 方法的具体内容，先大致说一下： Entry 对象用来记录 “拥有同一个 id 的两个元素，其在新、旧两个数组中的位置”。 trackNewIndex(_:) 方法接收索引作为参数，并存储到 Entry 对象中。同时该方法会返回这个 Entry 对象是否已经在之前出现过（相当于一个重复检查）。 现在我们就可以知道这个 for 循环大致的含义和作用了： 它遍历新数组，首先根据 id 将数组元素存储到 entries 对象中，其次对该数组做一个 “重复检查”，并记录下重复的次数。 Entry 对象和 trackNewIndex(_:) 方法 Entry 对象是一个结构体，它的完整声明比较长，这里我就不再重复贴了，读者可以翻到最上面 自行查看。 我们先把目光集中在 newIndices 这个属性以及 trackNewIndex(_:) 方法上。 trackNewIndex(_:) 方法的实现为： /// Tracks an index from the new indices, returning `true` if this entry has previously tracked a new index as a means to identify duplicates and `false` otherwise. func trackNewIndex(_ index: Int) -&gt; Bool { let previouslyEmpty = newIndices.isEmpty newIndices.append(index) // We've encountered a duplicate, return true so we can track it. if !previouslyEmpty, newIndices.count == 2 { return true } return false } 该方法将参数存储到 newIndices 里。这是它的作用之一：记录该 id 在新数组（new）中的位置。 然后判断，如果 newIndices 中的元素等于 2，则意味着出现了重复。这很好理解，newIndices 大于 1 意味着同一个 id 出现了两次。不过截止到发文我也没能理解，为什么是 == 2 而不是 &gt;= 2。结合后文推断，猜测是不希望 duplicates 中出现重复的元素。 记录旧元素 var oldResults = ContiguousArray&lt;OldRecord&gt;() oldResults.reserveCapacity(old.count) for index in old.indices { let id = old[index].diffIdentifier let entry = entries[id] entry?.pushOldIndex(index) oldResults.append(OldRecord(entry: entry)) } 记录旧元素的逻辑比记录新元素的逻辑要简单一些：当我们通过 id，如果在 entries 映射表中查到对应元素时（意味着该 id 存在对应的新元素），则存储该元素在旧数据中的索引值。否则不进行存储（因为 entries 的意思是存储 “新-旧” 的对应关系，不存在新，那么旧就没有意义）。 NewRecord &amp; OldRecord 在遍历新集合和旧集合的循环中，末尾都是创建了一个对应的 XXRecord 对象。这两个类型的声明比较相似，除了存储对应的 Entry 之外，还带有一个 Int? 类型的 correspondingXXXIndex 属性，该属性默认是 nil。 correspondingOldIndex 意味着在新集合中，该元素在旧集合中的位置。 correspondingNewIndex 意味着在旧集合中，该元素在新集合中的位置。 如果是 nil，则意味着该元素在另外的集合中不存在。 这两个属性都在后面的算法中起到了重要的作用。 确定元素变动 for newIndex in new.indices { let entry = newResults[newIndex].entry if let oldIndex = entry.popOldIndex() { let newItem = new[newIndex] let oldItem = other[oldIndex] if !oldItem.isDiffableItemEqual(to: newItem) { entry.isUpdated = true } newResults[newIndex].correspondingOldIndex = oldIndex oldResults[oldIndex].correspondingNewIndex = newIndex } } 这个对新集合的遍历，确定了两个集合之间元素的变动。 首先是对新集合的遍历，因为 newResults 可以说就是 new 通过 map 得到的，所以可以使用 newIndex 直接从 newResults 中进行取值，而不用担心是否会越界。 你应该还记得，entry 对象中存储着该 id 对应的元素在新、老集合中对应的索引。如果在新、老集合中不存在，那么对应的索引也就不存在。所以通过 popOldIndex() 方法（通过方法名也能看出它的作用），可以通过 entry 对象判断是否存在旧的元素。 到这一步我们应该能知道：如果 newResults 中的某一个元素的 correspondingOldIndex 是 nil，那么该元素就是新增的。那么反过来，oldResults 中的逻辑也是一样的。 实际上上一节中，关于 correspondingOldIndex 和 correspondingNewIndex 这两个属性的作用，也是我通过这个循环推断出来的。 接着往下看。如果存在旧的元素，那么我们就要比较新旧两个元素是否一致，如果不一致，则通过 entry.isUpdated 记录该元素发生了更新。 最后在 newResults 和 oldResults 中记录该元素在另外一个数组中的位置。 中场休息 到了这里，其实我们可以小结一下，先总结一下 newResults、oldResults 和 entries 三个变量的作用。 entries 的主要就是将新旧两个数组，通过相同的 id 做一个匹配，同时记录相应的索引。匹配到一起之后用来创建 newResults 和 oldResults 对象。该变量主要作用于算法的前半部分，为创建 newResults 和 oldResults 而生，创建完之后它的任务就结束了。 newResults 和 oldResults 这两个变量最主要的作用，按我的理解就是：“通过一个Int 类型的可选值，记录了 entry 的前世今生”。 correspondingXXXIndex 的存在有两个含义： 是否为空意味着有前世，或者有今生 不只是有，还能拿到对应的位置 感慨一下如果没有 “可选值” 这个概念，则需要借助 Bool + Int 两个变量来完成这个工作。或者判断一下 nil？ 删除、新增与移动 在得到 newResults 和 oldResults 两个对象后，要计算出两个集合之间的差异就比较简单了。 删除 后面的算法首先处理的是删除操作。 var deletes = [Int]() var deleteOffsets = [Int]() deleteOffsets.reserveCapacity(old.count) var runningDeleteOffset = 0 for index in old.indices { deleteOffsets.append(runningDeleteOffset) let record = oldResults[index] if record.correspondingNewIndex == nil { deletes.append(index) runningDeleteOffset += 1 } } 其实上文已经做了解释：如果 correspondingNewIndex 是 nil，则意味着该元素是被删除的元素。所以这次是对 old 进行遍历，而不是遍历 new 。 除了将被删除的元素记录到 deletes 之外，该循环还将 “在这个位置之前，有多少个被删除了多少个元素” 记录到了 deleteOffsets 数组中。 例如 [1, 2, 3, 4, 5, 6, 7] 和 [2, 3, 5, 7] 这两个集合做比较，那么 deleteOffsets 的值就是 [0, 1, 1, 1, 2, 2, 3]。代表着 “5这个元素之前，有两个元素被删除掉了”。 该算法是先将 runningDeleteOffset 添加到 deleteOffsets 里，再做递增操作。所以记录的是 “本元素之前，被删除了多少个元素”。 新增和移动 var inserts = [Int]() var updates = [(Int, Int)]() var moves = [(Int, Int)]() var insertOffsets = [Int]() insertOffsets.reserveCapacity(new.count) var runningInsertOffset = 0 for index in new.indices { insertOffsets.append(runningInsertOffset) let record = newResults[index] if let oldArrayIndex = record.correspondingOldIndex { if record.entry.isUpdated { updates.append((oldArrayIndex, index)) } let insertOffset = insertOffsets[index] let deleteOffset = deleteOffsets[oldArrayIndex] if (oldArrayIndex - deleteOffset + insertOffset) != index { moves.append((oldArrayIndex, index)) } } else { inserts.append(index) runningInsertOffset += 1 } } 首先我们发现遍历的开头，用了 record.correspondingOldIndex 来做判断，意味着 “该元素有没有对应的老元素”。 然后从下往上看 else 的逻辑，和删除的时候一样，这次循环通过 inserts 记录了 “该元素之前，插入了多少个元素”。这里我就不用具体的例子举例了，大家应该可以明白。 那么再回到 if 里的逻辑，首先 isUpdated 是之前已经判断过了的。 那么 (oldArrayIndex - deleteOffset + insertOffset) != index 该怎么理解呢？ 首先我们明确每个参数的含义： index 是某个元素在 “新集合” 中的位置。 oldArrayIndex 是该元素在 “旧集合” 中的位置。 deleteOffset 意味着该元素之前删除了几个元素。 insertOffset 意味着该元素之前添加了几个元素。 那么我们就可以理解了：对于旧集合而言，如果去掉被删除的，再加上新增的，如果索引位置不变，那么该元素的位置没有发生移动。 通过具体的例子来看一下： [1, 2, 3, 4, 5, 6, 7] 和 [2, 3, 7, 5]。我们遍历的是后者，即新集合。deleteOffsets 是 [0, 1, 1, 1, 2, 2, 3]。 假设此时我们遍历到了新集合中的 3。 那么 index 是 1，oldArrayIndex 是 2，deleteOffset 是 1，insertOffset 是 0。表达式为 2 - 1 - 0 == 1，不符合条件（注意代码中用的是 != 号），那么则不会添加到 moves 里，意味着 3 这个元素没有被移动。 再假设此时我们遍历到了新集合中的 7。 那么 index 是 2，oldArrayIndex 是 6，deleteOffset 是 3，insertOffset 是 0。表达式为 6 - 3 - 0 != 2，符合条件，所以判断 7 发生了移动。 最后遍历到 5 的时候， index 是 3，oldArrayIndex 是 4，deleteOffset 是 2，insertOffset 是 0。表达式为 4 - 2 - 0 != 3，符合条件，所以判5 也是发生了移动。 这样，我们就可以筛选出哪些元素发生了移动。 总结 epoxy 的 Diffing 算法大体可以分为一下几个部分： 将输入转换为 ContiguousArray 类型，提升效率。 使用哈希表，将同一个 id 的元素从新旧两个集合中摘出来，做匹配，同时记录原本的位置。 利用这个哈希表，配合 LCS 算法，找出哪些元素被删除，哪些元素是新增的。 根据需求，进一步将第三步的结果进行处理。 最后贴上一下我自己对源代码做的注释： /// 对两个 `Diffable` 元素集合（例如 `Array`）进行差异化处理，返回一个 `IndexChangeset`，表示从 `other` 到此集合的最小更改集合。 /// /// - Parameters: /// - from other: 旧数据集合。 public func makeChangeset(from other: Self) -&gt; IndexChangeset { // 在进行差异化处理之前将元素连续排列可以提高大约40%的性能。 let new = ContiguousArray(self) let old = ContiguousArray(other) // 通过 `dataID`，将 `self` 和 `other` 的对应元素存储到 `entries` 里。 var entries = [AnyHashable: Entry](minimumCapacity: new.count) var duplicates = [Entry]() // 用来记录新元素（`self`）和旧索引的一个对应关系 var newResults = ContiguousArray&lt;NewRecord&gt;() newResults.reserveCapacity(new.count) // for index in new.indices { // `new` 其实就是 `self`，从 `self` 里拿标识符 let id = new[index].diffIdentifier let entry = entries[id, default: Entry()] // 1. 记录这个新元素的位置 // 2. 判断当前数组的重复性，因为是数组，所以内部数据有可能会有重复的情况。 if entry.trackNewIndex(index) { duplicates.append(entry) } entries[id] = entry // `correspondingOldIndex` 默认是 `nil` newResults.append(NewRecord(entry: entry)) } // 用来记录旧元素（`other`）和新索引的一个对应关系 var oldResults = ContiguousArray&lt;OldRecord&gt;() oldResults.reserveCapacity(old.count) for index in old.indices { // 从 `other` 里拿标识符 let id = old[index].diffIdentifier // 用 `other` 的 `id`，去 `self` 里找对应的值 let entry = entries[id] // 如果找到的话，就将旧的索引存储到 `entry` 里。 // `entry` 里既存储了新（`self`）的索引，又存储了旧（`other`）的索引。 // 因为 `id` 是一样的，所以就通过 `id` 把新旧两个元素的索引对应起来了。 entry?.pushOldIndex(index) // `correspondingNewIndex` 默认是 `nil` oldResults.append(OldRecord(entry: entry)) } for newIndex in new.indices { // `newResults` 是通过遍历 new 得到的 // 所以 `index` 是一致的，可以直接用 `new` 的 `index` 拿到 `newResults` 的元素 let entry = newResults[newIndex].entry // 如果存在新元素对应的老元素的话，可能对应更新操作 if let oldIndex = entry.popOldIndex() { let newItem = new[newIndex] let oldItem = other[oldIndex] // 如果两个对象不相等，则是更新操作 if !oldItem.isDiffableItemEqual(to: newItem) { entry.isUpdated = true } // 记录两个元素相对的位置 // 可能从 `old` 更新到 `new`，所以索引是有需要的 newResults[newIndex].correspondingOldIndex = oldIndex oldResults[oldIndex].correspondingNewIndex = newIndex } } // 删除操作 var deletes = [Int]() var deleteOffsets = [Int]() deleteOffsets.reserveCapacity(old.count) var runningDeleteOffset = 0 for index in old.indices { deleteOffsets.append(runningDeleteOffset) let record = oldResults[index] // 这里等于 `nil`，意味着在上一个循环中，没有匹配到对应的 `new` 元素 // 所以意味着在 `new` 数组中，没有该元素存在，所以该元素被删除了 if record.correspondingNewIndex == nil { deletes.append(index) runningDeleteOffset += 1 } } var inserts = [Int]() var updates = [(Int, Int)]() var moves = [(Int, Int)]() var insertOffsets = [Int]() insertOffsets.reserveCapacity(new.count) var runningInsertOffset = 0 for index in new.indices { insertOffsets.append(runningInsertOffset) let record = newResults[index] // 有新，有老 if let oldArrayIndex = record.correspondingOldIndex { if record.entry.isUpdated { // 更新，记录元素的对应关系 updates.append((oldArrayIndex, index)) } // `insertOffsets` 和 `deleteOffsets` 里的值，类似于 `[0, 0, 1, 1, 1]` 这种形式 // 其长度分别等于 `new` 和 `old`，所以直接使用对应的 `index` 取值不会导致溢出 let insertOffset = insertOffsets[index] let deleteOffset = deleteOffsets[oldArrayIndex] // 旧位置 - 删除了的元素个数 + 新增了的元素个数 != 新位置 -&gt; 移动了 if (oldArrayIndex - deleteOffset + insertOffset) != index { moves.append((oldArrayIndex, index)) } } // 有新，没老 else { // 意味着新的元素相对于老的元素来说是插入进来的 inserts.append(index) runningInsertOffset += 1 } } EpoxyLogger.shared.assert( old.count + inserts.count - deletes.count == new.count, &quot;Failed sanity check for old count with changes matching new count.&quot;) return IndexChangeset( inserts: inserts, deletes: deletes, updates: updates, moves: moves, newIndices: oldResults.map { $0.correspondingNewIndex }, duplicates: duplicates.map { $0.newIndices }) } 后记 文章算是写完了，但是感慨自己虽然花了很多时间，算是 “弄明白” 了本算法，但是也仅限于看懂，然后感慨它的精妙。甚至不敢肯定以后遇到相似的需求时，能用上该算法。哎，希望今后还能有所长进。 ","link":"https://blog.rakuyoo.top/epoxy-source-code-notes-1/"},{"title":"加速 Swift Package Manager","content":"虽然国内实际开发用 Swift Package Manager（后称 SPM）的比较少，但是国外的一些开源库里却经常用到。所以经常遇到项目 clone 下来之后，SPM 加载失败导致项目无法运行的问题。 网上关于 SPM 加速的文章也有很多，方法也是五花八门，本文挑选几个进行记录。 根本问题 根本问题还是 Xcode 内部服务无法连接代理，导致访问 Github 经常超时。 所以解决方案还是要从代理入手。 解决方案 先列举一下本文记录的几种解决方案： 路由器端直接设置代理 通过命令行直接拉取 ClashX 增强模式 代理 Xcode 设置 SPM Mirror 以上方法在我看来谈不上哪个更好。开发应当在合适的时候选择合适的方案。 路由器端直接设置代理 在路由器上直接设置代理，从网络源头解决问题。但是哪怕是在一些公司，也不太方便添加软路由等方法做这些操作。有条件的个人可以搞一下。 通过命令行直接拉取 参考 如何让swift package manager走代理 具体方法： 首先是打开终端，然后在终端内按照 “为终端设置代理” 的方法，设置 https_proxy 、 http_proxy 和 all_proxy 在 项目根目录下 执行下列命令： xcodebuild -resolvePackageDependencies -scmProvider system 通过命令行直接拉取 SPM 依赖，执行完成后打开项目即可。 有的文章可能提到可以为 git config 设置代理，其命令为： # 需要将端口替换成自己的端口 git config --global http.proxy 'http://127.0.0.1:1080' git config --global https.proxy 'http://127.0.0.1:1080' 这里我没有进行尝试，有想法的读者可以自行尝试。 ClashX 增强模式 ClashX Pro 的增强模式可以穿透绝大多数软件，包括终端、Xcode。开启后直接使用 Xcode 即可下载依赖。 这个方案的缺点也比较明显：增强模式会导致一些内网服务不可用，所以建议仅在需要时临时打开，不建议常驻使用。所以你需要记得在 Fetch 成功后关闭增强模式。 如果好奇 ClashX 做了什么，可以参考：请问下增强模式是做了什么处理 代理 Xcode 如果你没有或者不想使用 ClashX，那么可以通过 Proxifier 进行拦截，然后再走代理。（不过虽然 Proxifier 提供了免费试用，但仍然算是收费那一档的） 有条件的伙伴可以查看 Xcode设置SPM代理 这篇文章，里面有详细的介绍。 设置 SPM Mirror 介绍和使用方法 最近偶然间发现，SPM 在 Swift 5 提供了一个 Mirror 镜像功能，提案在这里：Package Manager Dependency Mirroring 即如下命令： swift package config set-mirror --original https://github.com/QMUI/LookinServer.git --mirror http://git.aaaaaa.top/LookinServer.git --original 是原地址，而 --mirror 则是镜像仓库的 url。 所以有条件的通过可以在内网 Git 上 Fork Github Repo，或者在国内找寻现有镜像，然后通过 Mirror 下载依赖。 不过有一点需要注意：提案中有如下这么一句话： The Package.resolved file will contain the mirror URLs that were used during dependency resolution. 但是在 Xcode 15 上实践后发现 Package.resolved 显示的还是 Original URL，不是 Mirror URL。 搜索后发现有这么一个 issue：SwiftPM: mirrored URLs end up in Package.resolved file。也就是说 Package.resolved 文件中显示 Original URL 在现在是正确行为。至于 Fetch 时使用的哪个 url，可以在 Report navigator 中查看具体的 log。 Package.swift 如果你开发的是一个 Swift 框架，要管理 Package.swift 中的依赖（使用该文件打开项目）， 那么可以直接在项目根目录下执行上述命令。 xcodeproj 但是如果你是想为 xcodeproj 中添加的依赖设置 mirror，那么在项目根目录下使用 swift package config set-mirror 命令是会报错的：error: Could not find Package.swift in this directory or any of its parent directories. 通过报错可知该方法仅适用于 Package.swift 管理的项目。但是别着急。 在 Apple 文档 Adding package dependencies to your app 中有这么一小段描述： You can find the Package.resolved file inside your .xcodeproj directory at [appName].xcodeproj/project.workspace/xcshareddata/swiftpm/Package.resolved. 通过这段描述我们可以知道，project.workspace 里面有一个 swiftpm 目录，进入后会发现该目录下就有一个 configuration 文件夹。 所以我们可以直接将 set-mirror 命令生成的 mirrors.json 文件放到该文件夹下，即可完成镜像设置。 注意：一般情况下我们的 .gitignore 文件配置会导致该路径不会被提交到 git 上。这点请您注意。 此时回到 Xcode 中添加依赖，就会发现速度 ... 好像没有变化。这就十分尴尬了... 通过进一步观察、查看 log 以及搜索相关资料。我进一步确定：mirror 这个配置仅适用于 Package.swift 软件包。所以如果我们直接在 Xcode 里通过一个 github 的 url 来添加依赖，那么它是不会走 mirror 配置的。但是该依赖所依赖的其他组件，又能走 mirror 配置，因为它是一个 Package.swift 软件包。 所以回到这小节讨论的问题上：你只能使用上面提到的其他方法，来解决在 Xcode 里向 project 添加依赖的问题。 我们要放弃吗？并不是，我想到了一个折中/取巧的方法来解决该问题。详见我在 stackoverflow 上的回答。 总结 让我们回到实际场景下来看看上述提到的几个解决方案： 你的 CI/CD 只能访问内部网络： 不论你是在开发应用程序，还是你或你的公司在开发 SPM 软件包。在只能访问内部网络，完全无法访问 Github 的情况下，你只能通过 mirror 的方式来使用 SPM。 你的公司没有那么多要求，而且人均具备搭梯子的能力/你是具有搭梯子能力的个人开发者： 那么其他几种方法都可以满足你的需求。 ","link":"https://blog.rakuyoo.top/accelerated-swift-package-manager/"},{"title":"圆角 & 离屏渲染实践","content":"下文主要针对各种加圆角的情况进行实践，看看会不会出现离屏渲染的情况。 测试条件 测试平台：iPhone 12，iOS 16.3.1，Xcode 14.2。 测试方法：运行项目后，通过 Xcode 设置 “Debug -&gt; View Debugging -&gt; Rendering -&gt; Color Off-screen Rendered” 来打开离屏渲染检测。 主要测试点： 是否开启 masksToBounds 是否设置图片 是否设置背景色” 父视图是否具有透明度 结论 这里先把结论放到最前面，后面您可以跳过实践过程，直接切到解决方案一节。 ✅ 代表不会触发离屏渲染；❌ 代表会触发离屏渲染。 UIView UIImageView UIButton（只有文字） UIButton（图片/背景图） 直接设置 ✅ ✅ ✅ ❌ 背景色（非透明） ✅ ❌ ✅ ❌ 父视图具有透明度（alpha） ❌ ❌ ❌ ❌ 下面重点说一下 UIImageView 和 UIButton 的测试过程： UIImageView iOS 9 对 UIImageView 进行了一系列优化，相比较过去而言，触发离屏渲染的场景要小了不少 直接设置 lazy var testView = UIImageView().then { $0.image = UIImage(named: &quot;Forms/share-cover&quot;) $0.layer.masksToBounds = true $0.layer.cornerRadius = 30 } 一个设置了图片的 UIImageView。示例中同时设置了 masksToBounds 和 cornerRadius。从图片上可以看出来，并不会触发离屏渲染： UIImageView 不设置 masksToBounds，只设置 cornerRadius 的话不会显示圆角。 添加背景色 那如果我们手贱再给它添加一个背景色呢？ lazy var testView = UIImageView().then { $0.image = UIImage(named: &quot;Forms/share-cover&quot;) $0.layer.masksToBounds = true $0.layer.cornerRadius = 30 $0.backgroundColor = .white // 添加一个白色的背景色 } 果然手贱是没有好处的，设置了背景色后则会触发离屏渲染。 UIButton UIButton 默认也是没有背景色的，需要同时考虑文字和图片两种情况。 文字 如果不设置背景色的话，只有文字的 UIButton 对象是显示不出来圆角的，不过我也是分别测了一下。 lazy var testView = UIButton(type: .custom).then { $0.setTitle(&quot;这是一个标题&quot;, for: .normal) $0.layer.cornerRadius = 30 } 结论是：不论设置不设置背景色，只有文字的 UIButton 对象添加圆角后都不会触发离屏渲染。 同时只需要设置 cornerRadius，不需要开启 masksToBounds 就可以显示出来圆角。 图片 &amp; 背景图 因为现象一致，所以图片和背景图在这里归位一类进行讨论。 带图片的 UIButton 是重点测试对象，分下面几种情况： 直接设置 lazy var testView = UIButton(type: .custom).then { $0.setTitle(&quot;这是一个标题&quot;, for: .normal) $0.layer.masksToBounds = true $0.layer.cornerRadius = 30 $0.setImage(UIImage(named: &quot;Forms/share-cover&quot;), for: .normal) } 如果 UIButton 只设置 cornerRadius 而不开启 masksToBounds，那么图片是不会显示出圆角的。 从图上看，是会触发离屏渲染。 尝试解决离屏渲染问题 实践了哪些情况会触发离屏渲染，接下来就想办法尝试解决这些问题。 用父视图进行包裹 有的文章会提到用一个父视图包裹需要添加圆角的视图，然后将圆角添加到父视图上。 实践发现该方法并不能解决离屏渲染问题。 避免开启 masksToBounds UIImageView 视图可以在不开启 masksToBounds，仅设置 cornerRadius 的情况下显示圆角，只包含文字的 UIButton 对象也一样。 所以在这种情况下，不开启 masksToBounds 也可以避免触发离屏渲染。 为图片添加圆角 直接为图片添加圆角是比较常用的避免离屏渲染的方法。只不过现在比较常用的 UIImageView 视图已经不会触发离屏渲染了，但是 UIButton 依然可以这么做。 下面提几点实际需求里可能会遇到的问题： “拼接图片” 例如微信的群聊头像，可能涉及到多个图片在一个视图控件中进行展示。 这个时候最好将多张图片拼到一起，然后对这张图片整体设置圆角。而不是在一个 UIView 中尝试添加多个 UIImageVIew，再对 UIView 设置圆角。 视图尺寸不固定 一般 UIButton 会遇到 “尺寸不固定 + 需要圆角 + 背景色” 的情况。 例如页面某个位置有一个 “距离屏幕两侧 10px，可用状态背景色为蓝色，不可用状态下为灰蓝色” 的按钮。 此时可以考虑找 UI 切一个带圆角的纯色图片直接用作底色，但是我们开发也可以自己生成这样一张图片，然后通过拉伸来达到相同的效果。 首先我们先找喵神借用一个非常好用的枚举，来表示圆角： extension UIImage { enum Radius { /// 圆角半径应该按照图片**宽度**的比例计算。 /// 通常关联的值应该在0和0.5之间，其中0表示没有圆角，0.5表示使用图片宽度的一半作为圆角半径。 case widthFraction(CGFloat) /// 圆角半径应该按照图片**高度**的比例计算。 /// 通常关联的值应该在0和0.5之间，其中0表示没有圆角，0.5表示使用图片高度的一半作为圆角半径。 case heightFraction(CGFloat) /// 使用一个固定的点值作为圆角半径。 case point(CGFloat) func compute(with size: CGSize) -&gt; CGFloat { let cornerRadius: CGFloat switch self { case .point(let point): cornerRadius = point case .widthFraction(let widthFraction): cornerRadius = size.width * widthFraction case .heightFraction(let heightFraction): cornerRadius = size.height * heightFraction } return cornerRadius } } } 之后生成纯色图片： extension UIImage { static func color( _ color: UIColor, size: CGSize = .init(width: 1, height: 1), radius: Radius? = nil ) -&gt; UIImage { let cornerRadius = radius?.compute(with: size) let renderer = UIGraphicsImageRenderer(size: size) let image = renderer.image { _ in let rect = CGRect(origin: .zero, size: size) let path: UIBezierPath if let cornerRadius = cornerRadius { path = .init(roundedRect: rect, cornerRadius: cornerRadius) } else { path = .init(rect: rect) } color.setFill() path.fill() } guard let cornerRadius = cornerRadius else { return image } let insets = { UIEdgeInsets(top: $0, left: $0, bottom: $0, right: $0) }(cornerRadius) return image.resizableImage(withCapInsets: insets, resizingMode: .stretch) } } 你还可以扩展这个方法，配置需要圆角的位置，而不是为四个边都添加圆角，比较简单，这里就不再赘述了。 ","link":"https://blog.rakuyoo.top/off-screen-rendering-practice/"},{"title":"iOS自编笔试题","content":"最近要负责面试，于是各处搜罗了一些面试题，用于笔试（不包含算法），也当作自己的一个复习。 面试题部分请参考：iOS自编面试题 规则： 40分钟作答时间。 禁止使用手机查阅答案。禁止使用 ChatGPT 类软件查阅答案。 类名、变量名以及方法名，不记得全拼的允许使用缩写。 注： 本题目多数由 ChatGPT 生成，对应答案已记录在册。 Swift Swift中的let和var有什么区别？何时应该使用let？ Swift中的属性（Property）有哪些类型？它们的作用是什么？ Swift的常量（static）在什么时候确定？ 在Swift中，switch case 一个 NSObject 子类时，比较的是什么？ 请解释 Swift 中的范型擦除（Generic Erasure）是什么，并提供一个范型擦除的例子。 iOS 开发 你在实际工作中使用过UICollectionViewCompositionalLayout吗？如果使用过，请详细描述一下实现的功能。 请列举5-10个你最常用的三方库，并选择一个你最喜欢的进行介绍，阐述原因。 架构相关： 什么是单例模式？在iOS开发中，如何实现一个单例？ 代码格式规范 请列出你认为的 Swift 代码格式规范，包括但不限于缩进、空格、命名等方面。 在你的项目中，你如何确保代码格式的一致性？请举例说明。 请设计一个小模块，尽可能全面地向我展示你自己的编码风格。 ","link":"https://blog.rakuyoo.top/ios-self-paced-test-questions/"},{"title":"iOS自编面试题","content":"最近要负责面试，于是各处搜罗了一些面试题，用于当面面试使用（非笔试题），也当作自己的一个复习。同时提供了一个我自己角度的评判，仅供参考。 笔试题部分请参考：iOS自编笔试题 以下所以答案，如果面试时未能回答，作为补充问题回答正确后，相应分值 - 1。 回答了其他答案，正确且合理的情况下，酌情给分。 Swift 相关 主要考察Swift语法相关内容 基础 什么是Swift方法默认值？什么情况下会使用它？ 得分 分值 答案 1 在定义函数时给参数赋一个默认值，调用该函数时可以不传该参数值 3 减少函数重载的需要 1 避免在调用函数时多次输入相同的值 什么是Optional类型，它的作用是什么？如何使用Optional类型？ 得分 分值 答案 2 用来表示一个值可能存在，也可能不存在的情况 1 Optional 类型的作用是为避免因为没有处理 nil 而导致程序崩溃的问题 1 Optional 类型可以通过在类型名后加一个 ? 来定义 2 如果一个 Optional 类型的变量有值，那么可以使用可选绑定 （if let）将其取出 3 在使用 Optional 类型时，尽量避免使用强制解包 3 Optional 的实际实现是一个枚举 Swift中的struct和class有什么区别？什么情况下应该使用struct？ 区别： 得分 分值 答案 备注 1 class 支持继承，而 struct 不支持继承 - 1 class 支持类型转换，而 struct 不支持类型转换 这里的类型转换指的是子类和父类之间的隐式类型转换 1 class 是引用类型，而 struct 是值类型 - 使用情况： 得分 分值 答案 1 如果对象的复杂度较高，并需要支持继承和类型转换，应该使用 class 1 如果对象比较简单，并且需要高性能，可以考虑使用 struct 什么是 Swift 中的访问控制（Access Control）？它有哪些级别？ 得分 分值 答案 备注 1/3 Open: 可以被同一模块内和外部模块的代码访问，允许被继承和重写 提到外部模块计3分，否则1分 1/3 Public: 可以被同一模块内和外部模块的代码访问，但是在外部模块不能被重写或继承，内部模块可以 提到外部模块计3分，否则1分 1/3 Internal: 只能被同一模块内的代码访问，不能被外部模块访问（默认访问级别） 提到是默认权限计3分，否则1分 1 fileprivate: 只能被同一源文件内的代码访问 - 1 private: 只能在所定义的作用域内访问（比如函数、方法、类、结构体等），不能被同一文件中的其他作用域访问。 - 2 Xcode 生成的 struct 的初始化方法是 Internal 属性，有外部访问需求的话要手动实现 init 方法 - 什么是 Swift属性观察器（Property Observer）？ 得分 分值 答案 备注 1 知道属性观察器是 willSet 和 didSet - 1 只能应用于存储属性，而不能应用于计算属性 - 2+1 如果属性是在初始化期间（init和定义时赋值）设置的，则不会调用 willSet 和 didSet 观察器 回答 init，2分，回答定义时，1分，总分3分 Swift中一个方法如何返回多个值？ 得分 分值 答案 1 可以使用元组（Tuple）来实现一个方法返回多个值的效果 0 自定义类型、数组、字典等 什么是 Swift 中的 Extension（扩展）？如何使用 Extension？ 得分 分值 答案 1 允许开发者向已有的类型添加新的方法、计算属性等，而无需继承子类或使用 Objective-C runtime 1 不能添加存储属性或属性观察器 1 扩展中的方法不能和已有方法重名 2 使得代码更加模块化，易于维护 2 也可以对第三方类进行扩展，而无需改动原代码，更加方便地满足自己的需求 Swift中的协议（protocol）是什么？它有什么作用？ 如果回答中包含了协议中的范型等内容，参考 什么是范型 一题中的分值。 得分 分值 答案 备注 1 协议定义了一组方法、属性 - 1 可以被类、结构体和枚举类型实现，然后实现方就可以使用协议中提供的内容 - 3 提高代码的模块化程度、可重用性和可维护性，减少代码的耦合性 - 1/2 可以扩展协议，为协议添加默认实现 如果面试者没有提，可以进行提问“Swift 协议如何做默认实现？” 5 习惯使用扩展遵循协议，而不是在声明类型时 - -5 习惯在声明类型时遵循协议 - 1/3/10 提到了面向协议编程 仅仅提到，计1分；有对应描述、解释，计3分；描述详细，并配合示例，计10分 什么是泛型（Generic）类型？它们在Swift中有什么作用？ 笔试题中包含范型擦出，所以本题主要考察面试者有没有 “用范型提高代码可读性、维护性” 的意识。 在这里不必回答范型擦出。回答范型擦出不额外计分。 得分 分值 答案 1 可以用于定义函数、类、结构体、枚举等多种类型的通用类型机制 2 泛型类型可以用于消除代码重复，提高代码的可读性和可维护性 1 定义协议时，可以使用关联类型来实现更加灵活的数据处理 什么是Swift字面量协议？ 得分 分值 答案 1 ExpressibleByNilLiteral 1 ExpressibleByIntegerLiteral：允许类型从整数字面量创建实例。 1 ExpressibleByFloatLiteral：允许类型从浮点数字面量创建实例。 1 ExpressibleByBooleanLiteral：允许类型从布尔字面量创建实例。 1 ExpressibleByStringLiteral：允许类型从字符串字面量创建实例。 1 ExpressibleByArrayLiteral：允许类型从数组字面量创建实例。 1 ExpressibleByDictionaryLiteral：允许类型从字典字面量创建实例。 只要提到有这么几个协议（不用说出具体的名字），可以通过数字、字符串、Bool、数组、字典字面量来初始化对应的类型即可。 什么是延迟加载（懒加载，Lazy Loading）？在Swift中你一般什么时候使用它？ 得分 分值 答案 备注 1 在Swift中，延迟加载可以通过使用 lazy 关键字来实现 - 1 lazy 关键字可以用于属性 - 1 在第一次访问该变量或属性时，系统会执行相应的代码进行初始化 - 5 几乎无脑使用懒加载来定义任何的属性 - Swift中的高阶函数（Higher Order Function）有哪些？请分别说明它们的作用。 主要包含以下几个即可，其余的例如 contains、allSatisfy 等说了也计1分。 如果只说出了函数名，没有说明作用，依然得分。或者能说出来功能，具体函数名拼不全也可。 如果下面这些没有说全，面试官可提示 “还有没有了？” 得分 分值 答案 备注 1 map：将一个集合中的每个元素通过一个函数映射为另一个元素，返回一个新的集合 - 1 filter：过滤一个集合中符合特定条件的元素，返回一个新的集合 - 1 reduce：通过对一个集合中的所有元素进行累加或累积操作，返回一个最终的结果 - 1 flatMap：将一个集合中的每个元素通过一个函数映射为另一个集合，然后将所有集合中的元素合并为一个新的集合 - 1 compactMap：对一个序列进行变换操作，并返回一个新的序列，该序列将所有的 nil 元素去除 - 1 sorted：对一个集合中的元素进行排序，返回一个新的排序后的集合 - 1 forEach：对一个集合中的每个元素执行一段特定的代码 - flatMap 和 compactMap 这两个之间有什么区别？ 得分 分值 答案 备注 1 flatMap 用于高维数组的降维，例如二维数组拍平为一维数组 - 1 compactMap 用于过滤数组中的可选值，去掉所有返回 nil 的元素 - 3 Swift 4.2 中将这两个函数拆开，不可混用 提到不可混用即可 进阶 在Swift中如何实现一段代码在整个程序的生命周期内只会执行一次？ 得分 分值 答案 备注 0 dispatch_once 不得分，Swift 3 及以上版本已废弃 0 单例 不得分，题目要求的是 “一段代码”，而不是一个对象 3 (lazy var)/static + Void 类型的属性，定义时使用 {}() 包括需要执行的代码。借助了懒加载的特性 Swift中的类型推断（Type Inference）是什么？请说明它的作用 得分 分值 答案 备注 2 编译器能够推断出变量或常量的类型而不需要显式声明其类型 - 2 使代码更加简洁，减少了类型声明的冗余，同时也减少了因类型声明错误而引起的编译错误的可能性 - -10 不建议使用 如果面试者说建议显式声明类型（除非是为了解决Xcode代码提示的bug），否则扣10分 Swift中的final关键字是什么意思？它的作用是什么？ 得分 分值 答案 备注 1+2+2 final 关键字可以修饰类、方法和变量 提到类计1分；提到方法、变量每个2分，总分5分 1 final 修饰后不可集成、不可重写 - 3 private 的类型，其内的属性和方法会在编译时自动添加 final - Swift中的String类型和ObjC中的NSString类型有什么区别？ 得分 分值 答案 1 底层实现不同，Swift 的 String 类型是一个值类型，而ObjC的 NSString 类型是一个类 1 Swift的 String 类型可以无缝地与ObjC中的 NSString 类型互相转换 2 Swift中的字符串是Unicode兼容的，可以直接使用Unicode字符和表情符号，而ObjC中的字符串是基于ASCII编码的，无法直接处理Unicode字符和表情符号 Swift中的枚举相比较ObjC中的枚举有什么优势？ 得分 分值 答案 2 每个枚举成员可以有自己的关联值，这使得枚举更加灵活。而ObjC中的枚举只能包含整数 3 枚举是一等公民，它和类、结构体等类型一样，可以有自己的方法，甚至遵循协议。 Swift中的延迟（Defer）语句有什么作用？ 得分 分值 答案 备注 0/2/3 Defer语句可以用来在函数/作用域执行完毕之前执行一些清理工作 回答 “函数” 0分，“作用域” 2分，能给出作用域的例子，例如 if 中使用的情况，3分。如果面试者没提到作用域问题，则可以当作进阶问题抛出 1 Defer语句可以用来执行一些清理工作 - 1 无论函数是正常返回还是抛出异常，都会执行 - 1 虽然形式上像是闭包，但更像是语法糖，所以不会捕获变量 - 2 当函数内有多个 defer 语句时，将按照倒序的顺序执行，即最后一个 defer 先执行 如果面试者没回答，则可以当作进阶问题抛出 什么是Swift方法派发？有哪几种？ 方法调度和方法派发是一回事，不同的翻译 得分 分值 答案 备注 1 宏观上分为静态（直接）派发和动态派发两种 - 3 其中动态派发又包含两种：表派发/函数表派发和消息派发 - 5/6 静态派发：编译时找到指令所在的位置。所以执行速度快，还允许编译期做各种优化，例如内联 提到内联，额外加1分，共6分 5/6 表派发：运行时决定实现方式，理论上速度也很快，编译期决定函数表 提到编译期决定寒暑表，额外加1分，共6分 5 消息派发：Objective-C 的逻辑，可以在运行时修改消息接收对象 - 5 会通过看 SIL 确定派发方式 - 请说明Swift中Array类型的底层实现方式 总分值10分 Array 使用了一个名为“缓冲区”（buffer）的数据结构来存储元素。 缓冲区由三个部分组成： 指向元素存储位置的指针 缓冲区的容量大小 缓冲区的元素个数 Swift的switch语法和ObjC的switch语法，在底层实现方式上有什么区别？ 总分值10分 ObjC 的 switch 还是某种意义上的 goto 语法，命中的时候会跳到对应的代码的地址，这段代码是连续的，需要 break 来决定执行到哪里。这种实现方式主要是为了方便从早期 C 语言中继承而来。 而 swift 的 switch 会被编译成类似 if else 的语法，作用域更明确。 SIL是什么？怎么将Swift代码转换为SIL代码？ 得分 分值 答案 备注 1 SIL是Swift中间语言，是源代码和机器代码之间的一种中间代码 - 1/2 编译器会将Swift代码编译成SIL，然后对SIL进行优化和转换，最后将其编译成机器代码 提到编译器对 SIL 进行优化，计2分，否则计1分 3 使用 swiftc -emit-sil + &lt;file_name.swift&gt; 将 swift 文件转换为 SIL 代码 - 请简述 Swift extension 的实现原理 总分10分 在 Swift 内部，每个类或结构体都有一个名为 vtable（虚函数表）的表格，其中记录了类或结构体的方法的地址。当 Swift 调用一个方法时，它会先检查类或结构体的类型，然后查找该类型的 vtable 表格，以获取该方法的地址并调用它。 当我们在 Extension 中定义一个新方法时，Swift 编译器会将其添加到 vtable 表格中，并根据需要重新生成 vtable 表格，以确保所有方法都在正确的位置。这就是 Swift Extension 实现的底层原理。 iOS 开发相关 主要考察 iOS 开发相关知识点，和具体开发语言没有太大的关系。 基础 请简述iOS开发中，APNS的推送机制 总分5分 APNS的推送机制包括以下几个步骤： 应用程序在设备上注册远程通知。注册成功后，设备会产生一个唯一的令牌（device token），用于标识该设备。 应用程序将令牌发送给后台服务器，后台服务器将令牌与设备绑定起来。 当需要向设备发送通知时，后台服务器发送一个推送通知请求到APNS服务器。 APNS服务器将推送通知发送到指定的设备上。 设备接收到推送通知后，根据推送通知的内容执行相应的操作，例如打开应用程序、显示提醒等。 常见的触发离屏渲染的场景有哪些？ 总分10分 进阶 离屏渲染是什么？为什么会触发离屏渲染 总分15分 什么是RunLoop？RunLoop的运行模式有哪几种？ 总分10分 RunLoop是iOS中的一个事件循环机制，负责处理用户事件，如触摸事件、定时器事件、网络请求事件等，同时还要处理UI更新、线程通信等任务。 RunLoop会将事件源分发到相应的处理器进行处理，如果RunLoop没有事件需要处理，那么它会进入休眠状态，直到有新的事件到来。 常见的RunLoop运行模式有以下几种： DefaultRunLoopMode：默认模式，处理UI事件、定时器、网络事件等。 UITrackingRunLoopMode：处理UIScrollView滑动过程中的事件，优先级高于DefaultRunLoopMode。 CommonModes：是一个集合模式，可以添加多个模式，包括DefaultRunLoopMode和UITrackingRunLoopMode等，用于处理需要同时响应多个模式下的事件。 在运行RunLoop时，需要指定一个运行模式，如果没有指定，则会使用DefaultRunLoopMode。 iOS开发中的响应链机制是什么？请举例说明 总分15分 在iOS应用中，将用户的触摸事件（touch event）传递到正确的视图（view）或者控制器（controller）的过程就是响应链。 当用户在屏幕上进行触摸时，触摸事件会被UIWindow捕获，并从UIWindow开始向下传递，直到找到合适的响应者处理事件为止。 当用户触摸到界面上的按钮时，iOS系统会将此事件封装成一个 UIEvent 对象，并通过 UIApplication 对象将该事件发送给当前 UIWindow 对象。接着，该 UIWindow 对象会将该事件发送给最前端的 UIResponder 对象，即当前展示在屏幕上的 UIViewController 的 view 属性对应的 UIView 对象。这一过程即为响应链的开始。 接着，该 UIView 对象会通过 hitTest:withEvent: 方法判断当前触摸点是否在该 UIView 对象内。如果在，则该 UIView 对象会将该事件传递给自己的 touchesBegan:withEvent: 方法进行处理。如果不在，则该 UIView 对象会将该事件传递给其子视图进行处理。这一过程就是响应链的传递过程。 当子视图接收到事件后，会按照相同的方式进行处理。如果该子视图的 userInteractionEnabled 属性为 NO，则该视图将不会响应该事件，并将该事件传递给下一个响应者。如果子视图的 userInteractionEnabled 属性为 YES，则该视图会先进行 hitTest:withEvent: 方法的判断，如果触摸点在该视图内，则该视图将该事件传递给自己的 touchesBegan:withEvent: 方法进行处理，否则将该事件传递给下一个响应者。 最终，如果所有的子视图都没有处理该事件，则该事件会传递给该 UIView 对象的父视图进行处理，一直传递到 UIWindow 对象和 UIApplication 对象。如果该事件都没有被处理，则该事件将被丢弃。 架构相关 主要考察架构、设计能力 基础 什么是MVC架构？它的组成部分是什么？在iOS开发中，如何实现MVC架构？ 总分10分 什么是MVVM架构？它的组成部分是什么？在iOS开发中，如何实现MVVM架构？ 总分15分 什么是装饰器（Decorator）模式？在Swift中如何使用？ 得分 分值 答案 1 装饰器模式允许开发者在不修改对象结构的情况下，动态地添加行为。 3 在 Swift 中，装饰器模式可以通过使用协议和扩展来实现。 5 具体来说，可以定义一个协议来表示被装饰者的基本行为，然后创建一个装饰器协议，该协议继承自被装饰者协议，并添加了额外的行为。最后，可以使用一个类来实现装饰器协议，该类通过存储被装饰者对象的引用来实现对被装饰者对象的包装。 进阶 什么是尾递归优化（Tail-Call Optimization）？在Swift中如何实现尾递归优化？ 得分 分值 答案 2 尾递归优化是一种优化技术，它可以避免在递归算法中出现栈溢出的问题 3 在递归函数的最后一个操作是调用自身，并且没有其他操作时，尾递归优化可以将递归转换为循环，避免了栈的增长 什么是函数柯里化（Function Currying）？在Swift中如何实现函数柯里化？ 得分 分值 答案 2 函数柯里化是一种将多个参数的函数转换为一系列只有单个参数的函数的技术 3 在swift中，可以通过将函数返回值声明为闭包类型，将原本需要的参数放到返回值的闭包中，来实现函数柯里化 设计 在你的项目中，你是如何划分模块（Module）和层级（Layer）的？请举例说明。 总分10分 开放式题目。 不用 target-action 的方式如何设计一个路由组件？ 总分20分 开放式题目。 当你从ObjC转到Swift之后，你最大的感触是什么？ 总分20分 开放式题目。 当你从ObjC转到Swift之后，你做了哪些事让你的Swift代码看上去不那么ObjC？ 总分30分 开放式题目。下面提供几个示例： 使用Swift的数据结构和类型：使用Swift的Array、Dictionary等数据结构，避免使用NSArray和NSDictionary等。 使用Swift的可选型（Optional）：使用可选型来处理变量是否存在的情况，而不是定义0，空字符串，或者使用ObjC中的nil或NSNotFound等。 使用Swift的语言特性：使用Swift中的高阶函数和闭包等语言特性，避免使用ObjC中的循环和函数指针等。 减少使用全局变量：尽量避免使用全局变量，而采用更具体和局部的方式来传递数据。 减少使用强制类型转换：尽量避免使用强制类型转换，而采用更安全和优雅的方式来处理类型转换。 ","link":"https://blog.rakuyoo.top/ios-self-paced-interview-questions/"},{"title":"Python：Celery 的简单使用","content":"最近写的后端项目需要用到队列，其中有一些东西折腾了好一会儿，在这里记录一下。 Celery 是一个开源的分布式任务队列，可实现异步任务调度、定时任务、任务结果处理等功能。 本文基于 Celery 5.2.7 版本编写。 概览 Celery 其实包含了几个概念：Celery 客户端、消息中间件 Broker、结果存储 Backend 以及 任务执行单元 Worker。 大致的流程是：Celer 客户端发起任务 -&gt; 相关信息存储到消息中间件 Broker 中 -&gt; 任务执行单元 Worker 从 Broker 中读取任务（Task）然后执行 -&gt; 将执行结果存储到 Backend 中 -&gt; Celery 客户端从 Backend 中读取结果。 Celery 支持多种消息中间件和结果存储，本文主要使用 Redis 作为中间件和结果存储。 Celery 对象的初始化 &amp; 配置 在 Celery 类用 __init__ 方法初始化之后，有两种配置方式： __init__ 可以在初始化 Celery 对象的同时传入对应的配置。有以下几个比较常用的参数： main：主模块的名称 broker：消息中间件存储的访问地址 backend：结果存储服务的访问地址（可选的） 例如说一个可能的 Celery 对象的初始化代码为： from celery import Celery celery = Celery('flaskr', broker='redis://localhost:6379/0', backend='redis://localhost:6379/0') 可以使用同一个 Redis 服务同时作为中间件和结果存储。 flaskr 是包含 Celery 客户端的 Python 程序的名称，如果使用 Flask 框架开发，那么应当是和初始化 Flask 对象时使用的名称一致。 上文有提到 backend 是可选的，如果不传该参数，那么就无法通过 Celery 查询到任务的执行结果。 config_from_object Celery 类还提供了一个 config_from_object 方法，支持通过 “Python 配置文件” 进行配置。使用上代码类似下面这样： from celery import Celery celery = Celery('flaskr') celery.config_from_object('flaskr.tools.celery_config') celery_config 是文件名。flaskr.tools.celery_config 是这个文件的 “路径”，或者说当我们想要在代码中 import 该 python 文件时使用的路径。 celery_config.py 文件中的内容为： from os import environ CELERY_REDIS_URL = &quot;redis://&quot; + \\ &quot;:&quot; + \\ environ.get(&quot;REDIS_PASSWD&quot;) + \\ &quot;@&quot; + \\ environ.get(&quot;REDIS_HOST&quot;) + \\ &quot;:&quot; + \\ environ.get(&quot;REDIS_PORT&quot;) + \\ &quot;/&quot; + \\ environ.get(&quot;REDIS_DATABASE&quot;) BROKER_URL = CELERY_REDIS_URL CELERY_RESULT_BACKEND = CELERY_REDIS_URL 在该文件中，我们可以从环境变量中读取配置，或者直接写死了我们所需要的配置字段。 注意 Celery 对象的初始化以及配置必须要放在顶层，下面的代码是不行的： from celery import Celery celery = Celery('flaskr') def init_app(app): celery.config_from_object('flaskr.tools.celery_config') 因为后续 Worker 将会根据配置的路径，寻找 Celery 对象，从该对象中获取相应的配置。 该过程可以理解为：“不是获取一个已经初始化好了的对象，而是根据配置的路径，找到对应的文件/模块，重新初始化 Celery 对象”。 该过程只会执行顶层属性、方法，也就是说它不会调用你的 init_app 方法，所以这么写之后 Worker 侧的 Celery 对象属于只是被初始化但是没有被配置过的状态，所有的配置项都会是默认值。 Worker Worker 服务是独立于 Celery 客户端程序的，需要单独另起一个服务。 在终端执行下面的命令启动 Worker 服务： celery -A flaskr.libs.system worker -l info 其中 -A 参数后面接着的 flaskr.libs.system 是 Celery 对象所在的模块/文件路径。和 config_from_object 一样，这里也是 “当我们想要在代码中 import 该 python 文件时使用的路径”。要确保该路径下包含着 Celery 对象。 -l 参数是日志等级，示例中指定到 info 级别。 Celery 的使用 下面简单的介绍一下 Celery 框架的使用方法。 上文中有提到，Worker 会在运行时读取 Celery 客户端中的 Task 任务并执行。 定义任务 下面是一个简单的 Task 的定义： # in tasks.py from flaskr.libs.system import celery @celery.task def add(x, y): return x + y 你可以将其定义为顶层/全局方法，也可以将其包裹在类中。 如果包裹在类中，会涉及到 self 参数问题，请参考后面的小节：任务中的 self 上文我们在 flaskr.libs.system 模块下定义了 celery 对象，然后就可以在 tasks.py 文件中使用 @celery.task 装饰器，使一个方法成为 Celery 任务。 注意：如果你的 Celery 对象不叫 celery 而是其他名字，例如 app。那么你的装饰器应该是 @app.task —— task 是 Celery 对象的方法。 在 Task 任务内，return 结果代表着 “任务执行成功”，通过 raise 抛出异常意味着 “任务执行失败。” 执行任务 对任务方法调用 delay 方法以传递参数并执行任务： # 使用参数 4，5 调用任务 add result = add.delay(4, 5) # 对执行结果调用 get() 方法以获取任务执行结果 print(result.get()) get() 方法获取的是任务执行成功之后的结果，即 return 的结果。 如果要获取 raise 抛出的异常，我们可以使用 result.result 方法获取异常。 判断状态 在获取任务的执行结果之前，我建议您先判断当前任务的执行状态。 Celery 框架提供了 AsyncResult 类来查询任务执行状态： def exec_task(self): &quot;&quot;&quot;执行任务，并返回任务 ID&quot;&quot;&quot; result = add.delay(4, 5) task_id = result.task_id return task_id def check_status(self, task_id): &quot;&quot;&quot;根据任务 ID 查询任务状态&quot;&quot;&quot; from flaskr.libs.system import celery result = celery.AsyncResult(task_id) 有一些文章会执行使用 AsyncResult 类，而不是通过 celery 对象调用 AsyncResult。 但是在我实际操作过程中发现，不通过 celery 对象调用的话会找不到 Backend，怀疑也是和配置有关。 result 对象有一个 task_id 属性，使用该属性值作为参数初始化 AsyncResult 对象，就可以查询该任务的状态： result.state：获取当前任务的状态（字符串） result.ready()：判断任务是否已经结束（成功 or 失败） result.successful()：判读任务是否成功 result.get()：和上文提及的一样，获取执行成功的任务的结果（return） result.result：和上文提及的一样，获取执行失败的任务抛出的异常（raise） 一种有可能的处理流程如下： def check_status(self, task_id): # 查询任务状态 from flaskr.libs.system import celery result = celery.AsyncResult(task_id) logging.info(f&quot;查询任务结果，任务ID（{task_id}）对应的状态为：{result.state}&quot;) # 任务仍在执行中 if not result.ready(): return { &quot;code&quot;: &quot;105&quot;, &quot;message&quot;: &quot;任务仍在执行中&quot; } # 任务失败 if not result.successful(): error = result.result return { &quot;code&quot;: &quot;9999&quot;, &quot;message&quot;: f&quot;{str(error)}&quot; } # 获取任务执行结果 task_result = result.get() return { &quot;code&quot;: &quot;200&quot;, &quot;data&quot;: task_result, &quot;message&quot;: &quot;任务执行成功&quot; } 任务中的 self .task() 方法有一个 bind 参数，它的默认值是 False。该参数可以控制是否将任务绑定到实例对象上。这关乎到 self 参数的具体值。 定义在类中的任务 如果你的任务定义是包裹在类中，像是这样： from flaskr.libs.system import celery class Tasks(object): def __init__(self, value): self.value = value @celery.task def add(self, x): return x + self.value 那么此时需要注意，因为 bind 参数的默认值是 False，所以此时 add 任务没有被绑定到 Tasks 类实例上，所以此时 self 参数指的是任务本身，而不是 Tasks 类的实例对象。此时在 add 方法中，你将无法获取到 self.value 的值。 如果你想在 add 方法中使用 self.value，你需要将 bind 参数设置为 True，像是这样： ... @celery.task(bind=True) def add(self, x): return x + self.value ... 此时你可以顺利的获取到 self.value 的值。 定义在顶层的任务 如果任务直接定义在文件中，类似下面这样： from flaskr.libs.system import celery @celery.task def add(x, y): return x + y 那么首先，此时 bind 参数的值是 False。这个时候您无法在 add 方法的参数列表中添加 self，也无法在方法实现内使用 self 参数，这会导致编译失败。 如果你将 bind 设置为 True，那么就可以添加 self 参数了，例如下面这样： from flaskr.libs.system import celery @celery.task(bind=True) def add(self, x, y): return x + y 那么此时 self 代表什么呢？ 上文有提到，bind 参数用于将任务绑定到类实例。 实际上，此时 Celery 会在后台自动将该任务转换为一个带有实例方法的类，使你可以在任务函数中访问实例属性和方法。在这种情况下，self 参数表示当前任务的实例对象，就像在类中定义任务一样。 重试 Celery 框架本身支持 “重试” 功能，包含自动重试和手动重试两种方法。这里简单介绍一下手动重试的用法。 retry 方法的调用 # in tasks.py from flaskr.libs.system import celery @celery.task(bind=True) def add(self, x, y): try: # 某些可能会抛出异常的方法 return __some_method_may_throw_exceptions() except Exception as e: # 出错每15秒尝试一次，总共尝试3次 raise self.retry(exc=e, countdown=15, max_retries=3) finally: pass 该例子中，我们在文件内直接定义了一个任务，而且 bind 参数被设置为了 True。此时我们要通过 self 参数来调用 retry 方法，执行手动重试。 那如果 bind 是 False 的情况呢？此时我们可以使用 add.retry(...) 来触发手动重试。 如果任务定义在类中： 如果 bind 是 False，则可以直接使用 self.retry(...) 触发手动重试。 如果 bind 是 True，则需要使用 add.retry(...) 触发手动重试。 总结一下不同的场景： 任务定义在类中： 如果 bind 是 False，则可以直接使用 self.retry(...) 触发手动重试。 如果 bind 是 True，则需要使用 add.retry(...) 触发手动重试。 任务定义在文件中： 如果 bind 是 False，则可以直接使用 add.retry(...) 触发手动重试。 如果 bind 是 True，则需要使用 self.retry(...) 触发手动重试。 可以说定义在类中和定义在文件中，对于相同的 bind 参数，调用方式是相反的。 retry 方法的参数 说完了调用，我们来说一下参数。 countdown 定义重试的间隔，max_retries 定义最多重试几次。 exc 参数是一个可选参数，它代表着 “当前任务失败时产生的异常”。通过将 e 参数传递到 retry 方法中，可以让重试失败后抛出最开始的异常。您也可以自定义该异常参数。 在使用 retry 方法时，如果没有指定 exc 参数，Celery 框架将默认将任务的异常信息传递给 retry 方法。如果指定了 exc 参数，则会将指定的异常对象传递给 retry 方法。 retry 方法的结果 在上面的例子中，我们使用了 raise 将 retry 失败后的异常抛出，意味着任务失败。 同时我们还可以不使用该参数，在重试失败之后手动处理异常，例如： # in tasks.py from flaskr.libs.system import celery @celery.task(bind=True) def add(self, x, y): try: return __some_method_may_throw_exceptions() except Exception as e: max_retries = 3 # 超过最大次数，返回错误信息 if self.request.retries &gt;= max_retries: return __create_error_response() self.retry(exc=e, countdown=15, max_retries=max_retries) finally: pass 在这个例子中，如果超过了最大重试次数，因为我们没有将重试的异常抛给外界，所以在最后一次重试失败之后会再次触发 except，此时我们可以判断是否超过了最大重试次数，意味着重试失败。 当重试失败后，我们可以根据需求进行失败处理。 ","link":"https://blog.rakuyoo.top/simple-use-of-python-celery/"},{"title":"可组合的 UICollectionView 布局：UICollectionViewCompositionalLayout","content":"UICollectionViewCompositionalLayout 是 Apple 在 iOS 13 引入的，用于构建基于组合的 UICollectionView 布局的类。它允许开发者根据一系列的组合布局，更轻松、更灵活地创建复杂的 UICollectionView 布局。 本文主要围绕 UICollectionViewCompositionalLayout 中 “可组合” 这个特性进行讨论。 本文建立在您已经大致了解过 UICollectionViewCompositionalLayout，知道它是什么，以及基础的用法。下文将不再对其进行讲解。 组合类 FlowLayout 布局 在上一篇文章 NSCollectionLayoutGroup 之子视图的填充 的最后有提到一个场景，这里复用一下这个场景： 在前文中，我有说中间的 “工具Group” 不能使用 subitems 的形式进行初始化，应该使用 subitem + count 的形式初始化 —— 其实这不完全对。 实际需求上，如图这种类 FlowLayout 的布局往往会要求 “每行X个”，即平分展示。 此时如果你们需要考虑下面的场景： 屏幕旋转。 iPad 等屏幕尺寸会发生变化的情况。 AutoLayout 侧不想要获取屏幕尺寸去计算出 cell 宽度。 那么 subitem + count 就没法做了：subitem + count 会忽略布局方向上的尺寸，所以此时很难单纯用 AutoLayout 做出平分的效果，或者说为了实现平分，需要多次刷新页面布局。 此时有一种用 subitems 实现的方式：以每行/列作为一个 Group，Group 内使用 subitems。大致的代码如下所示： let itemSize = NSCollectionLayoutSize( widthDimension: .fractionalWidth(1/4), heightDimension: .estimated(69) ) let normalLayoutSize = NSCollectionLayoutSize( widthDimension: .fractionalWidth(1), heightDimension: .estimated(500) ) let toolsListGroup = NSCollectionLayoutGroup.horizontal( layoutSize: normalLayoutSize, subitems: [ .init(layoutSize: itemSize) ] ) let rows = 5 let toolsGroup = NSCollectionLayoutGroup.vertical( layoutSize: normalLayoutSize, subitems: [ [toolsTitleItem], Array(repeating: toolsListGroup, count: rows), [ toolsFooterItem, sectionPaddingItem ], ].flatMap { $0 } ) 上面的代码首先定义了2个 Size，一个是 Item 的尺寸，另外一个算是一个 “占位符”。 然后是2个 Group： toolsListGroup 以行为单位展示 cell。 toolsGroup 则是最外面的 Group，它的 subitems 包含多个不同的 Group。其中的 Array(repeating: toolsListGroup, count: $0) 则是根据行数生成对应数量的 Group。 这种写法有一个注意点还有一个疑惑： 注意点是：当 cell 的数量不是行的整数倍的时候，需要手动创建占位 cell。 举例来说，cell 有 5 个，对应的就是2行，cell 分布是 4 + 1。此时因为后面还有其他等待被填充的 Item/Group，如果没有创建占位 cell，则后面的 cell 会被提前，使用 toolsListGroup 里的 Item 布局。 疑惑的是：我并不知道为什么这么做能生效。 按理说 subitems 的布局方式是不会限制内容的数量的，那么为什么它能每行正好装4个（就这个例子来说，每个Cell的宽度是屏幕的1/4），下一行的 Cell 则使用下一个 Group 的布局？ 有关于 UICollectionViewCompositionalLayout 组合复杂布局的文档还是太少，目前还没找到这么做能生效的原因。 ","link":"https://blog.rakuyoo.top/composable-uicollectionview-layout-uicollectionviewcompositionallayout/"},{"title":"NSCollectionLayoutGroup 之子视图的填充","content":"本篇主要讲解 NSCollectionLayoutGroup 内 “子视图” 的填充方式。 算是为 UICollectionViewCompositionalLayout 的讲解做一些铺垫，毕竟把 Item 的填充方式了解清楚了之后，才能组合多种 Group 来进行更复杂的布局。 NSCollectionLayoutGroup 不考虑方向的话，有两种填充子视图的方法。 概览 通过头文件您能查到如下几个初始化方法： open class func vertical(layoutSize: NSCollectionLayoutSize, subitems: [NSCollectionLayoutItem]) -&gt; Self @available(iOS, introduced: 13.0, deprecated: 16.0) open class func vertical(layoutSize: NSCollectionLayoutSize, subitem: NSCollectionLayoutItem, count: Int) -&gt; Self @available(iOS 16.0, *) open class func vertical(layoutSize: NSCollectionLayoutSize, repeatingSubitem subitem: NSCollectionLayoutItem, count: Int) -&gt; Self @available(iOS, introduced: 16.0, deprecated: 16.0, renamed: &quot;vertical(layoutSize:repeatingSubitem:count:)&quot;) public class func verticalGroup(with size: NSCollectionLayoutSize, repeatingSubitem subitem: NSCollectionLayoutItem, count: Int) -&gt; NSCollectionLayoutGroup 第三个方法是 iOS 16 新出的，同时废弃了第二个方法，而第四个方法在 iOS 16 引入同时又在 iOS 16 废除。 后文将围绕前两个方法展开讨论，不涉及到后两个方法。 subitem + count 这种初始化方式将在每个 Group 中重复 count 个 item。使用这种方法进行初始化时，在不同的方向上，layoutSize 会有不同的表现： vertical func vertical(layoutSize: NSCollectionLayoutSize, subitem: NSCollectionLayoutItem, count: Int) -&gt; NSCollectionLayoutGroup 在纵向上，NSCollectionLayoutItem 的 widthDimension 生效，但是heightDimension 将被忽略。 Item 的实际高度会是 .fractionalHeight(1/count)。 假设现在有下面这些代码： let itemSize = NSCollectionLayoutSize(widthDimension: .fractionalWidth(1/3), heightDimension: .absolute(100)) let item = NSCollectionLayoutItem(layoutSize: itemSize) let groupSize = NSCollectionLayoutSize(widthDimension: .fractionalWidth(1), heightDimension: .absolute(100)) let group = NSCollectionLayoutGroup.vertical(layoutSize: groupSize, subitem: item, count: 2) 此时 Item 的实际高度会是 50，相当于一个高度为 100 的 Group 内有2个高度为 50 的 Item。 horizontal func horizontal(layoutSize: NSCollectionLayoutSize, subitem: NSCollectionLayoutItem, count: Int) -&gt; NSCollectionLayoutGroup 和 vertical 时的情况相对应： 在横向上，NSCollectionLayoutItem 的 heightDimension 生效，但是widthDimension 将被忽略。 Item 的实际宽度会是 .fractionalWidth(1/count)。 这里就不再用代码举例说明了。 subitems 和上一种初始化方法相比，使用 subitems 的方式来进行初始化要简单的多，因为它内部的 Item 的大小完全取决于 Item 自身。考虑如下代码： let itemSize = NSCollectionLayoutSize(widthDimension: .fractionalWidth(1/3), heightDimension: .absolute(100)) let item = NSCollectionLayoutItem(layoutSize: itemSize) let groupSize = NSCollectionLayoutSize(widthDimension: .fractionalWidth(1), heightDimension: .absolute(300)) let group = NSCollectionLayoutGroup.vertical(layoutSize: groupSize, subitems: [item]) 该示例中，每个 Item 的高度就是 100，而宽度是 Group 的 1/3。每个 Group 中有多少个 Item 取决于数据源传递给 UICollectionView 的数量。 有一点需要注意的是，Group 只是用来辅助布局的，它本身并不会生成任何视图。所以尽管在本示例中它的高度只有 300，不足 UICollectionView 的高度，但是 Item 依然会充满整个 UICollectionView，而不是仅仅在高度为 300 的范围内进行滑动。 我并不能肯定这种现象可以称之为 “忽略 Group 的 layoutSize”，所以没有使用这种表述。 总结 这两种初始化方式各有各的用途： 在不考虑嵌套布局（Group 中嵌套 Item 和 Group）的情况下，这两种布局的使用场景大体上一致，只需要注意各自对 Item 大小的计算规则即可。绝大多数情况下我自己会选择 subitems 的初始化方式，因为 Item 的大小计算规则更为直观。 但是在嵌套布局的前提下，这两种初始化方式就会有比较大的差异。 首先嵌套布局的最外层只能使用 subitems 的方式初始化，数组中包含多种不同的 Item 或者 Group。 其次如果需要指定 Item 的数量，那么就只能使用 subitem + count 的初始化形式。 考虑下图的场景： 如果现在要求顶部的标题、中间的工具和下方的灰色分隔条，三部分放到一个 NSCollectionLayoutGroup 对象中，可以使用 [标题Item + 工具Group + 灰条Item] 的组合布局形式。 此时 “工具Group” 就不能使用 subitems 的方式进行初始化了，必须要指定工具的数量，否则 UICollectionView 会把下方的灰条也融到工具 Group 中。 ","link":"https://blog.rakuyoo.top/notes-of-nscollectionlayoutgroup/"},{"title":"NSCollectionLayoutSection 注意事项之 orthogonalScrollingBehavior","content":"本篇文章记录使用 NSCollectionLayoutSection 类的 orthogonalScrollingBehavior 属性的过程中遇到的问题。 通过设置该属性可以控制对应 Section 的滑动效果。 在阅读以下内容时，我将默认您已经掌握了 UICollectionViewCompositionalLayout 的基础用法，不再对一些细节进行补充说明。 Orthogonal Scroll View 这一节我们会涉及到两个系统的私有类型：_UICollectionViewOrthogonalScrollView 和 _UICollectionViewOrthogonalScrollerEmbeddedScrollView 因为这两个类型的名称太长，同时会多次重复提及，故下文使用 _UIOrthogonalScrollView 代替。 这两个类型可以看作是一个，早期 Apple 使用的是 _UICollectionViewOrthogonalScrollerEmbeddedScrollView 这个名称，后期在某个版本中改为 _UICollectionViewOrthogonalScrollView。 orthogonalScrollingBehavior 的默认值是 .none，当我们将其设置为其他值后，系统就会在 UICollectionView 上添加一层类型为 _UIOrthogonalScrollView 的 UIScrollView 子类作为 Cell 的父视图，如下图所示： 此时 Cell 的父视图不再是 UICollectionVIew。 我们测试过以下情况： UICollectionViewCompositionalLayout 只有一种布局。 NSCollectionLayoutGroup 的滑动方向和 UICollectionView 的滑动方向一致或不一致。 NSCollectionLayoutSection 的尺寸和 UICollectionView 相等或不相等。 在这三种情况下，只要修改了 orthogonalScrollingBehavior 属性，就会添加 _UIOrthogonalScrollView 视图。 在正式展开说明之前插一句：本文后面所提及的内容，都是在下面这个布局的基础上进行修改的： let itemSize = NSCollectionLayoutSize( widthDimension: .fractionalWidth(1/3), heightDimension: .absolute(50)) let item = NSCollectionLayoutItem(layoutSize: itemSize) let group = NSCollectionLayoutGroup.horizontal( layoutSize: .init( widthDimension: .fractionalWidth(1), heightDimension: .fractionalHeight(0.5)), subitems: [item]) let section = NSCollectionLayoutSection(group: group) return section 获取时机 那么该如何获取该视图呢，经过不完全测试，列表不包含任何 Cell 时，不会添加该视图。 该视图可在 viewDidLayoutSubviews 方法中通过遍历子视图获得。 /// `orthogonalScrollView` 可能的类型 /// /// 目前尚无法确定哪个版本的系统使用了哪个类型，所以为了稳妥起见，使用数组进行判断 private var orthogonalScrollViewTypes: [String] { [ &quot;_UICollectionViewOrthogonalScrollView&quot;, &quot;_UICollectionViewOrthogonalScrollerEmbeddedScrollView&quot; ] } var orthogonalScrollView: UIScrollView? { subviews.first { orthogonalScrollViewTypes.contains(&quot;\\(type(of: $0))&quot;) } as? UIScrollView } clipsToBounds _UIOrthogonalScrollView 的 clipsToBounds 属性默认是 false，这会导致一个问题： 如果您的 Section 设置了 contentInsets 之类的属性，导致比 UICollectionView 小，那么在滑动 _UIOrthogonalScrollView 的时候，其内容会超出 Section 的范围。 iOS 15 及以上 _UIOrthogonalScrollView 是严格跟着 Section 范围走的，例如下图： 蓝色选中的范围是 _UIOrthogonalScrollView，可以看到滑动的时候 Cell 明显超出了它的范围。 上文有提到获取 _UIOrthogonalScrollView 的时机，您可以在获取后通过手动将 clipsToBounds 设置为 ture 来解决该问题。 iOS 15 以下 经不完全测试，iOS 14.7.1 及以下版本符合该小节内容。iOS 14.7.1 - iOS 15 之间的版本没有经过测试，不确定符合上一小节还是该小节。 _UIOrthogonalScrollView 的范围是跟着 UICollectionView 走的，由下图所示： 和上一张图对比之后，可以很明显的发现区别（代码相同）。值得一提的是此时 _UIOrthogonalScrollView 的 contentInset 是 .zero 此时设置 clipsToBounds 就没有用了。 跟随滑动 如果您准备用 UICollectionViewCompositionalLayout 实现类似如上图的效果，那么您需要确保滑块是添加到 _UIOrthogonalScrollView 上。 考虑到该视图的获取时机，我建议您还是改用 UICollectionViewFlowLayout 实现。 .continuous 原本以为配合 UICollectionLayoutSectionOrthogonalScrollingBehavior.continuous，最终可以达到和 UICollectionViewCompositionalLayout 一样的效果，但是事实是并不会。 本文使用的例子里包含一个横滑的 group，item 的宽度是 group 的 1/3，group 的宽度和 UICollectionView 宽度一致。但是如果把 item 的宽度改为某个固定的值，例如 50，那么就会发现：屏幕上只会显示完整的 Cell。如下图所示： 右侧剩余的宽度不够第四个 Cell 显示出来，那么它就会隐藏掉，在后面再显示： 如果不修改 orthogonalScrollingBehavior（即值为 .none），则不会有该问题，但是 Cell 在超出屏幕后会换行展示，因为没有 _UIOrthogonalScrollView ... 所以建议这种需求还是使用 UICollectionViewFlowLayout 组合实现。 或者您知道什么更好的方式，请在评论区留言告诉我，谢谢。 ","link":"https://blog.rakuyoo.top/note-of-orthogonalscrollingbehavior/"},{"title":"fastlane match error: 503","content":"使用 match 命令更新开发证书的时候，遇到了 503 Service Temporarily Unavailable 的报错，搜到解决方案后想着还是记录一下吧。 本文转载自 Fastlane 503 Service Temporarily Unavailable 究其原因是因为 match 命令调用 Apple 服务太频繁/量过大，导致 503 服务暂时不可用。 此时可以使用下面的命令，清除 fastlane spaceship 的 cookie 来达到重制的目的： rm $HOME/.fastlane/spaceship/*/cookie 之后重新调用 fastlane match 命令即可。 ","link":"https://blog.rakuyoo.top/fastlane-match-error-503/"},{"title":"VPN 配置在 iOS 和 macOS 下的区别","content":"最近尝试使用 Mac Catalyst 将 App 移植到 macOS 上，打包时遇到了该问题，故记录一下。 在 .entitlements 文件中，需要使用 key com.apple.developer.networking.networkextension 来标记需要 VPN 权限。 但是这个 key 的值在 iOS 和 macOS 上是不同的： iOS：packet-tunnel-provider macOS：packet-tunnel-provider-systemextension 区别就在于 macOS 下多了 -systemextension 这部分，所以如果 App 提供 VPN 功能，那么 Mac Catalyst 和非 Mac Catalyst 的 Settings 就不能使用同一份 .entitlements 文件了。 ","link":"https://blog.rakuyoo.top/difference-between-vpn-entitlement-config-in-ios-and-macos/"},{"title":"Combine 操作符速查表","content":"文章以图文的形式，记录 Combine 中各操作符的用法以及效果。下文为原文章内容的转载 + 翻译。 本文转载自 Cheat sheet on s for iOS development 点击查看原文以更好的效果查看内容。 allSatisfy(_:) Publishes true if all elements match the condition. Publishes false &amp; completes if it receives an element that does not match the condition. 如果所有的元素都符合条件，则返回 true。如果收到一个不符合条件的元素，则返回 false 并完成信号。 append(_:) Appends the specified elements after the upstream publisher completes. 在上游发布者完成后添加指定的元素。 collect(_:) Collects up to the specified number of elements and emits them in an array. 收集多达指定数量的元素，并将它们发射到一个数组中。 combineLatest(_:) Publishes a tuple of the most recent element from multiple publishers when any of them emits a value. 当任何一个发布者发出一个值时，发布一个来自多个发布者的最新元素的元组。 compactMap(_:) Transforms each element with the provided closure and publishes any returned non-nil optional result. 用提供的闭包对每个元素进行转换，并公布任何返回的非零的可选结果。 contains(_:) Publishes true &amp; completes after receiving a matching value, or false when upstream publisher completes without producing a matching value. 在收到一个匹配的值后发布真值&amp;完成，或者在上游发布者完成后没有产生一个匹配的值时发布假值。 count() Publishes the count of all received elements when the upstream publisher completes. 当上游发布器完成时，发布所有收到的元素的计数。 dropFirst(_:) Omits a specified number of elements, then republishes all remaining elements. 省略指定数量的元素，然后重新发布所有剩余的元素。 drop(while:) Omits elements until a given closure returns false, then republishes all remaining elements. 省略元素，直到一个给定的闭包返回错误，然后重新发布所有剩余的元素。 filter(_:) Republishes all elements that match a provided closure. 重新发布所有符合所提供的闭包的元素。 first() Publishes the first element of the upstream publisher, then completes. 发布上游发布器的第一个元素，然后完成。 first(where:) Publishes the first element of a stream that satisfies a condition, then completes. 发布一个满足条件的流的第一个元素，然后完成。 flatMap(maxPublishers:_:) Transforms all elements from an upstream publisher into a new publisher. 将上游发布器中的所有元素转化为一个新的发布器。 ignoreOutput() Ignores all elements and passes along only the upstream publisher's completion state. 忽略所有元素，只传递上游发布者的完成状态。 last() Publishes the last element of the upstream publisher after it completes. 在上游发布器完成后，发布它的最后一个元素。 last(where:) Publishes the last element that satisfies a condition after the upstream publisher completes. 在上游发布器完成后，发布满足条件的最后一个元素。 map(_:) Transforms each element it receives from the upstream publisher with the provided closure. 用所提供的闭包对它从上游发布者那里收到的每个元素进行转换。 max() Publishes the maximum value received after the upstream publisher completes. 在上游发布者完成后，发布收到的最大值。 merge(with:) Publishes an element whenever any of the upstream publishers emits an element. 每当任何一个上游发布者发出一个元素，就发布一个元素。 min() Publishes the minimum value received after the upstream publisher completes. 发布上游发布者完成后收到的最小值。 output(at:) Republishes an element specified by its position, or completes if upstream publisher completes before the specified element. 重新发布一个由其位置指定的元素，如果上游发布者在指定的元素之前完成，则完成。 output(in:) Republishes elements specified by a range of positions. 重新发布由某个位置范围指定的元素。 prefix(_:) Republishes elements up to a specified maximum count. 重新发布元素，直到指定的最大数量。 prefix(while:) Emits values while elements meet the specified condition, otherwise it completes. 当元素满足指定的条件时发出数值，否则就会完成。 prepend(_:) Prepends elements with the specified values. 用指定的值对元素进行预置。 reduce(_:_:) Applies a closure to all elements and publishes a final result when the upstream publisher completes. 对所有元素应用一个闭包，并在上游发布器完成后发布一个最终结果。 removeDuplicates() Publishes an element only if it doesn't match the previous element. 只在一个元素与前一个元素不匹配时才发布。 replaceNil(with:) Replaces nil elements with the provided element. 用所提供的元素替换nil元素。 scan(_:_:) Transforms elements by applying a closure that receives its previous return value and the next element from the upstream publisher. 通过应用一个闭包来转换元素，该闭包从上游发布者那里接收其先前的返回值和下一个元素。 switchToLatest() Republishes elements sent by the most recently received publisher. 重新发布由最近收到的发布者发送的元素。 zip(_:) Waits until both publishers emit an element, then publishes the oldest unconsumed element from each one as a tuple. 等待，直到两个发布者都发出一个元素，然后将每个发布者中最古老的未被吸收的元素作为一个元组发布。 ","link":"https://blog.rakuyoo.top/cheatsheet-combine-operators/"},{"title":"GitLab-CI（三）：配置 GitLab-Runner","content":" 本文针对 GitLab-Runner 的配置进行讲解说明。 GitLab-CI 系列文章目前有三篇，主要讲解 GitLab-CI 的环境搭建与配置。本文为该系列的第三篇。 目录： GitLab-CI（一）：搭建 iOS CI 环境 GitLab-CI（二）：注册 GitLab-Runner GitLab-CI（三）：配置 GitLab-Runner config.tom 文件配置 在终端内输入 gitlab-runner list 命令，获取到 config.toml 文件路径，后续将针对该文件进行讲解说明。 concurrent 该文件顶部的 concurrent 字段意味着本台机器最多可以同时运行的作业数量。 请根据需求与 CI 机器的性能修改该字段。 例如 concurrent = 2 意味着 “该台电脑同一时间只能运行2个作业”。考虑到实际情况（一个作业对应一个打包任务），即同一时间内只能同时打2个包。 建议8g内存的机器将该值设置为2，以上内存的机器可将该值设置为4。 runners name 即注册 Runner 时填写的 description，可通过修改该值达到修改 Runner 名称的目的。 在下文介绍的描述中，也可修改。 在 GitLab - CI/CD 界面配置 找到群组/项目左侧导航栏最下方的设置按钮，依次选择 “设置（Settings） -&gt; CI/CD -&gt; Runners”，进入 Runner 设置页面。 找到需要编辑/配置的 Runner，点击右侧铅笔形状的 “编辑按钮”，进入编辑页面。 启用 通过勾选或取消勾选该选项，可开启/关闭该 Runner，被关闭的 Runner 将无法接收新的任务。 描述 即注册 Runner 时填写的 description，可通过修改该值达到修改 Runner 名称的目的。 最大作业超时 可设置任务超时时间。 超时时间有3种： Runner 处配置的超时时间，即此处配置的超时时间。优先级最高。 Git 项目的超时时间，可在 “设置（Settings） -&gt; CI/CD -&gt; 流水线通用设置” 处进行设置。优先级最低。 任务的超时时间，在具体的 .yml 文件中设置。优先级居中。 例如： Runner 配置了 2小时 超时 该 git 项目配置了 1小时 超时 所要执行的任务配置了 1.5小时 超时 则使用该 Runner 执行的任务的最终超时时间为 1.5 小时。 标签 可配置该 Runner 可接收的任务的标签。一个 Runner 可以设置多个标签，之间用 , 号进行分割。 只有当 Runner 的标签列表包含将要执行的任务的标签时，才会选择该 Runner 去执行这个任务。否则即使该 Runer 处于空闲状态，也不会被使用。 ","link":"https://blog.rakuyoo.top/gitlab-ci-config-runner/"},{"title":"GitLab-CI（二）：注册 GitLab-Runner","content":" 本文仅针对 macOS 和 Linux 平台，讲解如何注册 GitLab-Runner。Windows 平台不包含在本文介绍范围内。 GitLab-CI 系列文章目前有三篇，主要讲解 GitLab-CI 的环境搭建与配置。本文为该系列的第二篇。 目录： GitLab-CI（一）：搭建 iOS CI 环境 GitLab-CI（二）：注册 GitLab-Runner GitLab-CI（三）：配置 GitLab-Runner Runner 的分类 Runner 按照适用范围分为以下三类： 项目 Runner：仅某个项目可以使用该 Runner，其他项目不可使用。 群组 Runner：某个群组下的项目都可以使用该 Runner，其他群组下的项目不可使用。 共享 Runner：整个 GitLab 系统中的所有项目都可以使用该 Runner。 注册 Runner 前期准备 打开并登录 GitLab。 思考你需要注册哪种 Runner。 如果你需要注册第一种 “项目 Runner”，那么你需要打开某个具体的 git 项目。 如果你需要注册第二种 “群组 Runner”，那么你需要打开某个具体的群组。 找到左侧导航栏最下方的设置按钮，依次选择 “设置（Settings） -&gt; CI/CD -&gt; Runners”。 点击 Runner 右侧的 “展开” 按钮。 在展开内容的左侧，找到 “Set up a specific Runner manually” 中的内容。 后续将使用 “Set up a specific Runner manually” 中的内容注册 Runner。 注册 在终端中执行如下命令，开始注册流程： 注意： 如果是在 Linux 平台上注册，请使用 sudo 执行 register 命令，macOS 则直接执行下方命令即可。 gitlab-runner register 针对顶部黄色WARNING警告的说明请参考：顶部黄色WARNING警告。 1. 输入 GitLab URL 执行完命令后，首先要求输入 GitLab 的 url。填入你们 GitLab 的地址即可。 细心的你可能会注意到，“Set up a specific Runner manually” 中的 url 是 http，这里写的却是 https。 其实正确的应该是 https。如果你使用 http 进行注册，那么进行到最后你将收到 “认证失败” 的错误。 2. 输入 Token 这里的 token 来自 “前期准备” 一节中 “Set up a specific Runner manually” 里显示的 Token。 复制该 token 并贴入这里，按回车进入下一步。 3. 给 Runner 起一个名字 该值后续可进行修改，详见 配置 GitLab-Runner 第三步要求输入一个 description 描述，其实理解为 Runner 的名字会更好一点。 一般使用英文，例如 “sever_xxx_ios_project_spare”。 4. 设置 Runner 的标签 该值后续可进行修改，详见 配置 GitLab-Runner 标签列表（tag）决定着 Runner 可以执行哪些任务（job）。 在这一步您可以设置多个标签，之间使用 , 号分割即可。 一个清晰、简短并且可以表达 Runner 职责的标签名才是一个好的标签名。 针对 tag 的简单说明 上文有提到，Runner 首先会被项目、群组等范围限制着。 但是这样还不够，在下面这个现实场景中，就会有很多问题： 假设现在有一个“项目Runner”： 该 git 项目设置了多个CI/CD任务 该 git 项目内还注册了多个 Runner，甚至还有继承自群组的 Runner 此时如果你有特殊需求，需要任务A必须执行在RunnerA上，该如何做？ 这时我们就可以通过 “标签” 来做到这点，只需要确保RunnerA的标签包含任务A的标签即可。 只有当 Runner 的标签列表包含将要执行的任务的标签时，才会选择该 Runner 去执行这个任务。否则即使该 Runer 处于空闲状态，也不会被使用。 5. 无视这一步 直接按回车即可。 6. 选择执行器 当你输入正确的 url 和 token，那么你将最后一步：选择 Runner 的执行器： 在这一步输入你想选择的执行器的名称，输入回车即可完成 Runner 的注册。 有关执行器的详细说明，请参考 GitLab 的官方文档。这里重点说一下比较常用的两种执行器：shell 和 docker。 shell shell 执行器有以下几个特点： 当前任务在 本机的shell环境 下执行，意味着当前任务生成的文件将直接生成在本机路径内，不需要做任何外的配置，任务就可以访问本机中任意文件。 每个任务和每个任务之间没有隔离，意味着当前任务生成的文件，不需要任何额外配置，就能够被下个任务直接读取使用。 shell 执行器一般用于执行纯 shell 命令、脚本，同时对任务之间的隔离不敏感的任务。 iOS、macOS App 打包必须选择 shell 执行器，无法使用 docker 执行。 docker 简单地、按照本文作者的个人理解，解释一下 docker 是什么： docker 技术即容器技术，基于 docker 可以快速地配置好用户所需的开发环境，同时基于 docker，可以实现容器与容器之间的隔离。 当我们选择 docker 执行器后，下一步会让我们填写一个默认的 docker 镜像名称，该名称后续可更改。 Android 打包、上传 bugly 等操作均可使用 docker 执行器执行。 总结 iOS、macOS App 打包只能选择 shell。 不需要考虑任务与任务之间互相隔离的、轻量化的任务可以选择 shell。 需要复杂的开发环境，任务与任务之间需要互相隔离，或者较为繁杂的任务可以选择 docker。 顶部黄色WARNING警告 首先，如果是在 macOS 环境下，那么无视该警告即可； 如果是在 Linux 环境下，则是因为执行 gitlab-runner register 命令时没有添加 sudo 导致的，中断当前流程，重新执行 sudo gitlab-runner register 即可。 通过上文我们已经了解到，iOS 打包必须在 shell 环境下执行，打包过程中使用当前用户申请去执行相关 gitlab-runner 命令，所以在注册 runner 时，无法使用 sudo 执行。 GitLab-Runner 没有对此进行额外的判断，所以只要不加 sudo，就是 user-mode，就会输出该警告。 ","link":"https://blog.rakuyoo.top/gitlab-ci-register-runner/"},{"title":"GitLab-CI（一）：搭建 iOS CI 环境","content":" 本文档用于介绍如何搭建适用于构建 iOS 项目的 GitLab-CI 环境。 GitLab-CI 系列文章目前有三篇，主要讲解 GitLab-CI 的环境搭建与配置。本文为该系列的第一篇。 目录： GitLab-CI（一）：搭建 iOS CI 环境 GitLab-CI（二）：注册 GitLab-Runner GitLab-CI（三）：配置 GitLab-Runner macOS 构建 iOS 项目必须要基于 macOS 操作系统。 无法上云（因为公有云不支持 CPU 虚拟化），且目前没有提供的 macOS 服务器，故需要一台本地硬件来提供 CI/CD 服务。 最低要求 macOS 的系统版本要求符合开发所使用的 Xcode 所要求的最低系统版本。 因为一般来说，一个开发组内的系统版本保持一致，故 CI 机器的系统版本应当尽量保持一致。 可查阅 Apple 官方的 Xcode - 支持 文档，查看系统最低要求。 如果使用 Xcode 14.2 &amp; macOS 13 版本的系统进行开发，则 CI 机器最低应使用 macOS 12.5 版本。 翻墙 VPN 因为后续诸多步骤需要翻墙才能提高访问速度，甚至需要翻墙才能安装成功，所以建议先把翻墙VPN配置好。 开发环境 Xcode 在 macOS 上，Xcode 是大部分编程环境的基础；Xcode 包含例如 git 在内的一系列开发工具，所以第一步我们应当先下载 Xcode，之后再执行其他操作。 通过 Mac App Store 下载最新版本的 Xcode，或去官网下载对应版本的 Xcode。 如上所述，CI 机器上的 Xcode 版本应当与开发所使用的版本保持一致。 下载安装好 Xcode 之后，应当先打开一次 Xcode，完成后续流程，直至看到欢迎页面。 命令行工具 打开终端，输入以下命令，安装 Xcode 命令行工具： xcode-select --install 在弹出的弹窗内，选择安装，并根据提示完成后续安装即可。 Git 配置 安装 Xcode 的过程中，将自动安装 git 服务。此时您需要在命令行中进行一些额外的配置： git config --global user.name &quot;git账户名&quot; git config --global user.email &quot;注册git所用邮箱&quot; SSH 您需要配置 SSH，将本机的 SSH 添加到 GitLab 系统中。 首先进入 .ssh 目录： cd ~/.ssh # 如果该目录不存在，则需要您手动使用 mkdir ~/.ssh 进行创建 接着创建 ssh 文件： ssh-keygen -t rsa -C &quot;注册git所用邮箱&quot; 输入命令之后，连续敲击3个回车完成创建（请勿设置密码，否则可能导致一些问题） 最后，使用下列命令查看生成的公钥，并复制到 GitLab 中即可： cat id_rsa.pub 环境初始化 博主自己写了一个用于初始化环境的脚本： /bin/zsh -c &quot;$(curl -fsSL 以后的脚本路径)&quot; ci 在终端中，输入上述命令后，将安装如下服务： Homebrew（安装过程中需要用户手动选择国内镜像源） RVM，并确保 ruby 版本为 2.7.4 版本 Bundle Mint 在 ~ 目录下添加开机解锁密码（默认为 qazxsw2） 您可以 fork 该脚本后修改脚本内容，以安装适用于您自己的开发环境。 针对 Homebrew 的额外配置 执行初始化脚本时，可能出现如下报错： No remote 'origin' in ... fatal: not in a git directory Error 此时初始化脚本会被中断。我们可以执行 brew -v 命令查看提示，并按照提示执行如下命令： git config --global --add safe.directory homebrew-core路径 git config --global --add safe.directory homebrew-cask路径 git config --global --add safe.directory homebrew-services路径 将 homebrew-core、homebrew-cask 和 homebrew-services 的路径添加到 git config 中即可。 之后再重新执行初始化脚本即可。 内网 VPN 一般来说 GitLab 服务都安装在公司内网，需要连接内网才能进行访问。 您需要在这一步中根据您自身的情况，确保 CI 电脑可以顺利访问公司内网服务。 GitLab-Runner 整体安装、配置流程可参考 官方文档，本节内容仅作为补充说明使用。其内容与官方文档基本一致。 下载安装 因为官方使用的是亚马逊的 s3 服务器，在墙内下载会比较慢，建议开启翻墙 VPN 或在早上或者网络条件比较好的时候下载。 GitLab-Runner 作为 GitLab-CI 的基础，其版本应该和 GitLab 的版本有所对应，否则容易出现不匹配的情况。 14.8.1-jh 版本的 GitLab 可以使用 15.1.0 版本的 Runner。 在终端输入下面的链接下载安装包： 该安装包适用于 Intel 芯片的 Mac，如果是 M 系列芯片的 Mac，则需要将最后的 amd64 修改为 arm64。 sudo curl --output /usr/local/bin/gitlab-runner https://gitlab-runner-downloads.s3.amazonaws.com/v15.1.0/binaries/gitlab-runner-darwin-amd64 赋予权限 sudo chmod +x /usr/local/bin/gitlab-runner 在终端输入上述命令后，即可在终端内使用 gitlab-runner 命令。 注册 Runner 详细注册流程请参考 注册 GitLab-Runner 启动 Runner 首先打开终端，切换到当前登陆用户： su - &lt;username&gt; 接着安装 Runner 服务并启动： cd ~ gitlab-runner install gitlab-runner start 最后重启电脑即可完成 Runner 的安装。 可以在终端内输入 gitlab-runner list 命令，查看输出，并检查 config.toml 文件路径。 配置 Runner 请参考 配置 GitLab-Runner 文档中 concurrent 一节进行配置。 至此，这台电脑完成了 CI/CD 的相关配置。 ","link":"https://blog.rakuyoo.top/gitlab-ci-build-ios-environment/"},{"title":"Objective-C delegate 性能小优化","content":"之前也是对 delegate 怨念颇深——每次都需要检查 deleagte 并且检查是否响应了某个方法。不过那个时候的怨念是来自于 需要写那么一大段代码，而不是怨念性能。 过去的解决方法 当时的解决办法是写了这么一个 宏 ： #define MBCCheckDelegate(methodName) (self.delegate &amp;&amp; [self.delegate respondsToSelector:@selector(methodName)]) 实际上写成 内联函数 也是可以的。不过到了这一步又开始有点犯懒了，所以一直没有继续。 优化 直到今天看到了 参考1 的这篇文章。文章中提到了可以缓存 respondsToSelector: 方法的结果，让我感慨原来还能这么做！ @protocol SomethingDelegate &lt;NSObject&gt; @optional - (void)something:(id)something didFinishLoadingItem:(id)item; - (void)something:(id)something didFailWithError:(NSError *)error; @end @interface Something : NSObject @property (nonatomic, weak) id &lt;SomethingDelegate&gt; delegate; @end @implementation Something { struct { unsigned int didFinishLoadingItem:1; unsigned int didFailWithError:1; } delegateRespondsTo; } @synthesize delegate; - (void)setDelegate:(id &lt;JSSomethingDelegate&gt;)aDelegate { if (delegate != aDelegate) { delegate = aDelegate; delegateRespondsTo.didFinishLoadingItem = [delegate respondsToSelector:@selector(something:didFinishLoadingItem:)]; delegateRespondsTo.didFailWithError = [delegate respondsToSelector:@selector(something:didFailWithError:)]; } } @end 如上所示，便是使用 结构体 来缓存结果。我们也可以使用其他的数据结构来达到我们的目的。 然后在实际通过 delegate 对象调用方法时，只需要判断对应的属性即可。 但是有一点需要留意，Objective-C 可以动态添加方法，所以设置时的情况未必能代表调用时的情况。具体要不要用这种方法还要考虑团队内的开发氛围。 简写的 delegate 在有 protocol 之前，一般是给 NSObject 加一个 category 来声明 delegate 可以实现的方法。比如，现如今 CALayer 还有： @interface NSObject(CALayerDelegate) - (void)displayLayer:(CALayer *)layer; // ... other methods here @end 意思是告诉编译器，任何对象都可以实现 displayLayer: 方法。 如果这样写的话，在调用方法之前也要同样要用上面提到的 -respondsToSelector: 来检查。只要让 delegate 对象实现这个方法，然后把它保存在 delegate 属性里就行了，不用写 protocol 什么的。 苹果官方的库里有不少这样的东西，但是新写的代码最好还是用上面声明 protocol 的正规写法。 因为这种简写方法会污染NSObject（会干扰代码自动补全），并且也让编译器不易检查出打错字之类的 error。 参考 [爆栈热门 iOS 问题——戴仓薯] 如何写好一个 delegate ","link":"https://blog.rakuyoo.top/objc-delegate-optimization/"},{"title":"C 与 Objective-C 中的枚举","content":"本文介绍 C 与 Objective-C 中的枚举，同时对 Objective-C 中的字符串枚举进行介绍 C语言中的枚举 C语言中也是有枚举的，或者说枚举正是从C语言中诞生的。在C语言中，枚举使用 enum 关键字声明，辅以 typedef。让我们来举个例子： typedef enum RKOEnumType { RKOEnumTypeNone = 0, RKOEnumTypeNormal, ... } RKOEnumType; 上面的代码便定义了一个枚举类型 RKOEnumType，使用它声明变量时我们可以这么使用： RKOEnumType type = RKOEnumTypeNone; 而用它进行判断时则是这样的： switch (type) { case RKOEnumTypeNone: // some code break; case RKOEnumTypeNormal: // some code break; ... } Objective-C 中的枚举 在 Objective-C 语言中，Foundaation 框架为我们封装了一套更方便的枚举定义方法。那便是 NS_ENUM 和 NS_OPTIONS。 NS_ENUM NS_ENUM 用来申明单选类型的枚举，即枚举变量中只可以有一种值。让我们来看一下栗子： typedef NS_ENUM(NSInteger, RKOEnumType) { RKOEnumTypeNone = 0, RKOEnumTypeNormal, ... }; 我们可以看到，这里用 NSInteger 声明了枚举变量的类型，即整数类型，其在使用时和 C语言 并无区别，依然使用 switch 语法用以区分每种不同的枚举变量。 NS_OPTIONS 除了 NS_ENUM 之后，Objective-C 还为我们封装了一套 NS_OPTIONS。有的时候单一的枚举值并不满足我们的需求，例如在如下场景时：我们希望设一个视图的可选转方向，它即可向左旋转，又可向右旋转。这时如果我们使用枚举来代表旋转方向的话，显然我们不应该设置一个枚举值为“既可向左又可向右”，我们依然定义一个 RKORotatTypeLeft 一个 RKORotatTypeRight，然后多选它们。那么我们该如何定义呢？下面来举个例子： typedef NS_OPTIONS(NSUInteger, RKORotatType) { RKORotatTypeRightNone = 0, // 2进制中的 0000 RKORotatTypeRight = 1 &lt;&lt; 0, // 值为2的0次方，二进制中的 0001 RKORotatTypeLeft = 1 &lt;&lt; 1, // 值为2的1次方，二进制中的 0010 ... }; 这样我们使用 NS_OPTIONS，结合位移操作，就定义了一个可以进行多选的枚举类型了。需要注意的是，NS_OPTIONS 使用的变量类型为 NSUInteger。而不是 NS_ENUM 的 NSInteger。 在对变量赋值时，我们使用 | 或运算符连接多个不同的枚举值，例如： RKORotatType type = RKORotatTypeRight | RKORotatTypeLeft; 判断 NS_OPTIONS 在对变量的判断上与 NS_ENUM 不同，因为变量中可以包含多个值，所以我们就不方便使用 switch 来进行判断了。 一般的，我们使用 if 来进行 NS_OPTIONS 类型的枚举的值判读，如下所示： // RKORotatTypeRight if (type &amp; RKORotatTypeRight) { // some code } // 包含 RKORotatTypeLeft if (type &amp; RKORotatTypeLeft) { // some code } 使用 if 判断，结合 &amp; 与运算符，我们即可判断变量中是否包含某个具体的枚举值，进而执行操作。 排除 有的时候，我们为了方便，有可能定义下面这样的一个选项枚举： typedef NS_OPTIONS(NSUInteger, RKORotatType) { RKORotatTypeRightNone = 0, RKORotatTypeRight = 1 &lt;&lt; 0, RKORotatTypeLeft = 1 &lt;&lt; 1, RKORotatTypeTop = 1 &lt;&lt; 2, RKORotatTypeBottom = 1 &lt;&lt; 3, RKORotatTypeAll = RKORotatTypeRight | RKORotatTypeLeft | RKORotatTypeTop | RKORotatTypeBottom, }; 这里我额外定义了一个枚举选项为 RKORotatTypeAll，代表所有的旋转类型。现在我们有可能有这么一个需求，点击某个按钮，或者执行了某个操作之后，要禁止这个视图向左旋转。 我们可以重新对 type 进行赋值，也可以像下面这样，将 RKORotatTypeLeft 从 type 中排除掉： type = type &amp; (~RKORotatTypeLeft); 将 RKORotatTypeLeft 取反后再与 type 进行与操作，即可从 type 中移除 RKORotatTypeLeft 选项。 C数组与字符串枚举 如上所述，c 与 Objective-C 的常见枚举都是 int 类型，如果我们需要一个字符串枚举呢？ 我们可以借助 c 数组来实现。首先定义一个常见的 Objective-C 枚举： typedef NS_ENUM(NSUInteger, KLType) { KLTypeRed = 1, KLTypeGreen = 2, KLTypeOrange = 3, }; 随后使用该 Objective-C 枚举作为下标，定义 c 数组： // 这个是 Map NSString * 类型的数组 NSString *KLTypeStringMap[] = { [KLTypeRed] = @&quot;红色&quot;, [KLTypeGreen] = @&quot;绿色&quot;, [KLTypeOrange] = @&quot;橘色&quot; }; // 使用： KLTypeStringMap[KLTypeRed]; 参考 iOS Enum-枚举的正确使用——Snow_xueY NS_OPTIONS枚举的用法——Channe OC中枚举写法 以及 字符串类型枚举实现探索 ","link":"https://blog.rakuyoo.top/enums-in-objc/"},{"title":"SQL 基础语句与 SQLite 基础操作","content":"这篇文章主要是围绕 SQLite，记录总结 SQL 的一些用法。 至于 iOS 中其余的几种存储方式例如 .plist 等，以及数据库的概念，SQL 的概念等，先不做介绍。 SQL 的特点 不区分大小写。 每条语句必须以分号 ; 结尾。 不可以使用关键字来命名表、字段。（关键字见下一小节） SQL 中的关键字 select 、 insert 、 update 、 delete 、 from 、 create 、 where 、 desc 、 order 、 by 、 group 、 table 、 alter 、 view 、 index etc. 注意，为了保持良好的数据库编程习惯，我们在编写 SQL 语句的时候，应当采用关键字大写的形式。 SQL 语句的种类 DDL 数据定义语句 数据定义语句 DDL 的全称是 Data Definition Language。 其包含 create 、 drop 等操作。create 即 创建表 (create table) 。drop 则是 删除表 （drop table）。 DML 数据操作语句 数据操作语句 DML 的全称是 Data Manipulation Language。 包括 insert 、 update 、 delete 等操作。 第一次接触的时候，只看这些关键词的话可以会误解 delete 为删除表的操作。实则不然，DML 全部都是对 数据 的操作，DDL 才是对于表的操作。 所以这里 insert 为在表中 添加 （插入）数据，update 为 修改 表中的数据，delete 为 删除 表中的数据。 DQL 数据查询语句 数据查询语句 DQL 的全称是 Data Query Language。用于查询并获得表中的数据。 关键字 select 是 DQL （也是所有 SQL ）用的最多的操作。 其他的 DQL 常用的关键字有 where 、order by 、 group by 、 having 等。 SQL基础操作 以下内容就是重点要记录的部分了。为了更加清晰的划分各个模块，以下内容将使用分割线来对各个模块进行划分。 创表 CREATE TABLE 表名 (字段名1 字段类型1, 字段名2 字段类型2, ...); 上面个这条语句是最基本的创表语句，create 关键字 和 table 关键字连用，用于创建一张表，用 () 英文括号将多个 字段名 和 字段类型 括起来。 一般的，我们不使用上面那条语句，为了更加的安全，我们使用 下面这种写法进行表的创建 ： CREATE TABLE IF NOT EXISTS 表名 (字段名1 字段类型1, 字段名2 字段类型2, ...); IF NOT EXISTS 用于判断所要创建的 表是否存在，如果不存在的话才会创建。 举个例子的话创建一张表的实际语句是这样的： CREATE TABLE IF NOT EXISTS t_student (id integer, name text, age inetger, score real); tip: 又一则良好的 SQL 编码习惯——在创建表时，表名前加上 t_ 来表明这是一张表，同理，创建视图时使用 v_ 来表明视图。 然而，当我们使用例如 Navicat Premium 等软件，在 GUI 中创建表，然后使用 DDL 查看该表的创表语法时，我们会看到如下的语句： CREATE TABLE &quot;t_shop&quot; ( &quot;name&quot; text, &quot;price&quot; real, &quot;left_count&quot; integer ); 表名和字段名都加了引号。我稍微查了一下，好像 oracle 是需要加引号的，好像和大小写有关。不过在 SQLite 中不加引号即可。毕竟 Navicat Premium 中集成了多种数据库，不是仅有 SQLite。 字段类型 SQLite 将数据划分为以下几种存储类型： 名称 含义 integer 整型值 real 浮点值 text 文本字符串 blob 二进制数据（比如文件） 然而呢，实际上 SQLite 是 无类型 的。就算我们将一个字段声明为 integer，还能可以存储类如 字符串之类的数据（注意，这里 主键除外）。 这也就意味着我们在建表的时候实际上是可以不声明类型的，只声明名称即可。但是为了保持良好的编程习惯，可可读性等等，我们还是应该声明类型的。 删表 删表的时候使用关键字 drop，和创表时一样，分为普通的写法和我们平时会用到的写法。这里就直接给出后者了： DROP TABLE IF EXISTS 表名; 举个例子： DROP TABLE IF EXISTS t_student; 插入数据 INSERT INTO 表名 (字段1, 字段2, ...) values (字段1的值, 字段2的值, ...); 提炼一下上述语法，即 insert into ... values ...。感觉还是很好理解的：插入到... 它的值是 ... 举例来说： INSERT INTO t_student (name, age) VALUE ('mj', 10); 注意： 在存储 字符串 时，我们应当使用 单引号 将其括起来。 在插入数据时，并不一定要每个字段都有值，即表名后面的那个括号，不必包含表中的所有字段。 更新数据 UPDATE 表名 SET 字段1 = 字段1的值, 字段2 = 字段2的值, ...; 上面便是更新数据的语法格式，但是如果仅仅是套用这个格式的话，例如下面这条语句： UPDATE t_student SET name = 'jack', age = 20; 这条语句将会将整张表的所有数据，name 字段的值都改为 jack，age 字段的值都改为20。 所以在使用的时候我们还要结合下一大节的条件语句进行使用。 删除数据 DELETE FROM 表名; 删除数据的一般语法也很简单，但是和更新数据一样。执行了上面的操作之后，整张表的所有数据都会被删除。所以也要结合条件语句进行使用。 条件语句 在 创表 和 删表 时，我们曾经使用过 if 这个关键词，可以理解为如果。但是这个 if 并不是条件判断语句的用词。 在条件判断语句中，我们使用 where 关键字，可以翻译为 当....时，初次接触时我很好奇，如果不使用 if 的话，为什么不用 when 呢？感觉这里可能更是为了突出 在某行记录 的这个概念吧。 条件语句的格式也很好记忆，即在需要条件判断的语句后面，跟上由 where 开头的条件判断语句即可。 格式 含义 WHERE 字段 = 某个值 字段的值是否等于某个值 WHERE 字段 != 某个值 字段的值是否不等于某个值 WHERE 字段 &gt; 某个值 字段的值是否大于某个值 WHERE 字段 &lt; 某个值 字段的值是否小于某个值 WHERE 字段1 = 某个值 AND 字段2 &gt; 某个值 关键字 AND 用于连接两个判断条件 WHERE 字段1 = 某个值 OR 字段2 &gt; 某个值 关键字 OR 用于连接两个判断条件 举例： -- 将 t_student 表中年龄大于10并且姓名不等于jack的记录，年龄都改为5。 UPDATE t_student SET age = 5 WHERE age &gt; 10 AND name != 'jack'; -- 删除 t_student 表中年龄小于等于 10 或者 年龄大于 30 的记录。 DELETE FROM t_student WHERE age &lt; 10 OR age &gt; 30; -- 将 t_student 表中名字等于 jack 的记录，score 字段的值都改为 age 字段的值。 UPDATE t_student SET score = age WHERE name = 'jack'; 特别注意：在 SQLite 中，等于 这个概念使用的是 单独的等号。和 Obj-C 以及其他大部分编程语言都不相同。 查询数据 查询操作使用关键字 select。格式为： SELECT 字段1, 字段2, ... FROM 表名; 如果我们想要查询整张表的数据，可以使用 * 号来代替字段名，即 SELECT * FROM 表名; 举个例子： SELECT name, age FROM t_student; SELECT * FROM t_student WHERE age &gt; 10; 别名 在进行查询操作时，我们可以为 字段 和 表 起一个 别名，从而我们我们查看或者进行某些操作。不过需要注意的是，别名 顾名思义，只是另外一个称呼，并不会修改字段本身的名称。 别名 使用关键字 as，不过该关键字是可以省略的，而且，这个别名可以使用 中文。其格式如下： -- 最基本的写法为： SELECT 字段1 AS 别名1, 字段2 AS 别名2, ... FROM 表名 AS 表别名; -- 当隐藏 AS 时 SELECT 字段1 别名1, 字段2 别名2, ... FROM 表名 表别名; 同时，如果我们为表起了一个 别名，那么我们在查询时就可以这么定义需要查询的字段： SELECT 表别名.字段1 别名1, 表别名.字段2 别名2, ... FROM 表名 表别名; 即用 表别名 后接 . 再接字段名的格式，用 表别名 来引用表中的字段。这样做的好处是，在使用如 Navicat Premium 时，可以提示该表有哪些字段，以防字段太多我们记不清楚。而且当多表联查时出现了字段名重名的问题的话，表别名 也是一个很好的解决办法。 举个例子： SELECT s.name 姓名, s.age 年龄 FROM t_student s; 计算记录的数量 在 SQL 中有这么一个函数 count() ，其作用为记录某个字段的数量。格式为： SELECT COUNT(字段名) FROM 表名; 不过一般来说，我们直接使用 * 号来代替字段名。即： SELECT COUNT(*) FROM 表名; 举个例子： SELECT COUNT(*) FROM t_student WHERE score &gt;= 60; 查询结果排序 我们可以使用关键字 order by 对查询结果进行排序，其格式为： SELECT 字段名 FROM 表名 ORDER BY 字段名 排序规则; 其默认是按照 asc ，即升序排列。我们可以显示的修改排序规则，改为 desc， 即降序排列。如果省略排序规则的话，则是默认的升序排列。 举个例子： -- 省略排序规则。(按照升序排列 SELECT * FROM t_student ORDER BY age; -- 降序排列 SELECT * FROM t_student ORDER BY age DESC; 同时我们还可以指定按照多个字段进行排序，例如下面这条语句： SELECT * FROM t_student ORDER BY age ASC, height DESC; 这条语句会将查询结果按照字段 age 升序排列，年龄相等的话就会按照字段 height 降序排列。 未完待续 ... ","link":"https://blog.rakuyoo.top/sql-basic-and-sqlite-basic/"},{"title":"去掉隐式动画","content":"实际上 iOS 好像是会在很多地方默认添加上动画的。这些动画倒也还好，但是有的时候就和需求不符就比较烦人了。 前几天偶然的再次查到了这个方法，于是决定开个文章记录一下。 感谢参考1，原作者的总结。本文基本引用自参考1。 说到隐式动画，之前在做手势控件的时候，对于 CALayer 的一些操作就会有隐式动画，例如说下面这行代码就会有隐式动画： self.frameLayer.frame = self.frameView.bounds; 这里介绍几种可以去掉系统动画的方法。如果您想隐藏的是 CALayer 相关的动画，请直接参考 方法三。 方法一 方法一是使用 UIView 类的类方法： performWithoutAnimation [UIView performWithoutAnimation:^{ // your code }]; 如果你的 App 需要最少支持到 iOS 7 。那么推荐你使用该方法。 方法二 该方法可行，但是实在是太蠢了，不建议使用 在你知道会产生动画的代码处，再次添加一个动画，只不过将这个动画的时间设为0： [UIView animateWithDuration:0 animations:^{ [collectionView performBatchUpdates:^{ [collectionView reloadItemsAtIndexPaths:@[[NSIndexPath indexPathForItem:index inSection:0]]]; } completion:nil]; }]; 方法三 方法四是针对 CALayer 的隐式动画的。这种情况下上面的三种方法是无效的，只能采用方法四： [CATransaction begin]; [CATransaction setDisableActions:YES]; // CALayer 动画 self.frameLayer.frame = self.frameView.bounds; [CATransaction commit]; 参考 iOS开发小技巧-如何去掉隐式动画——木头加白水 ","link":"https://blog.rakuyoo.top/remove-implicit-animation/"},{"title":"PushKit 的集成","content":"实际上项目中很早就由我提出要将现有的 APNs 更改为 PushKit 了。只不过当初后端的同学很忙，没有时间搞这个... 现在因为要接入 CallKit，所以这个不搞不行了。那么也来记录一下吧。 这篇文章中没有涉及到证书的申请，这块儿我还不太会，等到未来接触了会回过来补充的 流程 导入框架 Xcode设置 这里要区分一下 Xcode 的版本。如果你的版本是 Xcode 9 以前的版本，那么请按照如下步骤配置： 在 Xcode 中选中你的项目，切换到主 Target 下。 点击 capabilities 选项卡，找到 Background Modes 这一项。 勾选 Voice over IP 以及 Remote notifications 两项即可。 如果你的 Xcode 版本是 Xcode 9 及以上版本，那么很遗憾。苹果在 Background Modes 中移除了 Voice over IP 这一项，所以我们只能选择手动地在 info.plist 文件中进行配置： 我们可以选择 Open As Source Code 模型，然后添加如下代码： 我们也可以直接打开 info.plist 文件，然后在 Required background modes 中添加图中的这项。 最后我们在 Build Phases 的 Link Binary With Libraries 中添加 PushKit 的系统库，如下图所示： AppDelegate.m 首先我们在 AppDelegate.m 中导入头文件： #import &lt;PushKit/PushKit.h&gt; 其次我们还需要遵守 PKPushRegistryDelegate 协议。 注册推送 我们在 AppDelegate.m 文件中，编写如下方法： // 注册pushKit推送 - (void)registerPushKitNotification { if ([UIDevice currentDevice].systemVersion.floatValue &gt;= 8.0) { PKPushRegistry *pushRegistry = [[PKPushRegistry alloc] initWithQueue:dispatch_get_main_queue()]; pushRegistry.delegate = self; pushRegistry.desiredPushTypes = [NSSet setWithObject:PKPushTypeVoIP]; } } 注意： PushKit 的支持是需要 iOS 系统大于 8.0 的。 PKPushTypeVoIP 是一个 key 值，具体的会放在后面讲解。 之后我们在 application:didFinishLaunchingWithOptions: 方法中调用上述方法即可。 代理方法 pushRegistry:didUpdatePushCredentials: PKPushRegistryDelegate 协议中有一个必须要实现的代理方法： - (void)pushRegistry:(PKPushRegistry *)registry didUpdatePushCredentials:(PKPushCredentials *)pushCredentials forType:(PKPushType)type; 简单说一下，就是当收到一个指定的 PKPushType（这是一个枚举）类型的新的 credentials（凭证/证书）（包括 push token）时，就会调用这个方法。 在这个方法中，我们可以获取 token 值，然后将其缓存起来： - (void)pushRegistry:(PKPushRegistry *)registry didUpdatePushCredentials:(PKPushCredentials *)credentials forType:(NSString *)type { NSLog(@&quot;--- Token ---%@ ====%@&quot;, credentials.token, [credentials.token description]) NSString *deviceTokenStr = [[credentials.token description] stringByReplacingOccurrencesOfString:@&quot; &quot; withString:@&quot;&quot;]; deviceTokenStr = [deviceTokenStr stringByReplacingOccurrencesOfString:@&quot;&lt;&quot; withString:@&quot;&quot;]; deviceTokenStr = [deviceTokenStr stringByReplacingOccurrencesOfString:@&quot;&gt;&quot; withString:@&quot;&quot;]; NSUserDefaults *groupDefault = [[NSUserDefaults alloc] initWithSuiteName:GroupDefaultsName]; [groupDefault setValue:deviceTokenStr forKey:DeviceTokenKey]; [groupDefault synchronize]; } pushRegistry:didInvalidatePushTokenForType: 接着我们可以在 pushRegistry: didInvalidatePushTokenForType: 方法中清除 Token - (void)pushRegistry:(PKPushRegistry *)registry didInvalidatePushTokenForType:(PKPushType)type { NSUserDefaults *groupDefault = [[NSUserDefaults alloc] initWithSuiteName:GroupDefaultsName]; [groupDefault setValue:@&quot;&quot; forKey:DeviceTokenKey]; [groupDefault synchronize]; } pushRegistry:didReceiveIncomingPushWithPayload: 接下来的代理方法特别重要，那就是 pushRegistry:didReceiveIncomingPushWithPayload: 。这个方法不是必须实现，但是我们一般将业务逻辑写在该方法中。 一般我们可以在这个方法中执行 CallKit 相关的代码，或者结合本地推送进行操作。 - (void)pushRegistry:(PKPushRegistry *)registry didReceiveIncomingPushWithPayload:(PKPushPayload *)payload forType:(PKPushType)type { // your code } 参考 论养成一个好的习惯有多重要.....当初写这篇文章的时候居然忘记记录参考了，结果现在回过头来补充的时候找不到当初的参考了....如果您发现这篇文章引用了您的文章，请评论或者邮件告诉我，我会在这里引用上您的文章链接。谢谢！ ","link":"https://blog.rakuyoo.top/pushKit-integration/"},{"title":"从 Share Extension 跳转到 Containing App","content":"Share Extension 这篇文章就不多讲了。 单纯的说一下从前辈那里请教来的如何从 Share Extension 跳转到 Containing App 吧。 代码 NSURL *customURL = [NSURL URLWithString:@&quot;ShareKit://&quot;]; SEL sel = NSSelectorFromString(@&quot;openURL:&quot;); NSExtensionContext *context = [[NSExtensionContext alloc]init]; [context openURL:customURL completionHandler:nil]; UIResponder *responder = self; while (responder != nil){ if ([responder respondsToSelector:sel]) { [responder performSelector:sel withObject:customURL]; } responder = responder.nextResponder; } 原理 代码是从前辈那里请教过来的，原理就是利用 UIResponder 查找响应链。直接在 Extension 中调 openURL 方法是会闪退的，因为响应不了方法。 所以使用 UIResponder，从响应树上查找能够响应这个方法的对象，也就是 Safari，或者其他使用原生分享的 App。 用途 用途这块实际上也没必要多说，不过说一点，这个功能上架不会被拒，像是 QQ浏览器，微信， 支付宝 等等都有这种跳转的功能，不过 微信 的及其隐蔽。所以大家在使用中可能还是得把入口藏得深一些了。 ","link":"https://blog.rakuyoo.top/jump-from-share-extension-to-app/"},{"title":"Xcode9 快捷键","content":"开个贴记录一下 Xcode 9 下自己常用的几个发生了变化的快捷键。 快速跳转至定义 Xcode 9 下有这么一个新的功能，command + 鼠标左键 之后会弹出下图的这个菜单。 但是这里部分的功能很少用到，再加上之前已有的习惯，所以这个改动真的让人很难受。而且它提供的快捷键 control + command 有的时候还按不出来。 实际上这里有个新的快捷键： command + 鼠标右键 大家可以回去试一下，可以直接跳转到定义。 代码折叠 代码折叠的方式也改了，现在的快捷键是： 局部折叠（折叠一个函数） Command + Option + 方向键左/右 全局折叠（折叠当前文件下的全部函数） Shift + Command + Option + 方向键左 / 右 折叠注释块：（/* */之间的文字） Ctrl + Shift + Command + 方向键左 / 右 隐藏/显示左侧边栏 Command + 0 我的 macOS 版本是 macOS 10.12.6，在这个版本下，Xcode 9 不能做到两个项目分屏.... 不知道和系统或者 Xcode 版本有没有关系... 所以这里只能曲线救国一下，打开单个的代码文件，然后用这个文件和项目进行分屏。 但是这样所导致的问题就是，运行项目后，代码文件 那边左侧的边栏会显示出来，但是因为是单独的文件，所以没有右上角的快捷按钮... 所以这个小节介绍的快捷键就很重要了。 参考 Xcode 9 快速跳转到定义新姿势（Jump to Definition） Xcode 9 代码折叠，全局折叠，快捷键 ","link":"https://blog.rakuyoo.top/xcode9-shortcuts/"},{"title":"iOS 开发之适配 iOS11","content":"好了，现在 Xcode 9 的正式版终于推送了，那么我们也该适配 iOS 11了。 iOS 11的改动还是挺大的，这篇文章主要只是把自己遇到的问题记录下来，没有遇到的暂时先不会出现在文章内。等有时间的话会去网上找一找然后总结下来的。 NavigationBar TitleView 宽度 我的 App 中是有这么一个自定义的NavigationBar TitleView的，其宽度原本应该与屏幕宽度一致。在 iOS 10 以及 Xcode 8.3.3 上一切安然无恙。但是在 iOS 11 以及 Xcode 9 上就变成了下图所示的模样，挤到了一起。 我们查看层次结构可以发现，TitleView变得这么小了。 解决方法 我们先来看解决方法，再来说原因。 解决方法很简单，在自定义的 TitleView 的类中添加下面的代码，就ok了。 // 适配iOS11的NavigationBar Tiltle宽度。 - (CGSize)intrinsicContentSize { return UILayoutFittingExpandedSize; } 现在我们再运行程序，一切正常： 原因 通过参考1，里面有这么一句话： As an explanation: the titleView is now laid out with Auto Layout. Since it looks for the intrinsicContentSize, this is what worked. 我翻译不好就不翻译了，大概的意思好像就是现在 TitleView 会去寻找 intrinsicContentSize 这个属性，所以我们现在如果需要去重写一下这个属性的 get方法 。 然而这个属性是只读的，所以我们要么重写这个属性，然后给它赋值，要么重写 get方法 ，直接 return 。 更加详细的可以移步到参考1去查看。 右侧视图 今天测试妹子提了这么一个 bug，操作步骤简化一下是这样的： 点击页面（该页面 NavigationBar 包含一个自定义的 rightBarButtonItem ）中的一个 UISearchBar ，调用一个点击手势，push 到一个新的页面 newViewController。 newViewController 的 NavigationBar 的 titleView 是一个自定义的视图。其包含了一个 UISearchBar 和一个取消按钮。同时 NavigationBar 的左侧是没有返回按钮的。 理想的效果是，在进入 newViewController 的同时，其 NavigationBar 上的 UISearchBar 应该变成第一响应者并弹出键盘。 但是实际上，键盘是出现了一瞬间，然后就消失了。 解决方法 在点击手势的方法中，将 NavigationBar 的 rightBarButtonItem 置为 nil。然后在 viewWillAppear 中对 rightBarButtonItem 重新赋值。 注意：隐藏是无效的，必须置 nil 才可以。 原因 这个是什么原因我也不太清楚....找了一上午实在找不到，发现和 UISearchBar 以及控制器本身无关。和 NavigationBar（空白的）也无关。无奈只能挨个注释模块，最终找到了这个 rightBarButtonItem 的问题。 推测与 iOS 11 中 rightBarButtonItem 的占位有关。但是为何会导致键盘消失还是不太清楚.... TabaleView 切换时的滑动动画 在 iOS 11 上，push 进入 Table View 或者 pop 出 Table View 的时候，会有一个 自下而上 的滑动效果。 解决方法 将 Table View 的 contentInsetAdjustmentBehavior 属性设为 UIScrollViewContentInsetAdjustmentNever 即可。 注意：需要做版本判断。这个属性是从 iOS 11 开始才有的。 参考代码如下： if (@available(iOS 11.0, *)) { self.tableView.contentInsetAdjustmentBehavior = UIScrollViewContentInsetAdjustmentNever; self.tableView.contentInset = UIEdgeInsetsMake(0, 0, [[UIScreen mainScreen] bounds].size.height == 812 ? 83 : 49, 0); self.tableView.scrollIndicatorInsets = _tableView.contentInset; } 原因 这个效果实际上不是 Table View 的。而是 Scroll View 的，详情见下面的 Scroll View 小节。 ScrollView Scroll View在 iOS 11 新增的两个属性：adjustContentInset 和 contentInsetAdjustmentBehavior。 adjustContentInset 表示 contentView.frame.origin 偏移了scrollview.frame.origin 多少；是系统计算得来的，计算方式由 contentInsetAdjustmentBehavior 决定。 contentInsetAdjustmentBehavior 是个结构体，具体如下： typedef NS_ENUM(NSInteger, UIScrollViewContentInsetAdjustmentBehavior) { UIScrollViewContentInsetAdjustmentAutomatic, UIScrollViewContentInsetAdjustmentScrollableAxes, UIScrollViewContentInsetAdjustmentNever, UIScrollViewContentInsetAdjustmentAlways, } 每一种分别代表着一种计算方式： UIScrollViewContentInsetAdjustmentAutomatic：如果 Scroll View 在一个 automaticallyAdjustsScrollViewContentInset = YES 的 控制器 上，并且这个 控制器 包含在一个 Navigation Controller 中，这种情况下会设置在 top &amp; bottom 上 adjustedContentInset = safeAreaInset + contentInset 不管是否滚动。其他情况下与第二个的 UIScrollViewContentInsetAdjustmentScrollableAxes 相同。 UIScrollViewContentInsetAdjustmentScrollableAxes：在可滚动方向上 adjustedContentInset = safeAreaInset + contentInset ，在不可滚动方向上 adjustedContentInset = contentInset ；依赖于 Scroll Enabled 和alwaysBounceHorizontal / vertical = YES，scrollEnabled 默认为 yes，所以大多数情况下，计算方式还是 adjustedContentInset = safeAreaInset + contentInset UIScrollViewContentInsetAdjustmentNever：adjustedContentInset = contentInset。 adjustContentInset 值不受 SafeAreaInset 值的影响。 UIScrollViewContentInsetAdjustmentAlways： adjustedContentInset = safeAreaInset + contentInset 权限 保存图片 在 iOS 11 中，如果你使用 XCode 9 及以上版本打包。那么在保存图片的时候，可能会遇到下面这个崩溃： This app has crashed because it attempted to access privacy-sensitive data without a usage description. The app's Info.plist must contain an NSPhotoLibraryAddUsageDescription key with a string value explaining to the user how the app uses this data. 解决方法 我们可以用 Source Code 的方式打开 info.plist 文件 输入下面的代码： &lt;key&gt;NSPhotoLibraryAddUsageDescription&lt;/key&gt; &lt;string&gt;App需要您的同意,才能保存图片&lt;/string&gt; 又或者选择 Property List 方法，在 App Transport Security Settings 项中点击加号，添加 Photo Library Additions Usage Description 字段。Type 选择 String，然后在 Value 中添加提示语。 原因 这个崩溃的原因是因为，在 iOS 11 中，保存图片也像调用相机一样，需要获取权限了。 参考 iOS 11 navigationItem.titleView Width Not Set 你可能需要为你的APP适配iOS11 iOS 11中APP中tableView内容下移20pt或下移64pt的问题适配的一个总结 iOS 11 NSPhotoLibraryAddUsageDescription 错误的解决办法 ","link":"https://blog.rakuyoo.top/available-for-ios11/"},{"title":"Objective-C 中的 nullable、__nullable 和 _Nullable","content":"这次的内容是关于 nullable、__nullable 和 _Nullable 的。对于我们这样的初学者，在看第三方框架或者官方文档的时候经常会看到这样的字样。然而平时也没有去研究它到底是什么，这次就来好好记录一下一下吧。 本次的内容转载自参考1，感觉原作者的分享与贡献。 前言 在 Swift 中，我们会使用 ? 和 ! 去显式声明一个对象或者方法的参数是 optional 还是 non-optional ，而在 Objective-C 中则没有这一区分，这样就会带来一个问题：在 Swift 与 Objective-C 混编时，Swift 编译器并不知道一个 Objective-C 对象或者一个方法的参数到底是 optional 还是 non-optional ，因此这种情况下编译器会隐式地都当成是 non-optional 来处理，这显然是不太好的。 解决方案 为了解决这个问题，苹果在 Xcode 6.3 引入了一个 Objective-C 的新特性： Nullability Annotations ，这一新特性的核心是两个新的类型修饰： __nullable 和 __nonnull 。从字面上我们可知， __nullable 表示对象可以是 NULL 或 nil，而 __nonnull 表示对象不应该为空。当我们不遵循这一规则时，编译器就会给出警告。 在 Xcode 7 中，为了避免与第三方库潜在的冲突，苹果把 __nonnull/__nullable 改成 _Nonnull/_Nullable 。再加上苹果同样支持了没有下划线的写法 nonnull/nullable ，于是就造成现在有三种写法这样混乱的局面。 但是这三种写法本质上都是互通的，只是放的位置不同，举例如下： 方法返回值修饰： - (nullable NSString *)method; - (NSString* __nullable)method; - (NSString* _Nullable)method; 声明属性的修饰： @property (nonatomic, copy, nullable) NSString *aString; @property (nonatomic, copy) NSString *__nullable aString; @property (nonatomic, copy) NSString *_Nullable aString; 方法参数修饰： - (void)methodWithString:(nullable NSString*)aString; - (void)methodWithString:(NSString * _Nullable)aString; - (void)methodWithString:(NSString * __nullable)aString; 而对于 双指针类型对象 、 Block 的返回值 、 Block 的参数 等，这时候就不能用 nonnull/nullable 修饰，只能用带下划线的 __nonnull/__nullable 或者 _Nonnull/_Nullable ： - (void)methodWithError:(NSError* _Nullable * _Nullable)error - (void)methodWithError:(NSError* __nullable* __null_unspecified)error; // 以及其他的组合方式 - (void)methodWithBlock:(nullable void(^)())block; // 注意上面的 nullable 用于修饰方法传入的参数 Block 可以为空，而不是修饰 Block 返回值； - (void)methodWithBlock:(void(^ _Nullable)())block; - (void)methodWithBlock:(void(^ __nullable)())block; - (void)methodWithBlock:(nullable id __nonnull(^)(id __nullable params))block; // 注意上面的 nullable 用于修饰方法传入的参数 Block 可以为空，而 __nonnull 用于修饰 Block 返回值 id 不能为空； - (void)methodWithBlock:(id __nonnull(^ __nullable)(id __nullable params))block; - (void)methodWithBlock:(id _Nonnull (^ _Nullable)(id _Nullable params))block; // the method accepts a nullable block that returns a nonnull value // there are some more combinations here, you get the idea 使用规范 在看了原生 iOS SDK 里 Foundation 和 UIKit 的头文件以及苹果的博文 《Nullability and Objective-C》 ，我们总结如下使用规范： 对于属性、方法返回值、方法参数的修饰，使用： nonnull/nullable； 对于 C 函数的参数、Block的参数、Block返回值的修饰，使用： _Nonnull/_Nullable， 建议弃用 __nonnull/__nullable 。 Nonnull Audited Regions 如果需要每个属性或每个方法都去指定 nonnull 和 nullable ，将是一件非常繁琐的事。 苹果为了减轻我们的工作量，专门提供了两个宏： NS_ASSUME_NONNULL_BEGIN 和 NS_ASSUME_NONNULL_END 。在这两个宏之间的代码，所有简单指针对象都被假定为 nonnull ，因此我们只需要去指定那些 nullable 指针对象即可。如下代码所示： NS_ASSUME_NONNULL_BEGIN @interface myClass() @property (nonatomic, copy)NSString *aString; - (id)methodWithString:(nullable NSString*)str; @end NS_ASSUME_NONNULL_END 在上面的代码中， aString 属性默认是 nonnull 的， methodWithString:方法的返回值也是 nonnull ，而方法的参数 str 被显式指定为 nullable 。 不过，为了安全起见，苹果还制定了以下几条规则： 通过 typedef 定义的类型的 nullability 特性通常依赖于上下文，即使是在 Audited Regions 中，也不能假定它为 nonnull； 对于复杂的指针类型（如 id * ）必须显式去指定是 nonnull 还是 nullable。例如，指定一个指向 nullable 对象的 nonnull 指针，可以使用 __nullable id * __nonnull ； 我们经常使用的 NSError ** 通常是被假定为一个指向 nullable NSError 对象的 nullable 指针 疑问 虽然在 Xcode 7 里面，苹果建议我们放弃使用 __nonnull/__nullable ，改用 _Nonnull/_Nullable 来修饰对象可否为空，但即使是在最新 ios 9.3 SDK 的 Foundation 和 UIKit 的头文件里我们可以看到官方原生类的方法参数仍然在用 __nonnull/__nullable 修饰。 另外为什么已经有了 nonnull/nullable ，为什么还要增加 _Nonnull/_Nullable ？这到底是出于什么考虑？苹果在它的博文 《Nullability and Objective-C》 中也没有具体解释，于是 StackOverflow 上有个关于此问题的讨论： Difference between nullable, __nullable and _Nullable in Objective-C 。 参考 nullable、__nullable、_Nullable 究竟有什么区别呢？——bravegogo 注意：上面的blog标准该博文未转帖，然而并没有标注原帖地址，故只能在此放出转帖地址。 ","link":"https://blog.rakuyoo.top/nullable-of-objc/"},{"title":"Objective-C 之 Block 基础用法","content":"这两天改bug改的太忙了....实际上该整理的知识点挺多的，今天就先说一下 Block 吧。 本篇文章中大量内容参考、转载自文章末尾的参考1，感谢原作者的总结与分享。原文中还有更多更详细的内容，欢迎大家点击原文学习更多内容。 什么是Block 让我们先简单的介绍一下Block 代码块Block是苹果在iOS4开始引入的对C语言的扩展,用来实现匿名函数的特性，Block是一种特殊的数据类型，其可以正常定义变量、作为参数、作为返回值，特殊地，Block还可以保存一段代码，在需要的时候调用，目前Block已经广泛应用于iOS开发中，常用于GCD、动画、排序及各类回调。 综上我们可以得知： Block是一种数据类型。 Block可以做变量，也可以做参数，还可以做为返回值，甚至用来保存一段代码。 Block变量 Block的声明 Block的声明格式为：返回值(^Block名称)(参数列表)，让我们来举个栗子: NSString *(^aBlock)(NSString *str, NSArray *arr); 如上是一个返回值为NSString，名称为aBlock，拥有2个参数，分别为NSString和NSArray的一个Block代码块。 注意： 其中形参的形参名，即栗子中的str和arr可以省略不记。 这里的名称我们便可以将它想象成函数名。 给Block变量赋值 我们声明完Block之后，就该给变量赋值了。 这种情况下的格式为：变量名 = ^返回值类型(参数类型){函数体}。举个栗子： aBlock = ^NSString *((NSString *str, NSArray *arr) { NSLog(@&quot;%@, %@&quot;, str, arr); return str; }; 这里实际上我们是把一个函数体赋值给了Block变量。从这里我们也可以看出来Block的一个重要用法，那就是用来存储函数体。 注意： 这里的返回值类型可以不用声明，直接省略。因为编译器可以从存储代码块的变量中确定返回值的类型。 如果没有参数列表，在赋值的时候可以省略。 声明的同时赋值 把上面两点综合起来就就可以了，这里直接给个栗子吧。 NSString *(^aBlock)(NSString *, NSArray *) = ^(NSString *str, NSArray *arr) { NSLog(@&quot;%@, %@&quot;, str, arr); return str; }; 调用Block变量 Block变量的调用实际上和C语言函数的调用非常接近。为了节省时间，这里的参数str1和arr我只进行了初始化，没有赋值。 NSString *str1 = [[NSString alloc] init]; NSArray *arr = [NSArray array]; NSString *str = aBlock(str1, arr); NSLog(@&quot;%@&quot;,aBlock(str1, arr)); 是不是和C语言很像呢。 使用typedef定义Block类型 在实际使用Block的过程中,我们可能需要重复地声明多个相同返回值相同参数列表的Block变量,如果总是重复地编写一长串代码来声明变量会非常繁琐,所以我们可以使用typedef来定义Block类型。 // 定义一种无返回值无参数列表的Block类型 typedef void(^SayHello)(); // 我们可以像OC中声明变量一样使用Block类型SayHello来声明变量 SayHello hello = ^() { NSLog(@&quot;hello&quot;); }; // 调用后控制台输出&quot;hello&quot; hello(); 注意，在使用typedef的时候，(^)内的Block名已经被当做了整个Block类型的名称，而不单单是一个Block的名称。 在函数形参中使用Block 形参使用Block时函数的声明 Block作为函数形参是一个很重要的用法， 很多知名的第三方库或者网络请求中都会在函数的形参中用到Block。我们自己写代码的时候，偶尔为了方便高效，或者为了封装，也可以使用Block。 其格式为：-（返回值）函数名称：（Block返回类型（^）（参数列表））函数参数名 注意： 为了看起来清晰，在上面的格式中，我使用的都是中文标点，实际使用中肯定都要是英文标点的。 在作为形参使用时，将Block写在函数定义/声明的形参位置时，Block没有Block名称。 下面是我自己的在一个项目中定义的一个函数，就使用到了Block作为形参。在这里大家理解这种声明的格式即可。 - (void)autoresizingMaskOfChildView:(NSMutableArray&lt;NSNumber *&gt; *(^)())autoMaskArr { NSMutableArray *tempArrM = [NSMutableArray arrayWithArray:autoMaskArr()]; } 上面定义了一个返回值为空void，名称为autoresizingMaskOfChildView，拥有一个Block参数的函数。 其中Block参数是一个返回值为存储NSNumber的NSMutableArray，没有参数列表。 调用形参使用Block的函数 没有参数的Block形参 上面那个栗子中的函数，其调用的时候会是这样的： [self.view autoresizingMaskOfChildView:^NSMutableArray&lt;NSNumber *&gt; *{ &lt;#code#&gt; }]; 我们在&lt;#code#&gt;中输入自己的代码，这一部分的代码相当于是赋值给了Block形参。 有参数的Block形参 我们先定义一个函数，方法中定义了一个block数据类型参数（返回值为int类型的，且带有一个int类型的形参） - (void)calculate:(int (^)(int)) calculateBlock { //calculateBlock接受外界传入的代码块，也就意味着怎么去操作是由外界调用者决定的 self.result = calculateBlock(_result);//将_result的值作为实参传入 } 外部控制器调用时如下所示： [manager calculate:^int(int i) { //参数i自加1，然后返回 i++; return i; }]; NSLog(@&quot;%d&quot;,manager.result); //输出结果为1 通过最后的NSLog输出我们可以看到，在外界调用Block的时候，参数i的值被传给了函数定义时的参数_result。相当于_result = i。 即是外界提供值，然后赋值给Block内部的变量。而不是从Block中将值传给外界。 注意：上面这种写法实际上有很大问题，我们在这个函数声明的时候，没有说这个blcok是不能为空的，那么假如外界在调用这个block的时候，如果给block参数的位置传的是nil，而函数内部又给这个block传了参数，那么就会有产生Crach。 例如下面这样： [manager calculate:nil]; calculate:函数内部不变，依然是这样： - (void)calculate:(int (^)(int)) calculateBlock { self.result = calculateBlock(_result); } 这样就会导致Crach，所以我们在使用Block之前要对Block做判空处理。 - (void)calculate:(int (^)(int)) calculateBlock { if (!calculateBlock) { self.result = calculateBlock(_result); } } 使用typedef简化Block // 1.使用typedef定义Block类型 typedef int(^MyBlock)(int, int); // 2.定义一个形参为Block的OC函数 - (void)useBlockForOC:(MyBlock)aBlock { NSLog(@&quot;result = %d&quot;, aBlock(300,200)); } // 3.声明并赋值定义一个Block变量 MyBlock addBlock = ^(int x, int y){ return x+y; }; // 4.以Block作为函数参数,把Block像对象一样传递 [self useBlockForOC:addBlock]; // 将第3点和第4点合并一起,以内联定义的Block作为函数参数 [self useBlockForOC:^(int x, int y){ return x+y; }]; Objective-C 中的block属性 说完了形参与变量，那么在 Objective-C 的属性中该如何使用 block 语法呢？ 定义 block 类型 首先我们应该定义一个 block 类型： // 在#import 与 @interface 之间声明该 block 类型 typedef void(^ButtonEventsBlock)(UIButton *btn); 这样我们就定义了一个返回值为空，名称为 ButtonEventsBlock，带有一个 UIButton 类型参数的 block 类型了。 声明 block 属性 这一步很简单，就如同声明其他变量一样，使用之前定义好的 block 类型声明一个变量即可。 @property (nonatomic, copy) ButtonEventsBlock buttonEventsBlock; 对 block 属性赋值 这里我将演示两种方法：懒加载 与 Runtime。 懒加载 在代码中为 block 属性赋值和使用 懒加载 对 block 赋值在语法层面没有太大的差异，所以这里就只演示懒加载了。 那么让我们直接砍代码吧： - (ButtonEventsBlock)buttonEventsBlock { if (!_buttonEventsBlock) { _buttonEventsBlock = ^(UIButton *btn) { // your code }; } return _buttonEventsBlock; } 我们可以看到，在语法层面上，除了为 block 赋值的那部分外，其余部分与一般的懒加载并无差异。而为 block 赋值的那部分代码我们之前也已经提到过了。 Runtime 如果我们要在分类中使用 block 属性的话，我们就要借住 Runtime 来实现了。因为 Category 中并不会为我们自动生成成员变量。 那么我们代码长什么样子呢？ static void *buttonEventsBlockKey = &amp;buttonEventsBlockKey; - (ButtonEventsBlock)buttonEventsBlock { return objc_getAssociatedObject(self, &amp;buttonEventsBlockKey); } - (void)setMbc_buttonEventsBlock:(ButtonEventsBlock)buttonEventsBlock { objc_setAssociatedObject(self, &amp;buttonEventsBlockKey, buttonEventsBlockKey, OBJC_ASSOCIATION_COPY); } 哈，这里又和一般的在 Category 中使用属性的语法没有什么区别了。只不过，这里我们就没有办法使用懒加载为其赋值了，如果需要默认操作的话，我们只能在代码中对其赋值了。 参考 一篇文章看懂iOS代码块Block——蚊香酱 iOS开发：Block作为参数使用（常见于各框架）——夏鲁鲁 ","link":"https://blog.rakuyoo.top/objc-block-basic-usage/"},{"title":"Objective-C 中 id 和 instancetype 的使用与异同","content":"今天记录一下在Objective-C中，保留字id和instancetype各自的用法，以及异同。 id 什么是id类型 在概念上，id类似于Java中的Object类，可以转换为任何数据类型。换句话说，id类型的变量可以存放任何数据类型的对象。在内部处理上，这种类型被定义为指向对象的指针，实际上是一个指向这种对象的实例变量的指针。 下面是id类型在Objective-C中的定义: typedef struct objc_object { Class isa; } *id; 从上面看出，id是指向struct objc_object的一个指针。也就是说，id是一个指向任何一个继承了Object（或者NSObject）类的对象。 id的用法 id是一个指针 因为id类型是一个指针，所以我们在使用id类型的时候不需要添加任何的星号，例如: id foo = nil; 上面的代码声明了一个id类型的指针foo，这个指针指向NSObject的任意一个子类。 而下面的代码则定义了一个指针，这个指针指向另一个指针，被指向的这个指针指向NSObject的一个子类。 id *foo = nil; 可以接收任何消息 在ObjeciveC中，id是可以接收任何消息的，所以我们可以将一个方法或者一个变量声明为id类型，用来接收多种类型的数据。 例如NSArray中即可以接收NSString，也可以是NSObject，这时候就需要用id了。 小结 在我的理解看来，id类型就是一个万能的类型，当我们暂时不知道方法的返回值该是什么类型的时候，我们可以先尝试将方法定义为id类型，等待确定方法返回值后再回来修改，或者就直接定义为id类型罢了。 或者我们可以定义一个id类型的变量，用它来做中间变量，用来接收多种类型的数据。 关联返回类型 在介绍instancetype之前先讲解一个概念，就是关联返回类型的方法。 根据Cocoa的命名规则，满足下述规则的方法： 类方法中，以alloc或new开头 实例方法中，以autorelease，init，retain或self开头 会返回一个方法所在类 类型 的变量，这里我用空格和加粗断句，方便大家理解。也就是说，这些方法的返回值的类型，是调用这些方法的那个类的类型。NSArray调用的alloc方法，即[NSArray alloc]。那么这个方法的返回值就是NSArray类型的。同理[[NSArray alloc] init]返回的也是NSArray类型的值。 那么说回来，以上这些的方法呢，我们就称之为关联返回类型的方法。 instancetype 什么是instancetype类型 instancetype是从 clang 3.5 开始提供的一个关键字，表示某个方法返回的未知类型的Objective-C对象。单看这个定义，感觉和id还是挺像的。 instancetype的作用 当我们有一个自定义的方法，例如下面这个: @interface UIView (TestView) + (id) testFounation; @end TestView类是UIView类的分类，当我们如下调用testFounation方法时 [TestView testFounation]; 其返回值的类型和方法的声明相同，是id类型。 而如果我们如下使用instancetype声明testFounation的话 @interface UIView (TestView) + (instancetype) testFounation; @end 当我们再次调用testFounation方法: [TestView testFounation]; 其返回值的类型会是TestView类型。 小结 综上我们可以看出，instancetype的作用实际上就是使非关联返回类型的方法返回所在类的类型。 通俗的讲，就是当我们需要方法的返回类型不是未知的id类型，而是具体的，调用该方法的类的类型的时候，我们要使用instancetype类型。 再具体一些，当我们自定义init方法的时候，其返回值就是instancetype类型。 id和instancetype的异同 总结下来，id和instancetype的异同一共有以下几点: 相同之处 其都可以作为方法的返回类型。 不同之处 instancetype可以返回和方法所在类相同类型的对象，id只能返回未知类型的对象。 instancetype只能作为返回值，不能像id那样作为参数。instancetype只适用于初始化方法和便利构造器的返回值类型。 在不同的内存管理机制中: 在ARC(Auto Reference Count)环境下:instancetype用来在编译期确定实例的类型,而使用id的话,编译器不检查类型, 运行时检查类型。 在MRC(Manual Reference Count)环境下:instancetype和id一样,不做具体类型检查。 关于第三点，因为我现在还没有具体学习到内存管理相关的知识，可以参考下方参考中三木成森的文章，有举例说明该点。 参考 Objective-C中的id类型——Kilnn 理解Objective C 中id——做个不善的人 OC中instancetype与id的区别——三木成森 Objective-C中的instancetype和id区别——kuizhang1 ","link":"https://blog.rakuyoo.top/use-of-id-and-instancetype/"},{"title":"C语言的变量类型","content":"c语言复习之路，之一 变量类型 变量的存储类型 C语言根据变量的存储类型的不同，可以把变量分为:自动变量、静态变量、寄存器变量。 自动变量 定义：自动变量是存储在堆栈中的。 所有局部变量默认情况下都是自动变量。 声明周期：自动变量随所在代码块被执行而创建，随所在代码块执行结束而销毁。如果函数被重复调用，自动变量会被重复创建、销毁。 静态变量 定义：静态变量存储于静态内存中，也就是不属于堆栈。 哪些是静态变量: 所有的全局变量都是静态变量。 被关键字static修饰的局部变量也是静态变量。 声明周期:静态变量在程序运行之前被创建，始终存在。 寄存器变量 定义:存储在硬件寄存器中的变量，称为寄存器变量。 哪些变量是寄存器变量: 被关键字register修饰的自动变量都是寄存器变量。 只有自动变量才可以是寄存器变量，静态变量(全局变量)不行。 寄存器变量只限于int、char和指针类型使用。 生命周期:调用该函数时占用寄存器中存放的值，当函数结束时释放寄存器，变量消失。 使用注意: 不能过多使用。如果寄存器使用饱和时，程序将寄存器变量自动转换为自动变量处理。 为了提高运算速度，一般会将一些频繁使用的自动变量定义为寄存器变量，这样程序尽可能地为它分配寄存器存放，而不用内存。 register int a; // a就是一个寄存器变量 参考 【C语言】18-变量类型——M了个J ","link":"https://blog.rakuyoo.top/variable-types-in-c/"},{"title":"Git 基本操作总结","content":"总结一下 Mac 下 Git 的一些基本操作。 本文其实是把之前给大一学生讲 Git 时备课的稿子拿过来，整理修改，二次加工后的产物。 文章风格偏向于“备忘录”，而不是“教程”。部分地方说的不够清晰还请读者见谅。 至于Git的初学者可以拉到文章最下面，到廖雪峰大神的blog去学习。 概览 首先放一个后文涉及到的命令的总结，方便快速查找。 指令 作用 ssh-keygen -t rsa -C &quot;your-email@emial.com&quot; 新建 SSH ssh -T git@&lt;git url&gt; 检查本地和远端的 SSH 连通性 git add xx.txt 添加某一文件到缓存区 git add . 添加所有文件到缓存区 git commit -m&quot;注释信息&quot; 提交修改到工作区 git status 查看Git仓库状态 git diff 查看文件与之前有何不同 git reset --hard HEAD^ 返回到上一个版本 git reset --hard HEAD~10 向前回滚10个版本 git reset --hard commit ID值 回滚到某个具体的版本 git log --pretty=oneline 输出commit日志，包含作者，日期，提交说明，提交ID git reflog 输出所有的操作信息。方便我们在清屏或退出终端后查询commit ID值 git checkout -- file 撤销工作区修改(add之前) git reset HEAD file 撤销暂存区修改(commit之前) rm file 删除本地文件 git rm file 删除git仓库中的文件 git branch 查看分支列表，绿色为当前分支 git branch 分支名 创建分支 git checkout 分支名 切换分支 git checkout -b 分支名 切换分支，若不存在则创建它 git log --graph --pretty=oneline --abbrev-commit 输出带分支信息的log列表 git merge 目标分支名 合并分支 git branch -d 分支名 删除分支 SSH 首先是 SSH，建议通过 SSH 而不是 https 克隆项目，有一些公司甚至可能会设置只允许通过 SSH 进行克隆。 检查本地是否存在配置 SSH 本地目录为 ~/.ssh，可以在 Finder 或终端中进到该文件夹，查看是否存在 id_rsa 和 id_rsa.pub 文件，来判断是否已经创建了 SSH 配置。如果您之前没有配置过 SSH，那么可能连这个文件夹都不存在。 新建 SSH 如果没有，那就需要新建。命令如下： $ ssh-keygen -t rsa -C &quot;your-email@emial.com&quot; 输入完这条命令之后，我们连着敲三个回车，不管它显示的什么，一直敲回车。直到下面出现一个图形。 之后就可以到 ~/.ssh 文件夹下查看是否多出了 id_rsa 和 id_rsa.pub 文件。 添加到远端 Git 平台 之后我们可以通过 cat 命令，或者直接打开 id_rsa.pub 文件，将其中的内容复制到 Git 平台的 SSH 配置中即可。 检查连通性 添加完成后，我们可以执行下面的命令检查本地和远端 Git 的 SSH 连通性。 $ ssh -T git@&lt;git url&gt; 敲回车后，如果你是第一次连接，会询问你yes or no，输入yes，然后输入 Git 的密码。如果一切都没问题，会返回欢迎信息。 Git 配置 配置好 SSH 之后，就可以回过头来配置 Git。 Mac 平台在安装了 Xcode 后自带 Git 服务。此时我们可以直接在终端输入下列命令进行 Git 配置： $ git config --global user.name &quot;Your Name&quot; $ git config --global user.email &quot;email@example.com&quot; 新建 Git 项目与远端克隆 一般很少有只在本地使用 Git 进行版本管理的场景，基本都会和远端 Git 结合一起使用。 那么我建议的流程是： 现在远端建立空项目 将远端项目克隆到本地 在克隆的文件夹下创建项目进行开发 有的人习惯本地有项目之后，再和远端项目进行关联。这里不介绍该种方法。 如果你本地已经有代码： 没有集成 Git 的情况下，在远端空项目克隆到本地后，直接将本地已有的代码移动到该空项目中即可。 如果已经集成了 Git，则可以修改项目文件夹下的 .git/config 文件，将文件中的 remote 里的 url 项改为远端 url 即可。该操作也适用于更换项目的远端地址。 克隆项目 克隆的命令非常简单，而且非常顾名思义。在某一目录下执行下面的指令，就可以将远程仓库里的内容全部克隆到本地该文件夹下。 $ git clone &lt;ssh链接&gt; 推送文件 可以使用下面的代码将本地项目 push 推送到远程仓库。 $ git push origin master 该命令实际上是把当前分支 master 推送到远程。 Git 常用操作 添加，提交文件 添加、提交文件的步骤为: $ git add xx.txt $ git commit -m &quot;注释信息&quot; add，添加的意思，把xx.txt添加到git仓库。 commit，提交的意思。后面的-m的内容，是一个描述性内容，用来描述我们这次提交的对文件做了哪些修改的。 注意: 可以使用 git add . 将所有文件一并 add，适合对多个文件进行修改的情况。一般直接用这个就可以。 在提交时，-m 可以先输入一个左单引号，敲回车后可以输入多行注释信息。然后再输入右单引号，完成注释信息的输入。 这个 add 命令实际上是可以一口气添加多个文件，然后一口气 commit。 修改文件 指令 作用 git status 查看Git仓库状态 git diff 查看文件与之前有何不同 版本回退 下面的表格是版本回退的具体指令。 指令 作用 git reset --hard HEAD^ 返回到上一个版本 git reset --hard HEAD~10 向前回滚10个版本 git reset --hard commit ID值 回滚到某个具体的版本 下面的表格可以为我们提供我们所需要的commit ID值。 指令 作用 git log --pretty=oneline 输出 commit 日志，包含作者，日期，提交说明，提交ID git reflog 输出所有的操作信息。方便我们在清屏或退出终端后查询commit ID值 撤销修改 在我们 push 到远程仓库之前，我们可以用下表的指令来撤销我们对仓库内文件做出的修改。恢复到上次提交的状态。 指令 作用 git checkout -- file 撤销工作区修改(add之前) git reset HEAD file 撤销暂存区修改(commit之前) git reset --hard HEAD^ 回滚到上一版本(push之前) 删除文件/文件夹 指令 作用 rm file 删除本地文件 git rm file 删除git仓库中的文件 git rm 文件夹名 -r -f 删除git和本地中该文件夹及其下文件 注意 在 git rm 之前，如果我们想撤销，可以放弃工作区修改，git checkout -- file。 在 git rm 之后，如果我们想撤销，直接 git reset --hard HEAD^ 回滚到上一个版本就可以了 在这里多说一句，如果想要删除Git仓库，我们用代码 rm -rf .git 删除**.git文件夹**就可以了。 创建并切换分支 创建分支与切换分支相关的命令如下表所示: 指令 作用 git branch 查看分支列表，绿色为当前分支 git branch 分支名 创建分支 git checkout 分支名 切换分支 git checkout -b 分支名 切换分支，若不存在则创建它 git log --graph --pretty=oneline --abbrev-commit 输出带分支信息的log列表 合并分支 在我们切换回master分支后，执行下面的命令，就可以用快速合并的模式将目标分支合并到master分支上。 $ git merge &lt;目标分支名&gt; 注意: 假如我们要将A分支，合并到 master 分支，那么我们必须先切换到master分支，然后再执行把A分支合并到 master 分支的操作。 介绍一个 git 指令 git ls-files，该指令可以查看当前 git 仓库下的文件。如果想验证新建文件是否合并成功，可以使用该指令。 删除分支 删除分支的命令如下所示。注意，这条命令可不是类似 branch rm 了。 $ git branch -d &lt;分支名&gt; 分支管理小结 关于分支管理的一般过程，我做了一个 GIF，如下图所示: 如图GIF演示了从创建分支到合并分支的过程，其中每一个圆饼代表着一次 commit。 参考 廖雪峰大神的blog ","link":"https://blog.rakuyoo.top/git/"}]}